{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"3_ULMFit-Model.ipynb","provenance":[{"file_id":"1Dw3EHwCOqtsodQ9oUqQZu7DtZu2iwxSk","timestamp":1617873945026},{"file_id":"1oIes-blFKt_KvqSBdl91szZxfST5ctg2","timestamp":1617280863523}],"collapsed_sections":["advisory-influence"],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"floral-recognition","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040020113,"user_tz":-420,"elapsed":25427,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"aa510971-e763-4ce0-a12f-01026517c150"},"source":["!pip install tensorflow_text\n","!pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n","!pip install pythainlp\n","!pip install fastai\n","!pip install emoji\n","!ls"],"id":"floral-recognition","execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_text\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/c0fed4301f592c3b56638ae7292612c17d91a43891ba1aaf9636d535beae/tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4MB)\n","\u001b[K     |████████████████████████████████| 3.4MB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.4.1)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.11.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.3.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.12.4)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.2)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.12.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.19.5)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.6.3)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.1)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.32.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.36.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.5,>=2.4.0->tensorflow_text) (54.2.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.28.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.4)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.8.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.1)\n","Installing collected packages: tensorflow-text\n","Successfully installed tensorflow-text-2.4.3\n","Collecting https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n","\u001b[?25l  Downloading https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n","\u001b[K     | 12.6MB 128kB/s\n","\u001b[?25hCollecting python-crfsuite>=0.9.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 6.1MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp==2.3.1) (2.23.0)\n","Collecting tinydb>=3.0\n","  Downloading https://files.pythonhosted.org/packages/af/cd/1ce3d93818cdeda0446b8033d21e5f32daeb3a866bbafd878a9a62058a9c/tinydb-4.4.0-py3-none-any.whl\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp==2.3.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp==2.3.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp==2.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp==2.3.1) (2020.12.5)\n","Building wheels for collected packages: pythainlp\n","  Building wheel for pythainlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pythainlp: filename=pythainlp-2.3.1-cp37-none-any.whl size=11006836 sha256=ab9ebbfbb4310c779ec1a21fad4c02c676601ec7bc1c76f3a8112a80b64c1691\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-fiwn0u9j/wheels/79/4e/1e/26f3198c6712ecfbee92928ed1dde923a078da3d222401cc78\n","Successfully built pythainlp\n","Installing collected packages: python-crfsuite, tinydb, pythainlp\n","Successfully installed pythainlp-2.3.1 python-crfsuite-0.9.7 tinydb-4.4.0\n","Requirement already satisfied: pythainlp in /usr/local/lib/python3.7/dist-packages (2.3.1)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (0.9.7)\n","Requirement already satisfied: tinydb>=3.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n","Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (1.0.61)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.8.1+cu101)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai) (4.6.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n","Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai) (0.9.1+cu101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n","Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai) (2.7.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n","Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.352.0)\n","Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.2)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->fastai) (3.7.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2020.12.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (54.2.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (2.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (4.41.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.4.1)\n","Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 5.6MB/s \n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-1.2.0\n","drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"advanced-favor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040023266,"user_tz":-420,"elapsed":28570,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"ad04c8a6-8436-4e66-d237-239a6e95efa5"},"source":["!pip install plotnine"],"id":"advanced-favor","execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.5)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.19.5)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2018.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rubber-count","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040025929,"user_tz":-420,"elapsed":31226,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"47085096-531f-4119-a305-7c87f9982069"},"source":["!pip install iterative-stratification"],"id":"rubber-count","execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting iterative-stratification\n","  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (0.22.2.post1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.0.1)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"superior-series","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040031013,"user_tz":-420,"elapsed":36301,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"918d57ca-c1f9-4c4c-9152-b6b479ec90a4"},"source":["!pip install sklearn_crfsuite"],"id":"superior-series","execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sklearn_crfsuite\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.9.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (1.15.0)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (4.41.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.8.9)\n","Installing collected packages: sklearn-crfsuite\n","Successfully installed sklearn-crfsuite-0.3.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tGmC9tkr0PJT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040031014,"user_tz":-420,"elapsed":36292,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"d6268ecd-78e2-4ba5-b6b0-9013149f475b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"tGmC9tkr0PJT","execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"recognized-willow"},"source":["import pandas as pd\n","import numpy as np\n","from pythainlp import word_tokenize\n","from ast import literal_eval\n","from tqdm import tqdm_notebook\n","from collections import Counter\n","import re\n","\n","#viz\n","from plotnine import *\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import fastai\n","from fastai.text import *\n","from fastai.callbacks import CSVLogger"],"id":"recognized-willow","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"western-silence"},"source":["benchmark_labels = ['การเมือง','ต่างประเทศ','เศรษฐกิจ','การศึกษา']"],"id":"western-silence","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"express-community"},"source":["def replace_newline(t):\n","    return re.sub('[\\n]{1,}', ' ', t)\n","\n","ft_data = 'ft_data/'"],"id":"express-community","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aware-liabilities","executionInfo":{"status":"ok","timestamp":1618040051882,"user_tz":-420,"elapsed":57146,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"9e081f68-897b-417e-cd19-07a4cd33517a"},"source":["df = pd.read_csv('/content/drive/MyDrive/prachathai-67k.csv',index_col=['labels'])\n","print(df.shape)"],"id":"aware-liabilities","execution_count":null,"outputs":[{"output_type":"stream","text":["(67889, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"id":"ordered-decade","executionInfo":{"status":"ok","timestamp":1618040051883,"user_tz":-420,"elapsed":57139,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"b19109a1-d013-4efe-a2a3-97c575239eb1"},"source":["df.head()"],"id":"ordered-decade","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>url</th>\n","      <th>date</th>\n","      <th>title</th>\n","      <th>body_text</th>\n","    </tr>\n","    <tr>\n","      <th>labels</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>['ข่าว', 'สิ่งแวดล้อม']</th>\n","      <td>https://prachatai.com/print/42</td>\n","      <td>2004-08-24 14:31</td>\n","      <td>นักวิจัยหนุน  \"แม้ว\"  เปิด  \"จีเอ็มโอ\"</td>\n","      <td>ประชาไท --- 23 ส.ค.2547  นักวิจัยฯ ชี้นโยบายจี...</td>\n","    </tr>\n","    <tr>\n","      <th>['ข่าว', 'สิ่งแวดล้อม']</th>\n","      <td>https://prachatai.com/print/41</td>\n","      <td>2004-08-24 14:22</td>\n","      <td>ภาคประชาชนต้านเปิดเสรีจีเอ็มโอ</td>\n","      <td>ประชาไท- 23 ส.ค.2547   นักวิชาการ ภาคประชาชน จ...</td>\n","    </tr>\n","    <tr>\n","      <th>['ข่าว', 'สิ่งแวดล้อม']</th>\n","      <td>https://prachatai.com/print/43</td>\n","      <td>2004-08-24 15:17</td>\n","      <td>จุฬาฯ ห่วงจีเอ็มโอลามข้าวไทย</td>\n","      <td>นโยบายที่อนุญาตให้ปลูกร่วมกับพืชอื่นได้นั้นถื...</td>\n","    </tr>\n","    <tr>\n","      <th>['ข่าว', 'การเมือง', 'คณะเศรษฐศาสตร์ มหาวิทยาลัยธรรมศาสตร์', 'ชนชั้นกลาง', 'ซีอีโอ', 'ทักษิณ ชินวัตร', 'ทักษิโนมิกส์', 'มหาวิทยาลัยธรรมศาสตร์', 'ระบอบทักษิณ', 'รังสรรค์ ธนะพรพันธุ์', 'สุวินัย ภรณวลัย', 'เกษียร เตชะพีระ', 'เสวนา']</th>\n","      <td>https://prachatai.com/print/45</td>\n","      <td>2004-08-24 15:58</td>\n","      <td>ฟองสบู่การเมืองแตก ทักษิณหมดกึ๋น ชนชั้นกลางหมด...</td>\n","      <td>ประชาไท -- 23 ส.ค. 47  ขาประจำทักษิณ ฟันธง ฟอง...</td>\n","    </tr>\n","    <tr>\n","      <th>['ข่าว', 'สิ่งแวดล้อม']</th>\n","      <td>https://prachatai.com/print/47</td>\n","      <td>2004-08-24 16:10</td>\n","      <td>กอต.เสนอเลิกถนนคลองลาน-อุ้มผาง</td>\n","      <td>ประชาไท-23 ส.ค.47  คณะกรรมการอนุรักษ์ ผืนป่าตะ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                               url  ...                                          body_text\n","labels                                                                              ...                                                   \n","['ข่าว', 'สิ่งแวดล้อม']                             https://prachatai.com/print/42  ...  ประชาไท --- 23 ส.ค.2547  นักวิจัยฯ ชี้นโยบายจี...\n","['ข่าว', 'สิ่งแวดล้อม']                             https://prachatai.com/print/41  ...  ประชาไท- 23 ส.ค.2547   นักวิชาการ ภาคประชาชน จ...\n","['ข่าว', 'สิ่งแวดล้อม']                             https://prachatai.com/print/43  ...   นโยบายที่อนุญาตให้ปลูกร่วมกับพืชอื่นได้นั้นถื...\n","['ข่าว', 'การเมือง', 'คณะเศรษฐศาสตร์ มหาวิทยาลั...  https://prachatai.com/print/45  ...  ประชาไท -- 23 ส.ค. 47  ขาประจำทักษิณ ฟันธง ฟอง...\n","['ข่าว', 'สิ่งแวดล้อม']                             https://prachatai.com/print/47  ...  ประชาไท-23 ส.ค.47  คณะกรรมการอนุรักษ์ ผืนป่าตะ...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAerN9rVy8nX","executionInfo":{"status":"ok","timestamp":1618040051884,"user_tz":-420,"elapsed":57132,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"05fbc7b9-4948-4af5-9c21-0403e7142a30"},"source":["df.info()"],"id":"mAerN9rVy8nX","execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 67889 entries, ['ข่าว', 'สิ่งแวดล้อม'] to ['ข่าว', 'สิทธิมนุษยชน', 'ต่างประเทศ', 'อูยกูร์', 'เตอร์กิสถานตะวันออก', 'สภาอุยกูร์โลก', 'ค่ายกักกัน', 'วันประกาศเอกราช', 'จีน', 'ศาสนา', 'อิสลาม', 'ค่ายกักกันปลูกฝังความเชื่อใหม่']\n","Data columns (total 4 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   url        67889 non-null  object\n"," 1   date       67889 non-null  object\n"," 2   title      67889 non-null  object\n"," 3   body_text  67889 non-null  object\n","dtypes: object(4)\n","memory usage: 2.6+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObX2jGQny8vE","executionInfo":{"status":"ok","timestamp":1618040051885,"user_tz":-420,"elapsed":57124,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"3e119ace-caa6-495a-d89f-40b9c8ea3389"},"source":["idx = pd.IndexSlice\n","idx"],"id":"ObX2jGQny8vE","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.core.indexing._IndexSlice at 0x7f4c89b49290>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3z_bk969y8xl","executionInfo":{"status":"ok","timestamp":1618040051885,"user_tz":-420,"elapsed":57116,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"8f02ee56-8912-483a-d9ab-5b7ff99b0b6b"},"source":["df.index.get_level_values(0).str.contains('การเมือง')"],"id":"3z_bk969y8xl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False, False, False,  True, ...,  True,  True,  True, False])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"RSdudxxEy80D"},"source":["df_politics = df.loc[idx[df.index.get_level_values(0).str.contains('การเมือง')],:]"],"id":"RSdudxxEy80D","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zp-ChQa1y82p"},"source":["politics = df_politics[:400]"],"id":"Zp-ChQa1y82p","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPaT77Wjy84w"},"source":["df_economics = df.loc[idx[df.index.get_level_values(0).str.contains('เศรษฐกิจ')],:]"],"id":"BPaT77Wjy84w","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjqiT88Yy87Y"},"source":["economics = df_economics[:400]"],"id":"HjqiT88Yy87Y","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbQiOe7tzGnC","executionInfo":{"status":"ok","timestamp":1618040051887,"user_tz":-420,"elapsed":57102,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"a2ecd65a-c719-46e0-f7e9-01a902c31fb5"},"source":["df.international = df.loc[idx[df.index.get_level_values(0).str.contains('ต่างประเทศ')],:]"],"id":"tbQiOe7tzGnC","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ZgirWGbdzGpf"},"source":["international = df.international[:400]"],"id":"ZgirWGbdzGpf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1g_aXpDizGsM","executionInfo":{"status":"ok","timestamp":1618040052567,"user_tz":-420,"elapsed":57772,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"0cccd203-b3e4-46eb-ff30-89bf5af3d429"},"source":["df.education = df.loc[idx[df.index.get_level_values(0).str.contains('การศึกษา')],:]"],"id":"1g_aXpDizGsM","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tNFeTj0ezGul"},"source":["education = df.education[:400]"],"id":"tNFeTj0ezGul","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4MYT--FzGw3"},"source":["df_merge = pd.concat([politics,economics,international,education],ignore_index=False)"],"id":"U4MYT--FzGw3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDFIZxD6zGzP"},"source":["df_clean = df_merge.drop_duplicates()"],"id":"tDFIZxD6zGzP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOFWNX8IzLz6"},"source":["df_clean = df_clean.reset_index(drop=False)"],"id":"eOFWNX8IzLz6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570},"id":"41e4JvE7zL2U","executionInfo":{"status":"ok","timestamp":1618040052569,"user_tz":-420,"elapsed":57760,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"35e9a9b0-6551-4806-87a6-c48caa4c63cb"},"source":["data = df_clean[['url','date','title','body_text','labels']]\n","data"],"id":"41e4JvE7zL2U","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>url</th>\n","      <th>date</th>\n","      <th>title</th>\n","      <th>body_text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>https://prachatai.com/print/45</td>\n","      <td>2004-08-24 15:58</td>\n","      <td>ฟองสบู่การเมืองแตก ทักษิณหมดกึ๋น ชนชั้นกลางหมด...</td>\n","      <td>ประชาไท -- 23 ส.ค. 47  ขาประจำทักษิณ ฟันธง ฟอง...</td>\n","      <td>['ข่าว', 'การเมือง', 'คณะเศรษฐศาสตร์ มหาวิทยาล...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>https://prachatai.com/print/46</td>\n","      <td>2004-08-24 16:02</td>\n","      <td>ชนชั้นกลางในความหมายของ เกษียร เตชะพีระ</td>\n","      <td>รศ. เกษียร เตชะพีระ กล่าวอธิบายความหมายของชนช...</td>\n","      <td>['ข่าว', 'การเมือง', 'ชนชั้นกลาง', 'ทักษิณ ชิน...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>https://prachatai.com/print/87</td>\n","      <td>2004-08-26 17:46</td>\n","      <td>ทักษิโณมิกส์ ในสายตารังสรรค์  ธนะพรพันธุ์</td>\n","      <td>ระบอบการเมืองการปกครองภายใต้ทักษิโณมิกส์เป็นร...</td>\n","      <td>['บทความ', 'การเมือง', 'เศรษฐกิจ', 'ขีดเส้นใต้...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>https://prachatai.com/print/4903</td>\n","      <td>2005-07-06 16:37</td>\n","      <td>ครม.เรียกเงินคืนสำนักงบฯ 50,000 ล้าน</td>\n","      <td>ประชาไท-5 กค. 48 คณะรัฐมนตรีเรียกงบประมาณคืนสำ...</td>\n","      <td>['ข่าว', 'การเมือง']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>https://prachatai.com/print/4904</td>\n","      <td>2005-07-14 14:26</td>\n","      <td>\"หมอแว\"หยุดกิจกรรมประชาสังคมลุยหาเสียงส.ว.</td>\n","      <td>[1]นายแพทย์แวมาหะดี แวดาโอ๊ะ อดีตจำเลยคดีเจไอ...</td>\n","      <td>['ข่าว', 'การเมือง']</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1539</th>\n","      <td>https://prachatai.com/print/38437</td>\n","      <td>2011-12-22 02:32</td>\n","      <td>60 ปี ธเนศวร์ เจริญเมือง: ล้านนายูโทเปีย กับ เ...</td>\n","      <td>\"ชาวแม่ขะ จานฟังตางนี้หื้อดี เปิ้นปิ๊กมาแล้ว  ...</td>\n","      <td>['บทความ', 'วัฒนธรรม', 'การศึกษา', 'สังคม', '6...</td>\n","    </tr>\n","    <tr>\n","      <th>1540</th>\n","      <td>https://prachatai.com/print/38489</td>\n","      <td>2011-12-25 23:33</td>\n","      <td>นิติราษฎร์ เดินหน้า ลบล้างผลพวงรัฐประหาร แก้ไข...</td>\n","      <td>นักวิชาการกลุ่มนิติราษฎร์ ประกาศกิจกรรมทางวิชา...</td>\n","      <td>['ข่าว', 'การเมือง', 'สิทธิมนุษยชน', 'การศึกษา...</td>\n","    </tr>\n","    <tr>\n","      <th>1541</th>\n","      <td>https://prachatai.com/print/38558</td>\n","      <td>2012-01-01 23:28</td>\n","      <td>[คลิป] ธงชัย วินิจจะกูล: เมื่อความจริง (นิยาย)...</td>\n","      <td>วิดีโอการอภิปรายของธงชัย วินิจจะกูล เกี่ยวกับห...</td>\n","      <td>['ข่าว', 'วัฒนธรรม', 'การศึกษา', 'สังคม', 'ขีด...</td>\n","    </tr>\n","    <tr>\n","      <th>1542</th>\n","      <td>https://prachatai.com/print/38623</td>\n","      <td>2012-01-05 18:24</td>\n","      <td>เสียงจากอดีตนักศึกษา: ถึง...เธอ, ผู้นำเชียร์งา...</td>\n","      <td>โหวกเหวกส่งเสียงดัง ด้วยมุ่งหวังยังจุดหมาย ซัก...</td>\n","      <td>['บทความ', 'การศึกษา', 'กวีประชาไท', 'จุฬาลงกร...</td>\n","    </tr>\n","    <tr>\n","      <th>1543</th>\n","      <td>https://prachatai.com/print/38647</td>\n","      <td>2012-01-07 16:26</td>\n","      <td>เรายังจะจั​ดงานบอลฯกั​นอีกหรือ</td>\n","      <td>ในขณะที่เสรีภาพในการคิด ในการพูดและความยุติธรร...</td>\n","      <td>['บทความ', 'การศึกษา', 'นักศึกษา', 'รักษ์ชาติ ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1544 rows × 5 columns</p>\n","</div>"],"text/plain":["                                    url  ...                                             labels\n","0        https://prachatai.com/print/45  ...  ['ข่าว', 'การเมือง', 'คณะเศรษฐศาสตร์ มหาวิทยาล...\n","1        https://prachatai.com/print/46  ...  ['ข่าว', 'การเมือง', 'ชนชั้นกลาง', 'ทักษิณ ชิน...\n","2        https://prachatai.com/print/87  ...  ['บทความ', 'การเมือง', 'เศรษฐกิจ', 'ขีดเส้นใต้...\n","3      https://prachatai.com/print/4903  ...                               ['ข่าว', 'การเมือง']\n","4      https://prachatai.com/print/4904  ...                               ['ข่าว', 'การเมือง']\n","...                                 ...  ...                                                ...\n","1539  https://prachatai.com/print/38437  ...  ['บทความ', 'วัฒนธรรม', 'การศึกษา', 'สังคม', '6...\n","1540  https://prachatai.com/print/38489  ...  ['ข่าว', 'การเมือง', 'สิทธิมนุษยชน', 'การศึกษา...\n","1541  https://prachatai.com/print/38558  ...  ['ข่าว', 'วัฒนธรรม', 'การศึกษา', 'สังคม', 'ขีด...\n","1542  https://prachatai.com/print/38623  ...  ['บทความ', 'การศึกษา', 'กวีประชาไท', 'จุฬาลงกร...\n","1543  https://prachatai.com/print/38647  ...  ['บทความ', 'การศึกษา', 'นักศึกษา', 'รักษ์ชาติ ...\n","\n","[1544 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"talented-fever"},"source":["### Train-validation-test Split"],"id":"talented-fever"},{"cell_type":"code","metadata":{"id":"electrical-luther"},"source":["\n","all_df = data[['body_text', 'labels']].copy()\n","all_df.head()\n","\n","#use only benchmark labels\n","for lab in benchmark_labels:\n","    all_df[lab] = all_df.labels.map(lambda x: 1 if lab in x else 0)\n","all_df.drop('labels',axis=1,inplace=True)"],"id":"electrical-luther","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"silver-championship","executionInfo":{"status":"ok","timestamp":1618040052570,"user_tz":-420,"elapsed":57751,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"bc462609-cf9f-446c-b46b-03ba881768ac"},"source":["all_df.head()"],"id":"silver-championship","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>body_text</th>\n","      <th>การเมือง</th>\n","      <th>ต่างประเทศ</th>\n","      <th>เศรษฐกิจ</th>\n","      <th>การศึกษา</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ประชาไท -- 23 ส.ค. 47  ขาประจำทักษิณ ฟันธง ฟอง...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>รศ. เกษียร เตชะพีระ กล่าวอธิบายความหมายของชนช...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ระบอบการเมืองการปกครองภายใต้ทักษิโณมิกส์เป็นร...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ประชาไท-5 กค. 48 คณะรัฐมนตรีเรียกงบประมาณคืนสำ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[1]นายแพทย์แวมาหะดี แวดาโอ๊ะ อดีตจำเลยคดีเจไอ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           body_text  ...  การศึกษา\n","0  ประชาไท -- 23 ส.ค. 47  ขาประจำทักษิณ ฟันธง ฟอง...  ...         0\n","1   รศ. เกษียร เตชะพีระ กล่าวอธิบายความหมายของชนช...  ...         0\n","2   ระบอบการเมืองการปกครองภายใต้ทักษิโณมิกส์เป็นร...  ...         0\n","3  ประชาไท-5 กค. 48 คณะรัฐมนตรีเรียกงบประมาณคืนสำ...  ...         0\n","4   [1]นายแพทย์แวมาหะดี แวดาโอ๊ะ อดีตจำเลยคดีเจไอ...  ...         0\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"assumed-charm","executionInfo":{"status":"ok","timestamp":1618040052570,"user_tz":-420,"elapsed":57743,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"4478ddb6-26ae-4193-de66-6512763606c4"},"source":["#randomly create test set\n","idx = np.arange(all_df.shape[0])\n","np.random.seed(1412)\n","np.random.shuffle(idx)\n","train_valid_idx, test_idx = idx[:int(all_df.shape[0] * 0.8)], idx[int(all_df.shape[0] * 0.8):]\n","print(len(train_valid_idx),len(test_idx))"],"id":"assumed-charm","execution_count":null,"outputs":[{"output_type":"stream","text":["1235 309\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"boring-neutral","executionInfo":{"status":"ok","timestamp":1618040052862,"user_tz":-420,"elapsed":58027,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"83010e44-faad-4db3-ead4-b5aac52c5a4d"},"source":["#save test_df\n","test_df = all_df.iloc[test_idx,:]\n","print(test_df.shape)\n","test_df.to_csv('/content/drive/MyDrive/ject_final_petch/test_df.csv',index=False)"],"id":"boring-neutral","execution_count":null,"outputs":[{"output_type":"stream","text":["(309, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"revolutionary-spine","executionInfo":{"status":"ok","timestamp":1618040052863,"user_tz":-420,"elapsed":58020,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"884ee25d-ceff-43bd-9978-b5f6c142e362"},"source":["#stratified shuffle split for validation set\n","from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","\n","train_valid_df = all_df.iloc[train_valid_idx,:]\n","ohe_df = train_valid_df.iloc[:,1:]\n","msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.125, random_state=1412)\n","train_idx, valid_idx = next(msss.split(ohe_df,ohe_df))\n","print(len(train_idx),len(valid_idx))"],"id":"revolutionary-spine","execution_count":null,"outputs":[{"output_type":"stream","text":["1082 153\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"external-plant","executionInfo":{"status":"ok","timestamp":1618040053582,"user_tz":-420,"elapsed":58732,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"3ddefd36-7ce1-4e15-f60e-f297d831254a"},"source":["#save train and valid_df\n","train_df = train_valid_df.iloc[train_idx,:]\n","print(train_df.shape)\n","train_df.to_csv('/content/drive/MyDrive/ject_final_petch/train_df.csv',index=False)\n","\n","valid_df = all_df.iloc[valid_idx,:]\n","print(valid_df.shape)\n","valid_df.to_csv('/content/drive/MyDrive/ject_final_petch/valid_df.csv',index=False)"],"id":"external-plant","execution_count":null,"outputs":[{"output_type":"stream","text":["(1082, 5)\n","(153, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mature-serial"},"source":["#save train_valid_idx, test_idx for reproducibility\n","import pickle\n","pickle.dump(train_valid_idx,open('/content/drive/MyDrive/ject_final_petch/train_valid_idx.pkl','wb'))\n","pickle.dump(test_idx,open('/content/drive/MyDrive/ject_final_petch/test_idx.pkl','wb'))"],"id":"mature-serial","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fantastic-soccer","executionInfo":{"status":"ok","timestamp":1618040053872,"user_tz":-420,"elapsed":59012,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"0ba008bc-14d1-4651-dc4c-af9b4207a931"},"source":["#test set prevalence\n","test_df.iloc[:,1:].mean(0)"],"id":"fantastic-soccer","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["การเมือง      0.469256\n","ต่างประเทศ    0.333333\n","เศรษฐกิจ      0.278317\n","การศึกษา      0.242718\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"biblical-command","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040074278,"user_tz":-420,"elapsed":79410,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"5c873900-66df-4ff6-f515-ab63ed9b33f0"},"source":["import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","from tqdm import tqdm_notebook\n","from collections import Counter\n","import re\n","\n","#viz\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from fastai.text import *\n","from fastai.callbacks import CSVLogger, SaveModelCallback\n","\n","from pythainlp.ulmfit import *\n","\n","model_path = '/content/drive/MyDrive/ject_final_petch/prachathai_data/'\n","\n","train_df = pd.read_csv('/content/drive/MyDrive/ject_final_petch/train_df.csv')\n","valid_df = pd.read_csv('/content/drive/MyDrive/ject_final_petch/valid_df.csv')\n","test_df = pd.read_csv('/content/drive/MyDrive/ject_final_petch/test_df.csv')\n","all_df = pd.concat([train_df,valid_df,test_df],0).reset_index(drop=True)"],"id":"biblical-command","execution_count":null,"outputs":[{"output_type":"stream","text":["Corpus: wiki_lm_lstm\n","- Downloading: wiki_lm_lstm 0.32\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1050919089/1050919089 [00:19<00:00, 55271021.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Corpus: wiki_itos_lstm\n","- Downloading: wiki_itos_lstm 0.32\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1530484/1530484 [00:00<00:00, 16837062.18it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"naughty-clearance"},"source":["#### Finetune Language Model"],"id":"naughty-clearance"},{"cell_type":"code","metadata":{"id":"neutral-extra"},"source":["#get vocab\n","thwiki_itos = pickle.load(open(THWIKI_LSTM['itos_fname'], 'rb'))\n","thwiki_vocab = fastai.text.transform.Vocab(thwiki_itos)\n","\n","tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th)\n","processor = [TokenizeProcessor(tokenizer=tt, chunksize=10000, mark_fields=False),\n","            NumericalizeProcessor(vocab=None, max_vocab=60000, min_freq=3)]"],"id":"neutral-extra","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"greenhouse-reunion","executionInfo":{"status":"ok","timestamp":1618040112892,"user_tz":-420,"elapsed":118014,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"0820d93f-69f8-49cd-e93f-fcc696f4a795"},"source":["data_lm = (TextList.from_df(all_df, model_path, cols=['body_text'], processor=processor)\n","    .split_by_rand_pct(valid_pct = 0.01, seed = 1412)\n","    .label_for_lm()\n","    .databunch(bs=64))\n","\n","data_lm.sanity_check()"],"id":"greenhouse-reunion","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(a, dtype=dtype, **kwargs)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"fiscal-commission"},"source":["data_lm.save('/content/drive/MyDrive/ject_final_petch/prachathai_lm.pkl')"],"id":"fiscal-commission","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"continuous-category","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040114220,"user_tz":-420,"elapsed":119330,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"83950c2d-5e82-4be2-a592-f962fe812f38"},"source":["data_lm = load_data(model_path,'/content/drive/MyDrive/ject_final_petch/prachathai_lm.pkl')\n","data_lm.sanity_check()\n","print(len(data_lm.train_ds), len(data_lm.valid_ds))"],"id":"continuous-category","execution_count":null,"outputs":[{"output_type":"stream","text":["1529 15\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"diagnostic-grade"},"source":["config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n","             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n","trn_args = dict(drop_mult=0.9, clip=0.12, alpha=2, beta=1)\n","\n","learn = language_model_learner(data_lm, AWD_LSTM, config=config, pretrained=False, **trn_args)\n"],"id":"diagnostic-grade","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"substantial-microwave","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040123154,"user_tz":-420,"elapsed":128254,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"298cee70-e2ed-4753-b659-c9ee53b47297"},"source":["#load pretrained models\n","learn.load_pretrained(**THWIKI_LSTM)"],"id":"substantial-microwave","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LanguageLearner(data=TextLMDataBunch;\n","\n","Train: LabelList (1529 items)\n","x: LMTextList\n","xxbos   \n","       \n","   \n","   \n","   \n","       \n","   \n","       \n","   \n","       \n","   \n","       \n","   \n","       \n","   \n","   6     สิงหาคม   2488   ระเบิดปรมาณู ลูก แรก ถูก ทิ้ง ลง ไป ถล่ม เมือง ฮิ โร ชิ มา ของ ประเทศญี่ปุ่น   นับ เป็นครั้งแรก ที่ มี การ ใช้ ระเบิดปรมาณู ใน สงคราม   และ นับ เป็นครั้งแรก ที่ มวล มนุษยชาติ ได้ สัมผัส กับ อานุภาพ แห่ง ระเบิดปรมาณู   อีก   3   วัน   ต่อมา   9   สิงหาคม   2488   ระเบิด ลูก ที่สอง ก็ ตามมา อีก ที่ เมือง นา งา ซา กิ   ผล ก็ คือ   ต่อมา วันที่   14   สิงหาคม ญี่ปุ่น ประกาศ ยอมแพ้   อย่าง ไม่มีเงื่อนไข ใดๆ   เป็น การยุติ มหา สงคราม เอเชีย บูรพา   หรือ สงครามโลก ครั้ง ที่   2   \n","   \n","       \n","   \n","   \n","   \n","   ผล จาก ระเบิดปรมาณู ครั้งนั้น   เชื่อ ว่า ทำให้ ชาวเมือง ฮิ โร ชิ มา ร้อยละ   40   ต้อง เสียชีวิต   และ เช่น กันที่ เมือง นา งา ซา กิ ที่ ได้ เสียชีวิต ของ พลเมือง ไป ประมาณ ร้อยละ   31   โดย มี ผู้ ที่ รอดชีวิต ใน ฮิ โร ชิ มา ประมาณ   210,000   คน   และ   166,000   คน ที่นา งา ซา กิ ที่ รอดชีวิต มาจาก เหตุการณ์ นั้น ได้ หรือ ที่ ชาวญี่ปุ่น เรียก ว่า เป็น พวก   ไฮ บา กู ชะ   ( hibakusha ) นั้น   ต้อง เจ็บปวด และ ทุกข์ทรมาน   จาก ผลกระทบ ที่ ได้รับ จาก กัมมันตภาพรังสี   ที่ เรียก ว่า เป็น คีลอยด์   ( keloid-   สภาพ ผิวหนัง ที่ เป็นแผล นูน ขึ้น มา )     บางคน ก็ เป็น ทั้งตัว   บางคน ก็ เป็น ทั่ว ใบหน้า     และ นอกจากนั้น ก็ ยัง มี ผลสืบเนื่อง มา จน ปัจจุบัน   หลาย คน ต้อง เป็น ไทรอยด์ เป็นพิษ   หลาย คน มี เนื้อร้าย ที่ ก่อ มะเร็ง ในที่สุด   \n","   \n","       \n","   \n","   ซา ดา โกะ   xxwrep 1 ซา กิ   อายุ ได้ เพียง   2   ขวบ   ใน ตอนที่ ระเบิดปรมาณู ถูก ทิ้ง ลง ที่ ฮิ โร ชิ มา     อีก   10   ปี ต่อมา   ( 2498 )   เธอ ป่วย ด้วย โรค ลูคี เมีย   เธอ ทุกข์ทรมาน จาก โรค นี้   มาก   แต่ ด้วย ความเชื่อ ของ ชาวญี่ปุ่น ที่ บอ กว่า หาก ใครก็ตาม ที่ สามารถ พับ นก กระเรียน ได้ ถึง   1   พัน ตัว จะ ทำให้ คำอธิษฐาน ขอพร ใดๆ เป็นจริง ขึ้น มา ได้   ด้วย ความปรารถนา ที่จะ หาย จาก โรค นี้   ซา ดา โกะ จึง เริ่ม ลงมือ พับ นก กระเรียน ที่ เตียง ใน โรงพยาบาล ที่ รักษาตัว อยู่ นั่นเอง   \n","   \n","       \n","   \n","   ทว่า   8   เดือน หลังจาก การ รักษาตัว   เด็กหญิง ก็ เสียชีวิต ลง โดย ที่ เธอ พับ นก กะ เรียน ไว้ ได้ แล้ว ถึง   666   ตัว     เมื่อ เพื่อน ใน ชั้นเรียน ของ เธอ รู้ ข่าว การ จากไป ของ เธอ ก็ รู้สึก ตกใจ และ เมื่อ รู้เรื่อง การ พับ นก กระเรียน ของ เธอ   เพื่อน ๆ จึง ได้ ช่วยกัน มา สานต่อ การ พับ นก กระเรียน จน ครบ   และ ได้ เริ่ม เก็บเงิน กัน เพื่อ สร้าง อนุสรณ์สถาน เพื่อ เป็น กำลังใจ ให้ แก่ ดวงวิญญาณ ของ เธอ และ ดวงวิญญาณ ของ เด็ก คนอื่นๆ ที่ ต้อง เสียชีวิต ไป เนื่องจาก สงคราม   ซึ่ง ในที่สุด ก็ เกิด เป็น อนุสาวรีย์ สันติภาพ ของ เด็ก ขึ้น ที่ อนุสรณ์ สถานที่ เมือง ฮิ โร ชิ มา ขึ้น มา ใน ปี   2501   ที่ เป็น ภาพ เด็กหญิง กำลัง ปล่อยนก กระเรียน อยู่ เหนือศีรษะ   ที่ ยังคง ปรากฏ อยู่ จน ทุกวันนี้   \n","   \n","       \n","   \n","   ซา ดา โกะ นั้น จากไป แล้ว   แต่   เคโกะ   โอ กุระ   นั้น ยัง คงอยู่   เธอ ก็ เด็ก ที่อยู่ ใน วัน ไล่เลี่ยกัน กับ ซา ดา โกะ เมื่อ ตอนที่ ระเบิด ลง ที่ ฮิ โร ชิ มา   ทุกวันนี้ หาก ดู เพียง ภายนอก แล้ว หน้าตา เธอ ดูเหมือน จะ เป็นปกติ   ทว่า   บาดแผล ใน ใจ นั้น อย่างไร ก็ ไม่ หาย   \n","   \n","       \n","   \n","   \" ฉัน ไม่มี แผล ตามตัว     แต่ ฉัน ยังคง ฝันร้าย อยู่ \"   เคโกะ กล่าว   เธอ ก็ เหมือน ๆ กับ บรรดา เหล่า ผู้รอดชีวิต   หรือ ไฮ บา กู ซะ ทั้งหลาย ที่   ได้รับ การ ตรวจ เช็ค ร่างกาย จาก คณะกรรมการ ผู้บาดเจ็บ จาก ระเบิดปรมาณู   ใน ช่วย 2 - 3   เดือน แรก หลังจากที่ มี ระเบิด   โดย ทาง สหรัฐฯ นั้น หลังจากที่ สงคราม สิ้นสุดลง แล้ว ก็ได้ เข้ามา ให้การ ดูแล ทางการแพทย์ แก่ ผู้ ที่ ได้รับ ผลกระทบ จาก สงคราม   และ ขณะเดียวกัน ก็ มี ส่ง ทีม นักวิทยาศาสตร์ เข้ามา ศึกษา ถึง ผลกระทบ ที่เกิด ขึ้นกับ คน ที่ ได้รับ กัมมันตภาพรังสี   \n","   \n","       \n","   \n","   เคโกะ กล่าวว่า มี รถ มา รับ เธอ เพื่อ ไป ตรวจ เช็ค ร่างกาย อยู่ หลายครั้ง   และ   เธอ ก็ มีความรู้สึก กลัว อยู่ เสมอ   เธอ กลัว ว่า จะ มี อะไร บางอย่าง อยู่ ใน ตัว เธอ   เป็น ความกลัว ใน สิ่ง ที่ มองไม่เห็น   ซึ่ง ใน ตอนนั้น พบ ว่า   เธอ มี ภาวะ เลือด จาง เล็กน้อย      เธอ ก็ ถาม ตัวเอง ทันที ว่า นี่ เกิด จาก ระเบิด ใช่ หรือไม่   และ ยัง ถาม ตัวเอง ต่อไป ว่า แล้ว ต่อไป จะ มีลูก ตามปกติ ได้ ไหม   \n","   \n","       \n","   \n","   ความกลัว ที่เกิด ขึ้นกับ เคโกะ นั้น ไม่ใช่ เรื่อง ผิดปกติ   เพราะ เด็ก ๆ มี อยู่ ใน ยุค นั้น ต่าง พบ กับ ความกลัว เช่นนี้ กัน ทั้งสิ้น   หลาย คน ยัง มี อาการ หลอน อยู่ จน ทุกวันนี้   \n","   \n","       \n","   \n","   วันนี้ เหตุการณ์ ใน ฮิ โร ชิ มา นั้น ล่วงเลย มาถึง   60   ปี   แล้วแต่ ผล ของ การกระทำ ที่ ใช้กำลัง ใช้ รุนแรง   การ ใช้ อาวุธ ที่ มี อานุภาพ การทำลายล้าง อย่างมหาศาล นั้น ยัง คงอยู่   เหตุการณ์   ฮิ โร ชิ มา นั้น ทำให้ ญี่ปุ่น ต้อง ยุติ การ ใช้กำลัง ทหาร เข้า ต่อสู้   และ   มี ข้อตกลง ที จะ ไม่มี การผลิต อาวุธ   อานุภาพ สูง อีก   \n","   \n","       \n","   \n","   ทว่า   จนกระทั่ง ปัจจุบันนี้   แม้ ผล จาก การ ใช้ ปรมาณู ได้ เสนอ ภาพ ให้ เห็น แล้ว ว่าการ ใช้ ความรุนแรง นั้น ก่อให้เกิด ความเสียหาย ต่อ มนุษยชาติ ทั้งปวง มาก ขนาด ไหน   แทนที่ บรรดา ประเทศ น้อย ใหญ่ ใน โลก นี้ จะ ร่วมกัน ขจัด อาวุธ เหล่านั้น ให้ หมด ไป     ทว่า   ยังคง มี ประเทศ ต่างๆ อย่างน้อย   8   ประเทศ   ได้แก่   สหรัฐอเมริกา   อังกฤษ   รัสเซีย   ฝรั่งเศส   เกาหลีเหนือ   จีน   อินเดีย   และ   ปากีสถาน   ( อาจ รวมทั้ง อิหร่าน )   ยังคง ครอบครอง อาวุธ นิวเคลียร์ และ ยังคง พัฒนา อาวุธ ชนิด นี้ อยู่   และ ที่ สำคัญ อาวุธ นิวเคลียร์ ที่ มี อยู่ ใน ขณะนี้ นั้น มี อานุภาพ ที่ ร้าย กว่า ปรมาณู ที่ ถล่ม ฮิ โร ชิ มา เสีย อีก   \n","   \n","       \n","   \n","   ฮิ โร ชิ มา   60   ปี   ไม่ได้ เตือนใจ ให้ กับ บรรดา ชาวโลก ทั้งหลาย เลย หรือว่า   ความรุนแรง เพียง หนึ่ง ครั้ง ที่ ใช้ ไป นั้น สร้าง ผลกระทบ ให้ เกิด แก่ มนุษยชาติ นาน กว่า   1   ชั่วคน   แล้ว ยัง คงจะ ใช้ ความรุนแรง และ พัฒนา อาวุธ เพื่อ ทำลายล้าง โลก กัน อีกต่อไป หรือ   \n","   \n","       \n","   \n","   ขอ ไว้อาลัย แด่   ผู้ ที่ ต้อง ตกเป็น เหยื่อ สงคราม และ เหยื่อ ความรุนแรง ทุก คนใน โลก   และ หวัง ว่า นก กระเรียน ของ ซา ดา โกะ จะ ได้ ดลใจ ให้ ผู้คน ที่ ใฝ่หา สันติภาพ ได้รับ สันติภาพ ใน เร็ว วัน   \n","  ,xxbos                                           \n","   \n","                                                       ภาพ จาก   xxurl   [ 1 ]   \n","   \n","       \n","   \n","   ประชา ไท  9   ก.พ.   2549   เอเอฟพี   -   พม่า   ประเทศ ที่ มี การ ปลูก ฝิ่น มาก ที่สุด เป็น อันดับ สอง ของ โล กรอง จาก อัฟกานิสถาน   ได้ ทำลาย พื้นที่ ปลูก ฝิ่น ในประเทศ ให้ ลดลง ได้ ถึง กว่า   60 %   จาก จำนวน เดิม เมื่อ   4   ปี ที่ผ่านมา   \n","   \n","       \n","   \n","   เดอะ   วี คลี่ ย์   เจอร์นัล   สิ่งพิมพ์ xxunk ของ พม่า รายงาน ว่า   ใน ปี   2005   พื้นที่ ปลูก ฝิ่น ของ พม่า อยู่ ที่   xxunk   เฮ ค ตา ร์   ( xxunk   ไร่ )   ซึ่ง ลดลง จาก ปี   2002   ถึง   60 %   และ ผลผลิต ฝิ่น ก็ ลดลง ถึง   61 %   อยู่ ที่   319   ตัน   \n","   \n","       \n","   \n","   พัน ตำรวจ เอก   xxunk   อ่อง   กล่าว กับ   วี คลี ย์   เจอร์นัล ว่า   พม่า ได้ ทำงาน ร่วมกัน กับ จีน และ อินเดีย ได้การ จัดการ กับ การ ลักลอบ ค้า ยาเสพติด   แต่ ทั้งนี้ทั้งนั้น ทาง พม่า เอง ก็ ยัง ต้องการ ความร่วมมือ จาก นานาชาติ ใน การ สร้าง มาตรการ ใน การป้องกัน และ ควบคุม ยาเสพติด ในประเทศ ด้วย   \n","   \n","       \n","   \n","   รายงาน การสำรวจ ประจำปี ของ สำนักงาน ว่าด้วย อาชญากรรม และ ยาเสพติด   องค์การสหประชาชาติ   ( xxunk )   ใน เดือน พฤศจิกายน เปิดเผย ว่า   พื้นที่ เพราะ ปลูก ฝิ่น ของ พม่า ได้ ลดลง อย่างต่อเนื่อง นับตั้งแต่ ปี   1996   เป็นต้นมา   ซึ่ง ในขณะนั้น มี พื้น ที่อยู่ ถึง   xxunk   เฮ ค ตา ร์   ( 850,000   ไร่ )   ที่ ใช้ ใน การ ปลูก พืช ที่ ผิดกฎหมาย   และ ใน ปี   2005   พม่า มี พื้นที่เพาะปลูก ฝิ่น อยู่ เพียง   xxunk   เฮ ค ตา ร์   \n","   \n","       \n","   \n","   สื่อ จาก ทางการ พม่า รายงาน เมื่อ เดือน ที่ผ่านมา ว่า   พม่า ได้ จับกุม ผู้ ที่ มีส่วน เกี่ยวข้อง กับ ยาเสพติด ถึง   xxunk   ราย   ใน ปี   2005   และ รัฐบาล ได้ ยึด ยาเสพติด ชนิด สาร กระตุ้น ได้ ถึง   xxunk   ล้าน เม็ด ใน ช่วงเวลา เดียวกัน   \n","   \n","       \n","   \n","   ทั้งนี้   พม่า ติด อยู่ ใน กลุ่ม ประเทศ ที่ มี การผลิต และ ค้า ยาบ้า มาก ที่สุด ใน โลก อีก ประเทศ หนึ่ง ได้ เปิดเผย ว่า มีแผน ที่จะ ทำลาย การผลิต ลง ให้ ได้ ภายใน ปี   xxunk   \n","   \n","       \n","   \n","   -xxrep 40   \n","   \n","   ที่มา   :   http :   /   /   xxurl   /   news   /   7598 _ xxunk htm   [ 2 ]   \n","   \n","       \n","   \n","       \n","   \n","  ,xxbos   ‘โถ่เรบอ’   นักเขียน เรื่องสั้น ป วา เก่ อญอ   เจ้า ของรางวัล ลูกโลก สี เขียว   ประเภท งานเขียน ปี   2550   กับ มุมมอง ความคิด ของ เขา ที่ สนทนา ว่าด้วย เรื่อง วิถี ชน เผ่า   ความฝัน   เอ็นจีโอ   ครู ดอย   และ   ‘เช วา ตัว สุดท้าย xxunk   \n","       \n","       \n","       \n","   “ ยุค นี้   มัน เหมือนกับ ว่า เรา จะ ต้อง มีความรู้ อีก ด้าน หนึ่ง   คือ ความรู้ เกี่ยวกับ จิตวิญญาณ   \n","   การ เป็นอยู่   การ ใช้ชีวิต   กลายเป็น เรื่องสำคัญ   \n","   ณ   วันนี้   แม้แต่ ตัว ผม ก็ ยัง ทำ ไม่ได้ เลย   \n","   คือ เหมือน เรา ต้องตาม ให้ เท่า ทันโลก ของ ทุนนิยม ”   \n","       \n","   “ผม ชัดเจน ใน การเมือง ของ ประชาชน ...   \n","   คือ เริ่ม เห็น ความสำคัญ แล้ว ว่า   การเมือง มัน ต้อง มาจาก ประชาชน   โดย ประชาชน จริงๆ ”   \n","       \n","   “ คือ ความคิด ของ ผม   มองว่า ... คุณ ไม่ จำเป็นต้อง เป็น เอ็นจีโอ   \n","   คือ เป็น อะไร ก็ได้   คือ อย่าง เรา อยู่ แบบ ชาวบ้าน   \n","   เรา จะ ต้อง ทันโลก   ทัน ใครก็ได้ ที่ เข้ามา   \n","   ถ้า จะ เข้ามา เอาเปรียบ   เรา จะ ต้อง ทัน ”   \n","       \n","   “ เรื่อง xxunk เขา ควาย   ตอน เด็ก ช่วง หน้าหนาว ก่อน จะ เกี่ยวข้าว   \n","   แล้ว ใน ลาย ที่ เขา ควาย แต่ละ เสียง   คน จะ เป่า ต้อง ฝึก   \n","   เป็นการ บอก โมง ยาม   บอก เวลา   บอก ทิศทาง   ซึ่ง   ณ   วันนี้ มัน ไม่มี แล้ว   \n","   แต่ละคน ก็ โหยหา   เนื่องจากว่า   ณ   วันนี้ ไป ไร่ ไป นา   ไป ด้วย รถมอเตอร์ไซค์ กัน แล้ว ”   \n","       \n","       \n","   \n","   \n","       \n","   ... ใน ปัจจุบัน เป็นที่ยอมรับ ว่า รูปแบบ วรรณกรรม ได้ เปลี่ยนไป ตาม ยุคสมัย   และ วัน เวลา ที่ แปรเปลี่ยน   รวมถึง สภาพ ทางสังคม ที่ เปลี่ยนแปลง ก็ นับว่า เป็น ปัจจัย หนึ่ง ที่ ทำให้ วรรณกรรม ที่ แสดงถึง ความ เป็น ท้องถิ่น มี ความเป็นสากล มากขึ้น   และ ถูก รับใช้ โดย เป็น กระบอกเสียง ให้ ผู้ ที่อยู่ ใน สังคม นั้น   โดย ปัจจุบัน รูปแบบ ที่จะ พบเห็น มาก ที่สุด คือ   “ เรื่องสั้น ”   ( short   story ) ซึ่ง เป็น รูปแบบ วรรณกรรม ทางตะวันตก ที่ ได้รับ การ ปรุงแต่ง อย่าง พอเหมาะ จาก นักเขียน   ใน การ สื่อ เรื่องราว ใน สังคม ของ ตน ให้ คนนอก สังคม ได้ เข้าถึง และ เข้าใจ มากยิ่งขึ้น   \n","       \n","   xxunk xxunk สีเหลือง ” หรือ xxunk   เป็น นามปากกา ของ   xxunk ศักดิ์   xxunk ’   นักเขียน สายเลือด   ป วา เก่ อญอ   ผู้ มี ผลงาน ผ่าน ตา ตาม หน้า นิตยสาร และ หนังสือพิมพ์ มา นาน พอควร   แต่   ‘เช วา ตัว สุดท้าย ’   เป็น รวม เรื่องสั้น เล่ม แรก ของ นักเขียน หนุ่ม ผู้ นี้   ‘โถ่เรบอ’   ได้ ใช้   ‘ เรื่องสั้น ’   ใน การ สื่อ สะท้อน ภาพ วิถีชีวิต ชาว ป วา เก่ อญอ   เพื่อ แสดงให้เห็น ถึง เรื่องราว ของ xxunk ที่ คน xxunk เข้าใจผิด ใน หลากหลาย เรื่อง นั้น   แท้จริง เป็น เช่นไร   โดย ผ่าน   10   เรื่องสั้น   และ อีก   1   ความรู้สึก ของ นักเขียน รุ่น ใหญ่   “ แสง ดาว   ศรัทธา มั่น ”   ผู้ เป็น แรงบันดาลใจ ของ   xxunk xxunk สีเหลือง ”   ที่ ทำให้ มือ ที่ เคย จับ xxunk เสียม เปลี่ยน มา จับ ปากกา เขียน เรื่องราว ชีวิต ของ ตน ให้ สังคม โลก ได้ รู้จัก   และ เข้าใจ วิถีชีวิต ของ ชาว ป วา เก่ อญอ มากขึ้น               \n","       \n","   ใน ความ เป็น เรื่องสั้น นั้น   แต่ละ เรื่อง ล้วน มี ความเป็นอิสระ ใน ตัวเอง   แต่ ใน หนังสือ   ‘เช วา ตัว สุดท้าย ’ ได้ ร้อย เรียง แต่ละ เรื่องสั้น ให้ ผู้อ่าน ค่อยๆ   ซึมซับ วัฒนธรรม ที่ มี กระบวนการ เป็นตัวของตัวเอง   และ มี ลักษณะ เป็น นามธรรม ของ ชาว ป กา เก่ อญอ อย่างแท้จริง ซึ่ง เป็น แนวคิด   ( theme )   หลัก ของ รวม เรื่องสั้น ชุด นี้ …   \n","       \n","       \n","                                                                                         xxunk พิณ ประภา   ขัน xxunk   \n","       \n","   บาง ตอน จาก   รายงาน   :   xxunk วา ตัว สุดท้าย \"   ความขมขื่น อัน xxunk ใน ความเปลี่ยนแปลง ที่ เป็นจริง   \n","                                                       http :   /   /   xxurl   /   journal   /   2008   /   01   /   xxunk   \n","       \n","   \n","       \n","       \n","       \n","       \n","       \n","   ‘โถ่เรบอ’   \n","       \n","       \n","       \n","       \n","       \n","   - ๑ -   \n","       \n","   เด็ก ดอย กับ ความฝัน เล็ก ๆ   \n","       \n","   เมื่อ ย้อนกลับ ไปดู ฉาก ชีวิต ของ คุณ ใน วัยเด็ก   เหมือนกับ เด็ก ดอย อีก หลาย คน มั้ย   กับ ความฝัน อยาก บิน ออกจาก ป่า มา อยู่ ใน เมือง ?   \n","   ความใฝ่ฝัน ของ ผม จริง   ๆ   นี่ มัน ออก อาการ มา ตั้งแต่ ตอน เรียนหนังสือ อยู่ บน ดอย แล้ว   วิชา ที่ ผม ชอบ คือ วิชา สังคมศึกษา   ก็ คิดดู   เด็ก บน ดอย   พอ จบ ม. 3   มา แบบ ช่วง หัวเลี้ยวหัวต่อ ไม่มีใคร แนะนำ   ตอนนั้น   xxunk รถ ถีบ ให้ คัน หนึ่ง   ผม ก็ รื้อ ออกมา ซ่อม   เดี๋ยว ก็ แกะ   เดี๋ยว ก็ เสีย   พี่ เขา เห็น อาการ แปลก ๆ   แบบนี้ ก็ เลย ส่ง ไป เรียน ช่าง ใน เมือง เชียงใหม่   \n","                 \n","   เรียนจบ ชั้น ไหน เหรอ   แล้ว xxunk กี่ ปี ถึง ไป เรียนต่อ   ?    \n","   จบ   ปวส. แต่ จบ แล้ว ความมั่นใจ มัน ไม่มี เลย   ไม่รู้ ว่า จะ ซ่อม รถ   ซ่อม อะไร   คือ พอ จบ ปุ๊บ ก็ ไป สมัคร ทำงาน เป็น xxunk อยู่ ตาม บริษัท   แต่ ก็ มา คิด ว่า   xxunk ผม ก็ ไม่ รู้เรื่อง รู้ ราว อะไร   แกะ รถ เสีย เป็น คัน ๆ   ซ่อม ไม่ได้   ช่วง ฝึกงาน นี่ บางที ก็ อยู่ บน ดอย   ความมั่นใจ มัน มี น้อยกว่า คน ข้างล่าง อยู่ แล้ว   ก็ กลัว ซ่อม รถ เขา เสีย   จะ เอา เงิน ที่ไหน ไป ใช้คืน เขา   ก็ เลย ไป สอบ เรียนต่อ ที่ ราช ภัฎ อุตรดิตถ์   \n","       \n","   ตอนนั้น คิด อยาก จะ เป็น ครู เหรอ   ?   \n","   ไม่ ได้คิด อะไร นะ   ก็ เรียน ตาม แม่ บอก   เพราะ แม่ อยาก ให้ เรียน   ความคิด ผู้ใหญ่ อาจจะ ดี   หวังดี กับ ลูก   ก็ เลย เรียน   เรียน ให้ มัน จบ   \n","                 \n","   เรียนจบ แล้ว กลับมา เชียงใหม่ เลย   xxunk   \n","   จบ แล้ว ผม ไป สกลนคร   ไปเที่ยว   แต่ ช่วง ที่ ผม อยู่ อุตรดิตถ์ นี่ ก็ดี นะ   ได้ เริ่ม อ่านหนังสือ มากขึ้น   แต่ ก็ อ่าน มั่ว   ยัง ไม่รู้ ว่า อ่าน ยังไง   ไป ฝึกสอน อยู่   จ. น่าน   ไป แถว บ่อ เกลือ   เสาร์ อาทิตย์ ก็ ขี่ xxunk เที่ยว ไป   ก็ ไม่รู้   ถ้า มอง ย้อน ไป ช่วง นั้น   ปี   2535   ผม อาจจะ ตาย ช่วง พฤษภาทมิฬ ก็ เป็นได้   เพราะ จำได้ ว่า ผม เกือบ นั่ง รถไฟ ไป ร่วม ชุมนุม กับ เขา เหมือนกัน   ตอนนั้น อยู่ เชียงใหม่ กลับมา เชียงใหม่ แล้ว   ไป เป็น เด็ก xxunk อยู่ แถว สถานีรถไฟ   \n","                 \n","       \n","       \n","   - ๒ -   \n","       \n","   เอ็นจีโอ - ชน เผ่า - เอ็นจีโอ - ชน เผ่า   \n","       \n","                 \n","   แล้ว มา เข้าทำงาน กับ เอ็นจีโอ ได้ ยังไง   ?   \n","   เริ่ม ตั้งแต่ ไป เจอ คนรู้จัก กัน ชื่อ บุญ เพิ่ม   ฤทัย xxunk   เขา เคย ทำงาน ร่วมกับ อาจารย์ ช ยันต์   xxunk ภูติ ใน สถาบันวิจัย สังคม   มหาวิทยาลัยเชียงใหม่ ในขณะนั้น   แต่ ก็ เป็น ส่วนหนึ่ง   ความคิด นี้ ก็ เริ่ม มี มา นิดๆ   พอ มา ที่ บ้าน ก็ เริ่ม มีปัญหา เรื่อง ป่าสน วัด จันทร์   \n","                 \n","   ช่วง ปัญหา   ออ ป. จะ ตัด โค่น ป่าสน วัด จันทร์ นั้น คุณ อยู่ ที่ไหน   ?   \n","   ผม อยู่ เชียงใหม่ แล้ว   แต่ ก็ รับ รู้เรื่อง ป่าสน วัด จันทร์ มา ตั้งแต่ ยัง เด็ก แล้ว   ตั้งแต่ ถนน เข้า มาถึง   ชาวบ้าน เริ่ม ประชุม กัน แล้ว   จำได้ ว่า ตอนนั้น ฟัง คำ ไทย   ภาษาไทย รู้ บ้าง   ไม่รู้ บ้าง   ไป ล้อมวง ฟัง ชาวบ้าน   เด็ก ๆ ไป กัน หมด   แต่ รุ่น คน เฒ่า คนแก่ ใน ยุค นั้น นี่ สู้ กัน ดุเดือด   พูด ไทย ก็ ไม่ได้   พูด คำเมือง ก็ ไม่ได้   ถกเถียง กัน อย่าง เครียด เลย แหละ   \n","       \n","   ตอนนั้น เจ้าหน้าที่ รัฐ เข้ามา ข่มขู่ ด้วย หรือเปล่า ?   \n","   ข่มขู่ สิ   จำได้ ตอน เป็น เด็ก   ที่ โบสถ์ บ้าน หนอง เจ็ด หน่วย   มี การ เถียง กัน ไป เถียง กัน มา   ไม่รู้ จะ ลงเอย ยังไง   ชาวบ้าน จึง พา กัน ปิด ถนน   เพราะ ถนน ก็ เพิ่ง มี มาถึง   ก็ ไม่ ให้ ตัด ไม้สน   ตัด ทำไม   ไม่ ให้ ทำ   ก็ ปิด ถนน   ปิด โรงเรียน   xxunk คน หนึ่ง ก็ บอก ปิด ก็ ปิด สิ   \n","                 \n","   การ เข้ามา อยู่ ใน เครือข่าย เกษตรกร ภาคเหนือ   xxunk   ตอนนั้น ได้ ทำ อะไร   เคลื่อนไหว อะไร บ้าง   ?   \n","   ก็ มีประสบการณ์ เพียบ เลย   ที่จริง ที่ ผม ย้าย มา อยู่   ค กน. ช่วง ที่ ไป เป็น อาสาสมัคร อยู่ กับ พี่ สมบัติ   ( บุญ คำ xxunk )   ได้ อะไร หลายอย่าง เลย นะ   อัด ความคิด ให้ แต่ละ อย่าง นี่ หลาย คนรับ ไม่ได้ ก็ เป๋ ไป เหมือนกัน   แต่ เรา กลับ ชอบ ความคิด แบบนี้   เหมือน ที่ เรา ใฝ่ฝัน มา นาน   สิ่ง ที่ เคย คิด ที่ เคย xxunk อยู่ ใน สมอง   พอ ไป อยู่ กับ เขา ได้ วิเคราะห์ สังคม   วิเคราะห์ แต่ละ อย่าง   และ หนังสือ แต่ละ เล่ม ที่ แก ให้ อ่าน   หนังสือ มัน ก็ พูด ถูก อย่าง ที่ เรา เคย คิด ไว้   เคย รู้สึก ว่า มัน น่าจะเป็น อย่างนี้   หนังสือ ก็ พูด ถูก   ก็ เลย คิด ว่า น่าจะ มา ทำงาน ร่วมกับ   ค กน.   แล้วก็ อาจจะ เป็น เพราะ จังหวะ เหมาะ ด้วย   จังหวะ สถานการณ์ ชาวบ้าน กำลัง เคลื่อนไหว   \n","       \n","   ชาวบ้าน เคลื่อนไหว เรียกร้อง อะไร หรือ   ?   \n","   อพยพ ไง   รัฐ จะ อพยพ คน ออกจาก ป่า   จะ ไล่ คน ออกจาก ป่า   ชาวบ้าน จึง เริ่ม มี การเคลื่อนไหว   ครั้งแรก   น่าจะ ประมาณ   ปี   2537   ตอนนั้น ยัง เรียน อยู่   พอ กลับมา ตื่นเต้น มาก   ถาม ตัวเอง ว่า ทำไม ชาวบ้าน ถึง มา กัน เยอะ ขนาด นี้   แต่ ไม่แน่ใจ นะ   ก็ เลย ขี่ รถ ไปหา พี่สาว   พี่สาว ก็ บอ กว่า   “ มึง นี่ ไม่ รู้เรื่อง รู้ ราว   ชาวบ้าน เขา เดือดร้อน จะ ตาย กัน แล้ว ”   มัน ก็ เหมือน มี เชื้อไฟ อยู่ ข้างใน อยู่ แล้ว นิดๆ   ก็ ตัดสินใจ ไป อยู่ ร่วมกับ   ค กน. ( เครือข่าย เกษตรกร ภาคเหนือ )   อยู่   ค กน.   ตอนนั้น   สนุก นะ   มี การทำงาน รณรงค์ เรื่อง รัฐธรรมนูญ   คือ ทำ เหมือนกับ เป็น กิจกรรม เลย     \n","       \n","   แสดงว่า ตอนนั้น   ความคิด เรื่อง สังคม   การเมือง เริ่ม ชัด แล้ว   ?   \n","   ชัดเจน   ผม ชัดเจน ใน การเมือง ของ ประชาชน   เพราะ ระบบ มัน ไม่ได้ อิง พรรค   เข้าใจ   คือ พรรคการเมือง มัน ก็ อีก อย่างหนึ่ง   ก็ เลย ชัดเจน   คือ เริ่ม เห็น ความสำคัญ แล้ว ว่า   การเมือง มัน ต้อง มาจาก ประชาชน   โดย ประชาชน จริงๆ   ซึ่ง ใน การ ที่ ผม เข้าไป ทำงาน เคลื่อนไหว ตรงนี้   ผม มอง กลุ่มคน ที่ทำงาน กับ ชาวบ้าน   เป็น ความคาดหวัง   ผม เข้าใจ ว่า   กลุ่ม นี้ เป็น คน ที่ ปฏิวัติ สังคม   สามารถ สู้ เพื่อ ชาวบ้าน   คน ทุกข์ คน ยาก   ผม เลย โดด เข้าไป ทำ เต็มที่ เลย   \n","       \n","   ช่วง นั้น   มอง ปัญหา ความ xxunk ล้ำ xxunk พันธ์ กับ สังคม ไทย อย่างไรบ้าง   ?   \n","   คือ มัน ก็ สะสม มา ตั้งแต่ เรียน   คือ ศักดิ์ศรี ของ เรา   ความจริง ก็ ไม่ อยาก ใช้ คำ ว่า ศักดิ์ศรี หรอก นะ   แต่ ความเสมอภาค มัน น่าจะ มี ใน ความ เป็น คน   ใน เมื่อ เรา อยู่ ใน กลุ่มคน   \n","       \n","   พอ มา อยู่ ช่วงหลัง ๆ   คุณ มอง เห็นภาพ เอ็นจีโอ เป็น ยังไง บ้าง   ?   \n","   ขัดแย้ง สิ   คือ ช่วง ที่ ผม ไป อยู่   ค กน. มัน เหมือนกับ เป็นช่วง ที่ กำลังจะ ขยับ   คือ ใน รุ่น ผม เนี่ย   คน ที่ เรียนหนังสือ มัน ก็ จะ มี ผม คนเดียว ไง   แล้ว โดย xxunk จะ มี พร้อม   มี หนังสือรับรอง   ก็ ขยับ ทำงาน เป็น เอ็นจีโอ เต็มตัว   แต่ พอ มอง เอ็นจีโอ   ให้ เป็น เอ็นจีโอ   มัน ก็ เริ่ม รู้สึก ไม่ สนุก แล้ว   \n","       \n","   หมาย ความ ว่า   เริ่ม เห็นภาพ ความขัดแย้ง กันเอง อย่างนั้น ใช่ไหม   ว่า บ้าง ครั้ง มัน เข้าไป ไม่ ถึง   ?   \n","   มัน เป็นเรื่อง ของ สำนึก ไง   สำนึก แบบ ถ้า จะ เป็น เอ็นจีโอ ก็ จะ เหมือนกับ เรา   จะ ต้อง ลอยตัว ออก ไป จาก ชาวบ้าน ไป แล้ว   ความเป็นอยู่ วิถีชีวิต ก็ จะ ต้อง เปลี่ยนแปลง แบบ หนึ่ง   \n","       \n","   กำลังจะ บอ กว่า   ถ้า เป็น เอ็นจีโอ จริง   ต้อง มา xxunk กับ ชาวบ้าน   ไม่ใช่ แยกส่วน แบบนี้ เหรอ   ?   \n","   คือ ความคิด ของ ผม   มองว่า   วิถีชีวิต สอง ด้าน   คือ คุณ ไม่ จำเป็นต้อง เป็น เอ็นจีโอ   คือ เป็น อะไร ก็ได้   คือ อย่าง เรา อยู่ แบบ ชาวบ้าน   เรา จะ ต้อง ทันโลก   ทัน ใครก็ได้ ที่ เข้ามา   ถ้า จะ เข้ามา เอาเปรียบ   เรา จะ ต้อง ทัน   \n","       \n","   ความจริง ช่วง นั้น   ผม มี ความคิด ว่า   ไม่จำเป็น   ถ้า เกิด ว่า มี กรณี ปัญหา อะไร เกิดขึ้น   ไม่จำเป็น ว่า ชาวบ้าน หรือว่า ใคร จะ ต้อง ไป แถลงข่าว   คือ ทุกคน ที่อยู่ ใน หมู่บ้าน สามารถ ที่จะ มี กำลัง เพียงพอ ที่จะ ทำ อะไร ด้วย ตัว ของ ตัวเอง ก็ได้   อย่าง เกิดเรื่อง ไม่เป็นธรรม อะไร ขึ้น มา   ก็ สามารถ รวมตัวกัน ขึ้น มา   ต่อรอง   ติดต่อ   ประสานงาน   จะ แถลงข่าว สามารถ ติดต่อ นักข่าว ได้   ไม่ จำเป็นต้อง อาศัย นักวิชาการ   เอ็นจีโอ   มา xxunk   แล้ว เอา ชาวบ้าน มา ยืน ข้างหลัง   ซึ่ง ชาวบ้าน เรา สามารถ แม้แต่ จะ พบ นายกรัฐมนตรี   ชาวบ้าน สามารถ บอก ปัญหา ด้วยตัวเอง ได้   \n","       \n","   เมื่อก่อนนี้   ผม ก็ เป็น นัก กิจกรรม คน หนึ่ง เหมือนกัน นะ   อยู่ บ้าน ก็ จัดงาน   เอา นู้น เอา นี่ มา ทำ   คิด อีกที   ผม เหนื่อย แทบตาย   คน ที่ ได้ นี่ คือ คน ที่ ขายของ   เป้าหมาย ที่ เรา ทำ ที่ มัน เหลือ นี่   เรา ได้ นิดเดียว   เรา เหนื่อย   ดีไม่ดี คน ที่ ค้าขาย นี่ ค้าน เรา เสีย ด้วยซ้ำ   ค้าน แนวคิด เรา   \n","       \n","   อย่าง การ ระดม พล   ระดม ชาวบ้าน   ผม ก็ เลย คิด ว่า   การ จะ ชุมนุม นี่   ผม ก็ มา แล้ว นะ   ชุมนุม ใน ศาลากลางจังหวัด   คน บน ดอย   อยู่ บน ดอย นี่ ก็ ทุกข์ยาก   ลำบาก กัน อยู่ แล้ว   ใคร ลงมา ก็ พก ตังค์ มา คนละ ร้อย สอง ร้อย   มอง ในแง่ เศรษฐกิจ ต้อง เอา มา ละลาย   หลายครั้ง แล้ว ที่ ผม ไม่เห็นด้วย กับ การ ชุมนุม   การ เอา ชาวบ้าน มา ชุมนุม   สงสาร ชาวบ้าน ที่ รวบรวม เงิน กัน ลงมา   บางที ความคิด ผม แบบนี้   ก็ สวนทาง   คุย กับ ใคร ไม่ ค่อย ได้ ( สีหน้า เคร่งเครียด )   \n","       \n","   จุด นี้ เลย ทำให้ เบื่อ ใช่ มั้ย   ?   \n","   มัน ก็ เบื่อ   หรือ อะไร ไม่รู้   ก็ ดู แล้ว มัน ไม่ เป็น ดั่ง ใจ นะ   \n","       \n","   ก็ เลย ออกจาก เอ็นจีโอ   ?   \n","   ก็ เป็น ส่วนหนึ่ง   แล้วก็ คิด ว่า อยู่ ไป ก็ ไม่ มีความสุข   กลับ ไป บ้าน ดีกว่า   ที่นา ก็ มี อยู่ ห้า หก ไร่   ไม่มีใคร ทำ หลาย ปี แล้ว   \n","                 \n","       \n","   - ๓ -   \n","       \n","   เมื่อ นก ป่า หวนคืน สู่ ป่า   กลับ ไป เป็น ครู ดอย   \n","       \n","       \n","   แล้ว คิด ยังไง ถึง ตัดสิน ไป เป็น ครู ดอย   มี แรงบันดาลใจ อะไร   ?   \n","   ก็ ไม่มี อะไร   ก็ ถ้า จะ เป็น เอ็นจีโอ   ก็ ขี้เกียจ ( หัวเราะ )   คือ ผม มองว่า การศึกษา ระบบ   กศน. ที่ ไป อยู่ ใน หมู่บ้าน เนี่ย   เรื่อง หลักสูตร เรื่อง อะไร พวก นี้ จะ กลาย เป็นเรื่อง รอง ไป เลย นะ   แต่ ตัว ครู ที่ ไป อยู่ นี่ จะ สำคัญ กว่า   คือ ชาวบ้าน จะ ได้รับ ผล   ได้ มาก ได้ น้อย   ก็ ขึ้นอยู่กับ ครู   เพราะ ครู จะ เป็น ตัวกระตุ้น   \n","                 \n","   หลักสูตร ที่ ใช้ สอน เด็ก บน ดอย มัน สอดคล้องกัน ไหม   ยัง อิง กับ หลักสูตร ส่วนกลาง บ้าง มั้ย   ?   \n","   เป็น หลักสูตร ที่ ปรับปรุง แล้ว นะ   คือ ความยืดหยุ่น มัน สูง ไง   จะ อิง เฉพาะ หลักสูตร มัน ไม่ได้   คือ   กศน.   อย่าง ที่ ผม อยู่ ใน เขต พื้นที่   อ. แม่ แจ่ม   บาง หมู่บ้าน ที่ มัน จะ ต้อง ดูแล เด็ก   ผม อยู่ กับ เด็ก   มัน ไม่ สนุก   ยังไง ไม่รู้   คือ มัน ไม่มี ศูนย์ เด็กเล็ก ไง   ก็ ต้อง อยู่ กับ เด็กเล็ก   ทิ้ง ก็ ไม่ได้   ก็ จำเป็นต้อง อยู่   แล้ว งาน   กศน. มัน มี เยอะ ด้วย บางที ก็ ลง ไป ข้างล่าง   บางที ก็ ขึ้น มา   บางครั้ง ถ้า ความรับผิดชอบ ของ เรา มัน ไม่มี   มัน ก็ จะ xxunk ๆ   \n","                 \n","   เคย มอง ไหม ว่า   การศึกษา ที่ เรา เข้าไป ส่งเสริม   ไป สอน ให้ กับ ชุมชน บน ดอย นั้น   มัน อาจ กลายเป็น ตัวเร่ง   ตัว ฉุด วิถีชีวิต ของ ชาวบ้าน ให้ เปลี่ยนแปลง เร็ว ขึ้น   ?   \n","   สังคม มัน ไป ไว ไง   แม้แต่ บน ดอย   ชาวบ้าน คลื่น มือถือ มี ที่ไหน   เขา ก็ รู้   มัน โยง เป็นระบบ เลย   ที่ ผม ดูๆ   นะ   แล้ว การ เคลื่อนตัว ของ เศรษฐกิจ   แล้วก็ ใน สังคม ชุมชน ระหว่าง เมือง กับ บน ดอย มัน เคลื่อนตัว ไว   แล้ว ถ้า มอง เรื่อง การศึกษา   มัน ก็ เหมือนกับ ว่า   ชาวบ้าน ใน เมื่อ ได้รับ บทเรียน แล้ วจะ ต้อง ย้อน กลับมา ที่ เก่า อีกครั้ง   เหมือน ละทิ้ง สิ่ง เก่าๆ   ดั้งเดิม   ละทิ้ง วัฒนธรรม บางอย่าง   คือ เหมือน ตาม กระแส ทุนนิยม อะไร ที่ เข้าไป ซัก พัก หนึ่ง   แต่ ในที่สุด   เมื่อ เขา เห็น ปัญหา แล้ว   พวกเขา ถึง จะ กลับมา   \n","                 \n","   ตอนนี้ กระแส เริ่ม กลับมา หรือยัง   \n","   มัน ก็ ไม่เชิง   แต่ มัน ก็ วน   ๆ   อยู่ แถว นั้น   อย่าง เรา เห็น ป่า ถูก ทำลาย   ผม ก็ ไม่รู้ ว่า จะ พูด ยังไง   ยกตัวอย่าง ชาวบ้าน ทำไร่ ปลูก มันสำปะหลัง   คือ สิ่ง หนึ่ง ใน ฐานะ ที่ เรา เป็น ครู   เรา เห็น   เรา รู้   แต่ เรา พูด โดยตรง ไม่ได้   แต่ ผม ก็ จะ พยายาม บอก กับ ชาวบ้าน ว่า ... พ่อ ลุง   พื้นที่ ตรงนี้   ปี ที่แล้ว ปลูก มันสำปะหลัง ขาย ได้ เท่าไร   ถ้า เขา บอ กว่า ขาย มันสำปะหลัง ได้   หนึ่ง หมื่น บาท   ผม ก็ จะ บอก เขา บอก ค่าใช้จ่าย ทั้งหมด มา ว่า   ซื้อ ปุ๋ย เท่าไร   ยา เท่าไร   จด ๆ   มา ให้ หมด   ค่าแรง ที่ จ้าง ไป เท่าไหร่   สรุป แล้ว ลงทุน ไป เกือบ ห้า หมื่น   ปี หนึ่ง ไม่ได้ อะไร ซัก อย่าง   \n","       \n","   คือ ด้าน หนึ่ง   เรา ก็ พูด ไป   แต่ ด้าน หนึ่ง เรา กำลัง อยาก บอก ชาวบ้าน ว่า   สิ่ง ที่ คุณ ทำไร่ มันสำปะหลัง   เหมือน คุณ ทำ ฟรี ๆ   ให้ กับ บริษัท   คน ที่ ได้ นี่ ชาวบ้าน ไม่ได้ หรอก   นี่ เป็น บทสรุป   อีก อย่าง คุณ สูญเสีย พื้นที่ ป่า   อีก อย่าง สุขภาพ ตัวเอง ยัง ต้อง โดน ยาพิษ   โดน สารเคมี   โดน อะไร อีก   ผม ก็ พยายาม เสนอ แนวคิด ไป   \n","       \n","   หมาย ความ ว่า   จะ ให้ความรู้ เพียงแค่ ใน หลักสูตร นั้น ไม่ ก็ ไม่ พอแล้ว   ?   \n","   ใช่   เรื่อง หลักสูตร นี่ ผม ว่า ไม่พอ   เพราะ นี่ เป็นเรื่อง ของ นโยบาย รัฐ   หลักสูตร นี่ ผม ว่า ถึง เรา จะ สอน   เรา ก็ ต้อง สอน เน้น ใน เรื่อง จิตสำนึก   \n","                 \n","   ทำ อย่างไร เรา ถึง จะ ให้ ชาวบ้าน รู้เท่าทัน ทุนนิยม   เท่าทัน สังคม   และ เทคโนโลยี ที่ มัน กำลัง พุ่ง ขึ้นไป บน ดอย ?   \n","   มัน เป็น อย่างนี้   คือ เรา ไม่ใช่ ว่า เรา จะ ผลัก ภาระ ให้ ครู อย่าง เดียว   ไม่ใช่ เฉพาะ ครู   กศน. แม้แต่ ใน มหาวิทยาลัย มัน ก็ ไม่ได้ สอน ใน เรื่อง เหล่านี้   คือ ใคร จบ เกษตร มา   ส่วนใหญ่   ก็ เหมือน ผลิต คน ไป ให้ บริษัท ยักษ์ ใหญ่ เลย   มี แต่ ฟาร์ม หมู   ฟาร์ม ไก่   พวก นี้ มัน คุม ระบบ ไป หมด เลย   \n","                 \n","   ยุค นี้   มัน เหมือนกับ ว่า เรา จะ ต้อง มีความรู้ อีก ด้าน หนึ่ง   คือ ความรู้ เกี่ยวกับ จิตวิญญาณ   การ เป็นอยู่   การ ใช้ชีวิต   กลายเป็น เรื่องสำคัญ   ณ   วันนี้   แม้แต่ ตัว ผม ก็ ยัง ทำ ไม่ได้ เลย   คือ เหมือน เรา ต้องตาม ให้ เท่า ทันโลก ของ ทุนนิยม   อย่าง มือถือ เดี๋ยวนี้ โปรโมชั่น เยอะ   เดือน ละ สาม ร้อย   โทร ฟรี   แต่ โทร ไป โทร มา ไม่ ฟรี ซัก หน่อย   คือ เขา คิด ใน เรื่อง ของ เศรษฐศาสตร์ หรือ ใน เรื่อง ของ กำไร   เรื่อง พวก นี้ เขา พร้อม อยู่ แล้ว   คุม เรา ไว้ หมด แล้ว   เหมือน เรา ไม่ใช่ ว่า เรา อวด ตัวเอง ว่า เรา ฉลาด มา ขนาด นี้   หรือว่า เรา รู้ มา ขนาด แล้ว เนี่ย   เรา ยัง ตก xxunk ของ มัน เลย   \n","       \n","       \n","       \n","       \n","   รวม เรื่องสั้น   ‘เช วา ตัว สุดท้าย ’   ประพันธ์ โดย   ‘โถ่เรบอ’   \n","   สำนักพิมพ์ สาน ใจ คนรัก ป่า   จัดพิมพ์   \n","       \n","       \n","       \n","       \n","   - ๔ -   \n","       \n","   xxunk   กับ งาน เรื่องสั้น ชุด   ‘เช วา ตัว สุดท้าย ’   \n","       \n","   ใน ฐานะ ที่ คุณ เป็น นักเขียน เรื่องสั้น ป วา เก่ xxunk แรก และ เล่ม แรก ใน ชีวิต   พอ จะ บอก ได้ มั้ย ว่า เนื้อหา เรื่องสั้น เล่ม นี้   มี เค้าโครง   ความคิด   ส่วนใหญ่ มัน มาจาก เรื่องจริง   ?   \n","   ใช่   จาก ชีวิตจริง ที่ ผม ประสบ มา ทั้งหมด   \n","       \n","   แล้ว มีความรู้สึก อย่างไร กับ เรื่องสั้น เรื่อง นี้   \n","   รู้สึก แบบ ไหน เหรอ   ก็ รู้สึก ดีใจ ที่ ได้ เผยแพร่ ออกมา   คือ ผม ต้องการ สื่อ ชีวิต ที่ มัน ล่มสลาย ไป ของ คน ป วา เก่ อญอ   \n","       \n","   คุณ พูด เหมือนกับ ว่า   วิถี มัน เปลี่ยนไป ถึงขั้น ทำให้ เห็นภาพ การ ล่มสลาย   ภาพ ของ โศกนาฏกรรม อย่างนั้น เลย หรือ   ?   \n","   ผม ว่า ทุก เรื่อง เลย ที่ ผ่าน ตา   ผ่าน ชีวิต ผม มา เป็น เช่นนั้น   \n","       \n","   ลอง เล่าเรื่อง   ‘เช วา ตัว สุดท้าย ’ ที่ เป็น ชื่อ ปก ให้ พอ เข้าใจ หน่อย ได้ มั้ย   ?   \n","   เรื่อง   ‘เช วา ตัว สุดท้าย ’   นะ ครับ   ที่ ผม ต้องการ สื่อ คือ   ผม มองว่า ใน สังคม ป วา เก่ อญอ เอง นั้น   เรื่อง การ กดขี่ สิทธิสตรี   มัน เป็นเรื่อง วัฒนธรรม ที่ บังคับ ผู้หญิง อย่างมาก   ลอง คิดดู อย่าง เรื่อง การ มี xxunk มัน ก็ เป็นเรื่อง ปกติ   แต่ ถ้า ผู้หญิง ป วา เก่ อญอ ยัง ไม่ได้ แต่งงาน ก็ จะ ต้อง อยู่ กับ เสื้อ เช xxunk ขาว ตัว นี้ ไป จนตาย   ลอง นึก ดูเหมือน ใน สังคม ป วา เก่ อญอ ต้อง ตรา อยู่ แล้ว   แต่ละ สังคม จะ ต้อง ฝัง อะไร บางอย่าง   ทั้งๆ ที่ สังคม ป วา เก่ อญอ นี่ ให้เกียรติ ผู้หญิง นะ   ถ้า ใน ความ เป็น ครอบครัว ผู้หญิง จะ เป็นใหญ่ ใน บ้าน เลย นะ   แต่ สำหรับ คน ที่ ไม่ได้ มีครอบครัว นี่   เธอ ยัง จำต้อง เป็น นางสาว จนตาย นี่   ผม ว่า   ชีวิต เธอ xxunk นะ   \n","                 \n","   แล้ว มี ทางออก ยังไง บ้าง   \n","   ไม่รู้ สิ   นอกจาก สังคม มัน จะ เปลี่ยนไป แล้ว   ผม ก็ ไม่รู้ ว่า ทางออก มัน จะ เปลี่ยนไป ยังไง นะ   แต่ ถ้า พูดถึง ใน ยุค สมัยนี้   ก็ เริ่ม จะ มี คน ได้ เรียนหนังสือ   บางที ผู้หญิง ไม่ แต่งงาน   ก็ สามารถ ขอ อยู่ ด้วยตัวเอง ก็ได้   เสื้อผ้า กู ไม่ ใส่ ก็ได้      \n","                 \n","   หรือว่า ความจริง แล้ว   จารีต แบบนี้   คน เฒ่า คนแก่ เขา สืบทอด มา   อาจ ต้องการ สื่อ ให้ เห็น ว่า ผู้หญิง   ป วา เก่ xxunk นั้น มี ความ xxunk   เขา ต้อง ให้ เห็น ตรงนี้ ก็ได้   ?   \n","   ผม ไม่แน่ใจ นะ   แต่ ใน ตำนาน บอ กว่า หญิง บริสุทธิ์ กลายเป็น เหล้า เป็น บุหรี่ เลย นะ   คือ ผู้หญิง ที่ ไม่ได้ แต่งงาน   หลังจากนั้น   มัน ก็ จะ เหมือน อาจจะ โดน สังคม xxunk   ถึง ไม่ได้ พูด   แต่ ก็ มอง ด้วย สายตา ที่ เหยียดหยาม   คน นี้ ที่ ตาย ไป ก็ xxunk   ก็ ขอให้ กลายเป็น เหล้า หรือ ยาสูบ   ใครๆ   ที่จะ สูบ ก็ จะ ต้อง ติด   ใครๆ   ที่ กิน เหล้า ก็ จะ ต้อง เมา   \n","                 \n","   ใน ตำนาน มี การ พูดถึง เรื่อง ผู้หญิง ที่ ไม่ได้ แต่งงาน แบบนี้ เลย เหรอ   ?   \n","   ใช่   เหมือน เธอ แค้น ไง   ก็ กลายเป็น ยาสูบ กับ เหล้า   พอให้ คน กิน   คน ก็ ติด   คน สูบ   คน ก็ ติด   ใครๆ   เห็น   ก็ ต้อง รัก   รัก ... เมา หัว xxunk หัว xxunk   เลย เหล้า เบียร์ นี้   \n","                 \n","   แล้ว ภาพรวม ของ หนังสือ เล่ม นี้ ล่ะ   \n","   คือ โดย ภาพรวม   ก็ อยาก จะ เล่าเรื่อง วิถีชีวิต ที่ ผม เคย เห็น ตอน เด็ก   ๆ   เรื่อง xxunk เขา ควาย ตอน เด็ก ช่วง หน้าหนาว ก่อน จะ เกี่ยวข้าว   แล้ว ใน ลาย ที่ เขา ควาย แต่ละ เสียง   คน จะ เป่า ต้อง ฝึก   แล้วก็ เป็นการ บอก โมง ยาม   บอก เวลา   บอก ทิศทาง   ซึ่ง   ณ   วันนี้ มัน ไม่มี แล้ว   แต่ละคน ก็ โหยหา   เนื่องจากว่า   ณ   วันนี้ ไป ไร่ ไป นา ไป ด้วย รถมอเตอร์ไซค์ กัน แล้ว   \n","                 \n","   ก็ มีเสียง ปี่ เขา ควาย   มีเสียง เต ห น่า   เต ห น่า นี่ ก็ เกือบจะ หาย ใน xxunk คน ป วา เก่ อญอ เลย นะ   ถ้า ไม่มี   xxunk โพ ’ ไว้ สัก คน   ตือ โพ นี่ สามารถ พลิกฟื้น   เหมือน xxunk เต ห น่า   ให้ ได้คืน มา   ผม ยอมรับ ใน อัจฉริยะ ใน ตัว เขา เหมือนกัน   \n","                 \n","   เรื่อง ส่วนใหญ่ เหมือนกับ กำลังจะ สื่อ ว่า   จริงๆ   แล้ว ชน เผ่า ป วา เก่ อญอ   มัน มี รากเหง้า   ?   \n","   คือ ใน กลุ่ม มัน ก็ จะ มี วิถีชีวิต   มีวัฒนธรรม   มี ประเพณี   มี จารีต อะไร เอาไว้ ตรงนั้น   แต่ว่า การดำรงชีวิต ใน สังคม เดี๋ยว   มัน ต้อง เจอ กับ หลาย สิ่ง หลายอย่าง จาก สังคม ภายนอก ที่ มัน เริ่ม ตี เข้ามา   เหมือนกับ การ พยายาม ที่จะ ยื้อ เพื่อที่จะ ให้ ดำรงอยู่ ของ ชน เผ่า   ยิ่ง ถ้า เป็น ในแง่ ของ มุมมอง ทัศนะ ก็ จะ เริ่ม เปลี่ยนไป เรื่อยๆ   อย่าง xxunk นี่ ก็ เกือบ อยู่ ไม่ได้ เหมือน อย่าง ที่ ผม เขียน เรื่อง   ‘ อพยพ ’   ชอบ   ที่จริง พะ xxunk พา ตอ พะ   ก็ เสีย ไป เมื่อไม่นานมานี้   ก็ เป็น อีก ตัวละคร หนึ่ง   \n","       \n","   แล้ว ทุกวันนี้   คิด จะ เขียน อีก ไหม   เขียน ใน มุม ของ ความ เปลี่ยน   นั่ง อยู่ นิ่ง ๆ   แล้ว เห็น ตัวละคร ผ่าน ไป   วัน ๆ   ?   \n","   อยาก เขียน   ก็ อยาก เขียน อยู่   แต่ บางครั้ง ก็ เหมือนกับ ว่า   ที่ ผม ไม่กล้า เขียน   ก็ เพราะ ผม ตี โจทย์ ของ ความ เปลี่ยน ของ สังคม ป วา เก่ อญอ ไม่ได้ ไง   ก็ เลย ไม่กล้า เขียน   ผม กลัว ถ้า ตี โจทย์ ไม่ แตก   สื่อ ไป แล้ว มัน ผิดพลาด ความเข้าใจ ของ คน ที่จะ อ่าน   เพราะ ทุกวันนี้   ผม มองว่า คน ที่ เขียน เรื่อง เขียน อะไร   ไม่มี ข้อมูล ใน เชิง ลึก อยู่ แล้ว เขียน แค่ ผ่าน   มัน ไม่ได้   การเขียน ต้อง รับผิดชอบ   ความรับผิดชอบ ตรงนี้ มัน ต้อง มี ไง   \n","                 \n","   นี่ หมาย ความ รวมถึง การเขียน เรื่องสั้น ด้วย ใช่ไหม   ?   \n","   ใช่   \n","       \n","   จะ จินตนาการ มั่ว ๆ   ก็ ไม่ได้ xxunk หรือ   ?   \n","   ผม เขียน เฉพาะเรื่อง เช วา ตัว สุดท้าย เรื่อง เดียว ผม ใช้เวลา คิด สาม ปี กว่า   เฉพาะ คิด อย่าง เดียว ถาม ข้อมูล ไป เรื่อยๆ   ผม ไม่ได้ ถาม หมู่บ้าน เดียว   เพราะว่า ข้อมูล เฉพาะที่ ผม เติบโต ใน หมู่บ้าน นี่ ที่ ผม เห็น กับ อีก หมู่บ้าน อื่น ที่ ไกล   ๆ   มัน เป็น อย่างนี้ จริง ไหม มัน ต้อง เช็ค ข้อมูล   \n","       \n","   แล้ว ต้องการ สื่อ ให้ คน ข้างนอก ได้ รับรู้   ?   \n","   ที่ การ เขียนหนังสือ ของ ผม ต้องการ สื่อ อยู่ แล้ว   อย่างน้อย ก็ ต่อไป อีก ซัก สิบ ปี   ยี่สิบ ปี   เมื่อ เด็ก ๆ   ป วา เก่ อญอ ใหม่ มา อ่าน   มา เจอ   \n","       \n","   บางที ... อาจจะ ไม่มี   หา ไม่ เจอ แล้ว   เช วา ตัว สุดท้าย ?   \n","   ใช่   น้อย มาก ที่จะ เหลือ   แม้แต่ อย่าง เพลง เต ห น่า ที่ ผม เขียน ไม่มี ให้ เห็น แล้ว นะ   ที่ แต่ละ ปี ที่จะ มี คน เฒ่า มา เล่านิทาน ใน หมู่บ้าน เหมือน งานประจำปี   xxunk ก็ กลายเป็น อีก แบบ หนึ่ง   กลายเป็น xxunk   งาน มี เวที   ดีไม่ดี ก็ มี การ เต้น xxunk xxunk   xxunk   มี หมด ละ   อบต. บน ดอย จัดงาน ที   ก็ เอา วง อิเล็ค โทน มา ซัก ตัว   นักร้อง ซัก คน   นัก เต้น อีก สี่ ห้า คน   xxunk กำนัน นี่ สนุก   อบต. นี่ สนุกสนาน ( หัวเราะ xxunk )   \n","       \n","   ใน หนังสือ มี เนื้อหา เกี่ยวกับ ความขัดแย้ง กับ รัฐ ด้วย ใช่ไหม   เรื่อง อพยพ   มาถึง ตอนนี้ ปัญหา นี้ ก็ ยัง ไม่ จบ   ยัง คาราคาซัง อยู่   คุณ มอง ยังไง   ?   \n","   ใน เชิง สังคม เหมือนกับ ว่า จะ คลี่คลาย ไป ใน ระดับ หนึ่ง   แต่ ในแง่ ของ กฎหมาย นี่   xxunk ผม พูด กับ พี่ ที่ บ้าน   ใน เรื่อง กฎหมาย ป่าไม้   ผม ไม่แน่ใจ   คือ ตอนนี้ บน ดอย   ถ้า ใคร อยาก ได้ที่ เหมือน ต้อง เคลียร์ ทิ้ง   ทั้งๆ ที่ เรา เสียดาย ต้นไม้   เรา จะ xxunk แบบ ไม่ ตัด ต้นไม้ ไม่ได้ หรือ   มัน ก็ เลย เหมือนกับ บังคับ ว่า   คุณ ต้อง เอา ที่ ต้อง เอา ไม้   ถ้า เอา ไม้ คุณ ไม่ สามารถ จับ xxunk ตรงนี้ ได้   ไม่ สามารถ ไป สร้างบ้าน ได้   ต้อง เคลียร์ ทิ้ง ให้ มัน โล่ง   \n","       \n","   แต่ จริงๆ   แล้ว ชาวบ้าน ชุมชน ก็ อยู่ ของ เขา มา ก่อน   ?   \n","   จริงๆ   แล้ว เขา ก็ อยู่ มา ก่อน   แต่ว่า มัน กลาย เป็นเรื่อง อันตราย   กับ การ เข้าไป จัด การจัดระบบ ตรงนี้   กระแส ทุนนิยม ขึ้นไป บน ดอย มัน เชี่ยว เกินไป   อย่าง ที่ดิน แถว บ้าน ผม   สอง ไร่ ราคา เกือบ ล้าน   ไม่มี ใบ อะไร ซัก อย่าง   ทำให้ ผม มองว่า   หรือว่า ชุมชน ที่ ปา ย มัน จะ ล่มสลาย เหรอ   การท่องเที่ยว ที่   อ. ปา ย มัน ใกล้ จะ ล่ม   แล้วก็ จะ แห่ กัน ขึ้นไป ที่ วัด จันทร์   แล้ว นายหน้า หรือ นายทุน ต้อง ขึ้นไป ปั่นราคา ไว้ ก่อน ไง   ที่ ไหม เหมาะ   ทำเล xxwrep 1 ดี   คือ ริมแม่น้ำ แม่ แจ่ม ซึ่ง ต้นน้ำ แม่ แจ่ม โดย ชื่อ   xxunk   มัน ดัง อยู่ แล้ว ไง   \n","                 \n","   แล้ว ตอนนี้ นายทุน เข้าไป จับจอง หรือยัง   หรือว่า ปั่น ทิ้ง ไว้   ?   \n","   ถึงขั้น ขาย ก็ มี   เริ่ม เข้าไป xxunk   แล้ว ละ   \n","                             \n","   มีความสุข ไหม กับ การ เขียนหนังสือ   การ เป็น นักเขียน   \n","   ผม ว่า มัน สนุก นะ   การ เขียนหนังสือ   \n","       \n","   สุดท้าย แล้ว   เชื่อ มั้ย ว่า   นักเขียน นั้น แยก ไม่ ออก กับ สังคม การ มือ xxunk   \n","   มัน ก็ ใช่   สังคม มัน ก็ เปลี่ยนไป   เปลี่ยนเป็น สังคม ใหญ่ มากขึ้น ทุกที   \n","       \n","       \n","  ,xxbos   \n","   \n","   วาน นี้   ( 21   ก.ย. 52 )   เมื่อ เวลา   9.00   น.   มูลนิธิ เพื่อ แรงงาน หญิง แห่ง เอเชีย   ( committee   for   asian   women :   caw )   ร่วมกับ   โครงการ รณรงค์ เพื่อ แรงงาน ไทย   ( thai   labour   campaign :   xxunk )   และ สหภาพ แรงงาน ไทร อัม พ์ อินเตอร์เนชั่นแนล แห่ง ประเทศไทย จัดกิจกรรม สาธารณะ เนื่องใน วัน สันติภาพ โลก   ( international   peace   day )   ที่ บริเวณ หน้า อาคารสำนักงาน สหประชาชาติ   เพื่อ รณรงค์ ให้ ประชาชน ทั่วไป ได้ มองเห็น ปัญหา ของ แรงงาน หญิง ที่ทำงาน ใน สถานการณ์ ความขัดแย้ง และ การ กดขี่   อีก ทั้ง ร่วม ส่ง ส ริม สันติภาพ ที่ ยั่งยืน   โดย มี แรงงาน หญิง และ นัก กิจกรรม ที่ทำงาน เกี่ยวข้อง ใน เรื่อง นี้ จาก ประเทศ ต่างๆ   อาทิ   พม่า   ฟิลิปปินส์   อินโดนีเซีย   และ ปากีสถาน   รวม กว่า   150   คน   \n","   เจอ xxunk   ฮอน คู ลา ดา   กรรม การบริหาร มูลนิธิ เพื่อ แรงงาน หญิง แห่ง เอเชีย   อ่าน แถลงการณ์   “ หยุด ทำสงคราม ต่อ แรงงาน สตรี :   หยุด นโยบาย การค้า เสรี ”   เนื่องใน วัน สันติภาพ โลก   โดย กล่าวถึง งาน สัมมนา เรื่อง   “ สงคราม อัน ซ่อนเร้น ต่อ สตรี :   สิทธิ แรงงาน หญิง ท่ามกลาง ภาวะสงคราม และ การ ปราบปราม ทางการเมือง ”   จัด ขึ้น โดย มูลนิธิ เพื่อ แรงงาน หญิง   เมื่อ วันที่   20   ก.ย. 52   ซึ่ง เป็น เวที แลกเปลี่ยน ประสบการณ์ ชีวิต ของ แรงงาน หญิง ท่ามกลาง สถานการณ์ ความขัดแย้ง และ การ ปราบปราม ทางการเมือง ใน บริบท ของ สงคราม ก่อการร้าย ระดับโลก และ วิกฤติ เศรษฐกิจ ใน ขณะนี้   ที่ พบ ว่า   ภาระ ซึ่ง เป็นผล จาก วิกฤติ เศรษฐกิจ ระดับโลก ต่อ กลุ่ม แรงงาน สตรี นั้น มี สัด ส่วนน้อย หาก เปรียบเทียบ กับ ปัจจัย ที่ ก่อ โดย ภาค ส่วน อื่นๆ   \n","   เนื่องใน วัน สันติภาพ โลก   วันที่   21   กันยายน   พ.ศ.   2552   ขอ เรียกร้อง ให้   ยุติ ความรุนแรง และ การ ปราบปราม ทางการเมือง   เพียง เพื่อ ตอบสนอง กระแส โลกา ภิ วัติ และ การค้า เสรี   และ เรียกร้อง ให้ เกิด การ ส่งเสริม งาน ที่ มีคุณค่า   ความมั่นคง ด้าน อาชีพ   ค่าแรง ที่ เป็นธรรม   และ สิทธิ เสรีภาพ ใน การ จัดตั้ง เพื่อ ศักดิ์ศรี ของ แรงงาน สตรี   รวมทั้ง คุ้มครอง สิทธิ การ เคลื่อนย้าย และ การทำงาน ของ แรงงาน ข้ามชาติ และ ครอบครัว ของ พวกเขา   \n","   เจอ xxunk   ยัง ได้ กล่าว ประณาม การ ละเมิด สิทธิ และ เสรีภาพ ของ แรงงาน หญิง ทุก รูปแบบ ที่ เป็นผล มาจาก ข้อตกลง การค้า เสรี ที่ ไม่เป็นธรรม   และ เตือน ลัทธิ คลั่ง ศาสนา ว่า กำลัง ทำร้าย เหยื่อ ซึ่ง เป็น สตรี ผู้บริสุทธิ์ จำนวนมาก   อีก ทั้ง ประณาม การ ใช้ สงคราม ก่อการร้าย เป็น เครื่องมือ เพื่อ รักษาผลประโยชน์ อัน ยั่งยืน หรือ ขยาย ฐาน ทางเศรษฐกิจ   การเมือง   และ การทหาร   ประณาม การ ปราบปราม การ ชุมนุม ประท้วง ของ กลุ่ม สตรี ซึ่ง ดำเนินการ อยู่ ภายใต้ กรอบ ของ กฎหมาย   รวมทั้ง ประณาม นโยบาย การ ใช้ ความรุนแรง ทางเพศ และ ละเมิด สิทธิ แรงงาน โดย รัฐ   เพราะ นั่น ถือว่า เป็นการ ปราบปราม การ เรียกร้อง เพื่อ เสรีภาพ   ประชาธิปไตย และ อัตลักษณ์ ของ คน กลุ่ม ต่างๆ   \n","   เธอ กล่าว ยืน ยัง ที่จะ ปกป้อง สิทธิ ของ แรงงาน ข้ามชาติ ผู้ ซึ่ง มี ส่วนสำคัญ ใน การพัฒนา เศรษฐกิจ ของ ประเทศ ปลายทาง การอพยพ   และ ย้ำ ว่า ปัจจัย ที่ ผลักดัน การอพยพ ของ แรงงาน   ได้แก่   การ ถดถอย ของ เศรษฐกิจ และ การ ปราบปราม ทางการเมือง   \n","   “ เรา จะ มุ่ง สร้าง สิ่งแวดล้อม ใน การทำงาน ที่ ดี เพื่อ สตรี   ให้ ปราศจาก ความหวาดกลัว และ ความหิวโหย   เรา จะ ต่อสู้ กับ ความ ท้าทาย ทุก รูปแบบ เพื่อ ปกป้อง สิทธิ และ ทรัพยากร ของ เรา เอง   เพื่อ ต่อต้าน สงคราม และ การทำร้าย ซึ่งกันและกัน ของ มนุษยชาติ   เรา จะ ต่อสู้ เพื่อ สร้าง ความสามัคคี และ สันติภาพ ”   กรรม การบริหาร มูลนิธิ เพื่อ แรงงาน หญิง แห่ง เอเชีย กล่าว   \n","   ด้าน   ธัญ ยธ รณ ์   คีรี ถาวร xxunk   รองประธาน สหภาพ แรงงาน ไทร อัม พ์ อินเตอร์เนชั่นแนล แห่ง ประเทศไทย   ขึ้น อ่าน แถลงการณ์ วัน สันติภาพ สากล ของ กลุ่ม แรงงาน หญิง จาก ไทร อัม พ์ฯ   ว่า   ความ รุ่น แรง ที่ แรงงาน ประสบ ไม่ได้ มี เพียง ความรุนแรง ที่ มองเห็น ใน เชิง ปรากฏการณ์   อย่าง การ ถูก ทำลาย ร่างกาย   ข่มขู่ คุกคาม   แต่ ยัง ได้รับ ความรุนแรง ที่ มองไม่เห็น อย่างชัดเจน   นั่น คือ ความรุนแรง เชิง โครง สร้างความสัมพันธ์ ที่ ไม่เป็นธรรม ของ หน่วย ต่างๆ   ใน สังคม   สะท้อน ให้ เห็น ผ่าน ความเหลื่อมล้ำ ทางสังคม ที่ นับวัน ยิ่ง มากขึ้น   ใน กรณี แรงงาน ไทร อัม พ์ฯ   ถูก เลิกจ้าง   ถึง   1,959   คน   ซึ่ง เกือบ ทั้งหมด เป็น สตรี   โดย บริษัทฯ   ใช้ วิกฤติ เศรษฐกิจ เป็น โอกาส ใน การ เลิก จ้างแรงงาน   ซึ่ง นั้น เป็น การผลักดัน แรงงาน เข้าไป สู่ วิกฤติ เศษ xxunk กิจ   และ ทำลาย สหภาพ แรงงาน   ทั้งนี้   เกือบ   3   เดือน แล้ว ที่ แรงงาน ชุมนุม อยู่ หน้า โรงงาน เพื่อ ต่อสู้ ให้ ได้ กลับ เข้าทำงาน   และ เคลื่อนไหว ใน สังคม ตลอด ระยะเวลา   แต่กลับ ไม่ ได้รับ การ เหลียวแล จาก รัฐ   \n","   การ ชุมนุม เมื่อ วันที่   27   ส.ค. ที่ผ่านมา   แรงงาน ไทร อัม พ์ฯ   พร้อมด้วย เพื่อน ผู้ ร่วม ชะตากรรม อย่าง คนงาน บริษัท เอ นิ ออน และ เวิลด์ เวล   การ ์ เม้นท์   ได้ รวมตัวกัน   กว่า   1,000   คน   ไป ทวงถาม ความคืบหน้า ใน การแก้ปัญหา ของ รัฐบาล ที่ ทำเนียบ และ รัฐสภา   ก็ ยัง ถูก เมิน ซ้ำ และ ยัง ถูก ละเมิด สิทธิ พลเมือง และ ทางการเมือง   รวมถึง สิทธิ ทางเศรษฐกิจ   สังคม   วัฒนธรรม   ด้วย การ พยายาม สลาย การ ชุมนุม   โดย เปิด เครื่อง ทำลาย ประสาท อย่าง   lrad   ส่งผล ให้ ผู้ ร่วม ชุมนุม หลาย คน ได้รับบาดเจ็บ   หู ชั้น กลาง อักเสบ   ตา xxunk มัว   หัว xxunk ผิดปกติ   อีก ทั้ง ยัง ออกหมายจับ ผู้ ร่วม ชุมนุม   ซ้ำ ความรุนแรง เหล่านี้ ถูก ตอกย้ำ ด้วย ความรุนแรง เชิง วัฒนธรรม ใน สังคม   ที่ยอมรับ ว่าการ เลิกจ้าง เป็นเรื่อง ปกติ   การ สลาย การ ชุมนุม เป็นเรื่อง ที่ยอมรับ ได้   รวมถึง การ มอง ชีวิต เหล่านั้น เป็นเรื่อง ไม่ โรแมนติก ไม่ น่าสนใจ   ปัญหา แรงงาน จึง ไม่ ค่อย มี พื้นที่ หรือ ได้รับ ความสนใจ   \n","   เนื่องใน วันนี้ เป็น วัน สันติภาพ โลก ที่ สหประชาชาติ กำหนด ขึ้น มา   จึง ขอให้ ทาง องค์การสหประชาชาติ ยุติ ความรุนแรง ที่ แรงงาน ประสบ   โดย ใน เบื้องต้น   1. ขอให้ ดำเนินการ ช่วยเหลือ ให้ ยกเลิก หมายจับ   เพราะ ถือ เป็นการ ชุมนุม โดย สันติ ปราศจาก อาวุธ   2. ขอให้   ดำเนินการ ตรวจสอบ การ ใช้ ความรุนแรง นำ โดย   พล.ต. ต   วิชัย   สังข์ ประไพ   ผู้บังคับการ ตำรวจนครบาล   1   ที่ พยายาม สลาย การ ชุมนุม โดย การ เปิด เครื่อง ทำลาย ประสาท อย่าง   lrad   โดย ไม่ได้ มี การ แจ้ง ให้ แก่ ผู้ ชุมนุม ทราบ ล่วงหน้า   รวมถึง การ ใช้ ความรุนแรง โดย รัฐ ต่อ ผู้ ชุมนุม ใน กรณี อื่นๆ   3. ขอให้ เข้ามา ตรวจสอบ และ หา มาตร การกดดัน ผู้ ที่ มีอำนาจ ใน การแก้ปัญหา กรณี เลิกจ้าง เพื่อ เป้าหมาย การ ทำลาย สหภาพ แรงงาน   \n","   4. ขอให้   un   เรียกร้อง และ บังคับ ให้ บริษัท   ปฏิบัติตาม   code   of   conduct   ของ กลุ่ม บริษัท ไทร อัม พ์ฯ   ซึ่ง มุ่งเน้น ถึง ความสำคัญ ของ การปกป้อง สิทธิมนุษยชน   และ พร้อมใจ ปฎิบัติ ตาม ข้อตกลง ที่ เกี่ยวข้อง ของ องค์กร แรง ระหว่างประเทศ   ( ilo )   และ ข้อตกลง ว่าด้วย ความร่วมมือ ของ องค์การสหประชาชาติ   ใน ด้าน กฎระเบียบ และ การ พัฒนาการ ปฎิบัติ งาน และ สภาวะ ทางเศรษฐกิจ   รวมถึง มาตรฐานสากล ของ องค์กร สหประชาชาติ   ( un   global   compact )   และ   5. ขอให้   un   เรียกร้อง และ บังคับ ให้ บริษัท เปิดเผย ข้อมูล ตาม หลัก มาตรฐานแรงงาน สากล และ แนวปฏิบัติ ของ บรรษัทข้ามชาติ   ( oecd   guidelines )   ตาม หลัก   oecd   guidelines   for   mnes   และ เปิดโอกาส ให้ องค์กร ของ ลูกจ้าง หรือ สหภาพ แรงงาน เข้าร่วม การปรึกษาหารือ กับ ฝ่ายบริหาร ใน การ ดำเนิน นโยบาย ต่างๆ   ที่ อาจ ส่ง ผลกระทบ ต่อ   ชีวิต ความเป็นอยู่ ที่ ดี ของ คนงาน   \n","   ใน ระยะ ต่อมา   1. ขอให้ พิจารณา ความรุนแรง เชิง โครงสร้าง จาก แนว นโยบาย เสรีนิยม ใหม่ ของ รัฐบาล   อย่าง   การ แปรรูป กิจการ ที่ เป็นการ ให้ สวัสดิการ แก่ ประชาชน ของ รัฐ ต่างๆ   ให้ เป็น กิจการ ของ เอกชน   เช่น   การนำ มหาวิทยาลัย ออก นอก ระบบ   การ แปรรูป รัฐวิสาหกิจ   รวมถึง   การ เปิด ให้ มี การ จ้าง งานเหมา ค่าแรง ที่ นายจ้าง ไม่ต้อง รับผิดชอบ ต่อ สวัสดิการ ต่อ ลูกจ้าง   การ เปิด เขต เสรี ทางการค้า   ที่ มี การ ห้าม การ ตั้ง สหภาพ แรงงาน   เป็นต้น   2. ขอให้ สนับสนุน ให้ ผู้ใช้แรงงาน และ ผู้ เสียเปรียบ ใน สังคม อื่นๆ   มี ความมั่นคง ทั้ง ทางเศรษฐกิจ และ สังคม   ผ่าน การ ส่งเสริม ให้การ กระจาย รายได้ อย่างเป็นธรรม   ลด ช่อง ทางสังคม   เช่น   การ ส่งเสริม ให้ มี หลักประกัน สุขภาพ   ที่อยู่อาศัย   การศึกษา   รายได้ และ การมีงานทำ โดย รัฐ ที่ ทั่วหน้า และ มีประสิทธิภาพ   การ ยกเลิก ภาษีมูลค่าเพิ่ม ที่ ผลัก ภา ระมา ให้ แรงงาน หรือ ผู้มีรายได้ น้อย   เป็นต้น   \n","   ธัญ ยธ รณ ์   กล่าวถึง การต่อสู้ ขณะนี้ ของ แรงงาน หญิง ไทร อัม พ์ ว่า   แรงงาน ยัง รวมตัวกัน อยู่ ที่ หน้า โรงงาน   โดย ใน วันนี้ นับ เป็นเวลา กว่า   85   วัน ของ การ ชุมนุม แล้ว   เพราะ พวกเขา ไม่มี ที่ ไป   แรงงาน ที่นี่ ส่วนใหญ่ ทำงาน มา ตั้งแต่ อายุ ยัง น้อย   ถึง วันนี้ ก็ อายุงาน กว่า   30 - 40   ปี   ถือว่า ทำกำไร ให้ แก่ บริษัท มากมาย แต่ ถึง วันนี้ ทาง บริษัท กลับ ไม่เคย สนใจ   การ เลิกจ้าง ทำให้ แรงงาน เกิด ความกดดัน   เกิด ความเครียด   เธอ เล่า ว่า มี แรงงาน ที่ มี อาการป่วย ด้วย โรคมะเร็ง ต้อง อา การทรุด หนัง ลง เพราะ ความเครียด จนกระทั่ง เสียชีวิต ลง เมื่อ เดือน สิงหาคม ที่ผ่านมา   และ พบ ว่า มี คน พยายาม ฆ่าตัวตาย ด้วย ปัญหา ถูก เลิกจ้าง   แต่ เพื่อน คนงาน สามารถ ช่วยเหลือ ไว้ ได้ ทัน   ทั้งนี้ ล่าสุด มี แรงงาน หลาย คน ที่ ต้อง ยกเลิก เช่า ห้องพัก แล้ว มา กินนอน อยู่ ใน ที่ ชุมนุม เพราะ ไม่ มีเงิน ที่จะ จ่าย ค่าเช่า   \n","   กับ คำถาม ที่ หลาย คน บอ กว่า ถูก เลิกจ้าง ทำไม ไม่ กลับบ้าน ที่ ต่างจังหวัด   ธัญ ยธ รณ ์ แสดง ความเห็น ว่า   แรงงาน ที่ เข้ามา ส่วนใหญ่ จาก บ้านเกิด มา หลาย สิบ ปี   และ หวัง มา ลง หลัก ปัก ฐาน ทำงาน และ ใช้ชีวิต อยู่ ที่นี่   วิถีชีวิต ของ พวกเขา ไป เปลี่ยนไป แล้ว   หลาย คน มี ภาระ รับผิดชอบ ต้อง ส่ง ลูก เรียน   ต้อง ผ่อน บ้าน เอื้อ อา ธร อยู่   อีก ทั้ง คนรู้จัก ที่ บ้านเกิด ใน ต่างจังหวัด ก็ น้อยลง   หลาย คน กลับ ไป ก็ แทบ ไม่ รู้จัก ใคร   ทั้งนี้ แม้ จะ มี แรงงาน บางคน สามารถ กลับ ไป อยู่ ที่ บ้านเกิด ได้   แต่ ส่วนตัว คิด ว่า เป็น ส่วนน้อย มาก   \n","   “ แรงงาน ต้อง ต่อสู้ เรียกร้อง สิทธิ   เพราะ เขา ไม่มี ทางเลือก   ไม่มีทาง ไป ”   รองประธาน สหภาพ แรงงาน ไทร อัม พ์ กล่าว   \n","   นอกจากนี้   ใน การ ป รา ศัย มี นัก กิจกรรม จาก องค์กร ณ์ ที่ทำงาน เกี่ยวข้อง ใน เรื่อง แรงงาน หญิง จาก ประเทศ ต่างๆ   ได้ ขึ้น พูด บน ผ่าน เครื่องขยายเสียง   อาทิ   นัก กิจกรรม จาก   xxunk   ซึ่ง เป็น องค์กร เพื่อ สิทธิ แรงงาน หญิง ในประเทศ ฟิลิปปินส์ ,   องค์กร สิทธิ แรงงาน พม่า   ( migrants   assistance   programme :   map ),   focus   of   the   global   south   และ   women   worker   organization   from   indonesia     \n","       \n","   นิค   จาก องค์กร   map   กล่าวว่า เมื่อ เรื่อง สิทธิมนุษยชน เท่ากับ สิทธิ ตาม ธรรมชาติ   สิทธิ ของ แรงงาน ก็ เป็น สิทธิ ตาม ธรรมชาติ เช่นกัน   และ เรา ทุกคน ต้อง ควร ร่วมกัน ปกป้อง   \n","   คา ร่า   องค์กร เพื่อ สิทธิ แรงงาน หญิง ใน อินโดนีเซีย กล่าวว่า   xxunk แรงงาน ยัง ไม่ ได้รับ ความเป็นธรรม แน่นอน ว่า ไม่มี มี สันติภาพ ใน ประเทศไทย   รวมทั้ง ใน การทำงาน ของ แรงงาน หญิง ทั่วโลก ด้วย   สันติภาพ มี สิ่ง ที่ สำคัญ อยู่   3   ข้อ   คือ การ มี อาหาร กิน ครบ ทั้ง สาม มื้อ   ลูกหลาน ได้ ไป โรงเรียน   และ พ่อแม่ มี งาน ทำ และ ไม่ ถูก คุกคาม   หาก ไม่มี   3   ข้อ ข้างต้น   ก็ ไม่มี สันติภาพ   เพราะ นี่ คือ ความต้องการ ขั้นพื้นฐาน   \n","   อนึ่ง   จาก เอกสาร แถลงการณ์   มูลนิธิ เพื่อ แรงงาน หญิง แห่ง เอเชีย   ( committee   for   asian   women :   caw )   เนื่องใน วัน สันติภาพ สากล   ใน ฉบับ ภาษาอังกฤษ ระบุ ว่า   ประชาธิปไตย และ สันติภาพ จะ แบ่งแยก ไม่ได้ จาก สิทธิสตรี   และ   สันติภาพ   ไม่ได้ หมาย ความ เพียงแต่ การ ไม่มี สงคราม   ทั้งนี้ ตั้งแต่ มี การ ประกาศ   วัน สันติภาพ สากล เมื่อ   26   ปี ที่ผ่านมา   ผู้หญิง ใน เอเซีย ยังคง ต้อง ต่อสู้ กับ ความขัดแย้ง ทางการเมือง มาโดยตลอด   ยกตัวอย่าง   ใน บังกลาเทศ ภายใน   2   ปี ภายใต้ สถานการณ์ ฉุกเฉิน ที่ เพิ่ง มี การ ยกเลิก ไป เมื่อ ปี ที่   2551   กิจกรรม ของ สหภาพ แรงงาน ถูก ระงับ ส่งผล ให้ แรงงาน กว่า   100   คน ถูก ฆ่า และ ทำร้าย   ส่วน ใน พม่า   รัฐบาล ทหาร ได้ ห้าม กิจกรรม ของ สหภาพ แรงงาน ใน ทุก รูปแบบ   และ ตัดสิน ให้ สหพันธ์ แรงงาน แห่ง พม่า   ( the   federation   of   trade   unions   of   burma :   xxunk )   เป็น องค์กร ก่อการร้าย และ ผิดกฎหมาย   นัก กิจกรรม สหภาพ แรงงาน และ ครอบครัว ถูก ขัง คุก   คนงาน และ สหภาพ แรงงาน ประสบปัญหา จาก ความขัดแย้ง ที่ มี ในประเทศ   ทำให้ ไม่มี การ จ้างงาน   ค่าจ้าง ลดลง   ไม่มี ความเป็นธรรม ทางสังคม   มี การย้ายถิ่น ฐาน และ คนยากจน มากขึ้น   \n","   นอกจากนี้ วิกฤติ เศรษฐกิจ ก็ เป็น อีก ส่วนหนึ่ง ที่ ทำให้ ภาระ ของ ผู้หญิง เพิ่มมากขึ้น   เพราะ ต้อง พยายาม หาเงิน ทำให้ การทำงาน ถูก กดขี่ ให้ อยู่ ใน สภาวะ จำยอม   และ การปกป้อง สิทธิ ใน การทำงาน ที่ สำคัญ ที่สุด คือ เสรีภาพ ใน การ รวมตัว ตรงนี้ ก็ ถูก กด ลง ไป ด้วย ข้ออ้าง ใน เรื่อง ความสูญเสีย ทางเศรษฐกิจ ของ นายจ้าง   ในขณะที่ ความรุนแรง ทางเพศ แผ่ขยาย ไป ใน วงกว้าง   เช่นเดียวกับ ที่ ผู้หญิง ต้อง สุญ เสีย เสรีภาพ และ ไม่มี พื้นที่ ทางการเมือง   \n","   แถลงการณ์ ได้ เรียกร้อง ให้ ยุติ การ กดขี่ ทางการเมือง   สังคม   และ ศาสนา ใน ทุก รูปแบบ   หาทาง แก้ปัญหา ความขัดแย้ง ที่ ยั่งยืน อย่าง เร่งด่วน   โดย ต้อง ให้ ผู้หญิง มีส่วนร่วม ใน การแก้ปัญหา เพื่อ นำ ไป สู้ สันติภาพ   \n","   ผู้สื่อข่าว รายงาน ว่าการ ชุมนุม ใน วันนี้ เป็นไป อย่างสงบ โดย มี ตำรวจนอกเครื่องแบบ เฝ้า สัง เกตุ การณ์   กลุ่ม ผู้เข้าร่วม มี การ ชู ป้าย และ ป้าย ผ้า ทั้ง ภาษาไทย และ ภาษาอังกฤษ   ระบุ ข้อเรียกร้อง   อาทิ   “ขอ สันติภาพ คืน มา ”   “ สู้ เพื่อ งาน ”   “ สู้ เพื่อ ลูก ”   “ สู้ เพื่อ สิทธิ ”   “ สู้ เพื่อ อนาคต ”   “ สู้ ไม่ ถอย ”   “ หยุด ความรุนแรง และ การ ปราบปราม ทางการเมือง ”   และ “ ปกป้อง สิทธิ แรงงาน หญิง ”   เป็นต้น   รวมทั้ง xxwrep 1 การ แสดงละคร   ( xxunk   xxunk )   หยุด ความรุนแรง ต่อ ผู้หญิง โดย กลุ่ม นักศึกษา ที่ มี เนื้อหา กระตุ้น ให้ แรงงาน หญิง และ คน ทั่วไป ลุกขึ้น มา ร่วมกัน ปกป้อง สิทธิ ของ แรงงาน หญิง   จากนั้น เป็นการ ร่วมกัน ร้องเพลง   และ รับประทาน อาหาร ร่วมกัน ก่อนที่จะ แยกย้าย เดินทาง กลับ   \n","   อย่างไรก็ตาม   ระหว่าง การ ทำกิจกรรม ของ กลุ่ม ผู้ ชุมนุม ได้ มี กำลัง ตำรวจ เข้ามา เสริมกำลัง และ ตั้งแถว อยู่ บริเวณ ใกล้เคียง   เพื่อ สัง เกตุ การณ์   และ ออกจาก บริเวณ ที่ มี การ ชุ มนุ น ก่อนที่ ผู้ ร่วม ชุมนุม จะ สลายตัว   \n","   ทั้งนี้   สหประชาชาติ ได้ กำหนดให้ วันที่   21   กันยายน ของ ทุกปี   เป็น วัน สันติภาพ โลก   และ กำหนดให้ ปี   ค.ศ. 2001 - 2010   เป็น   “ ทศวรรษ สากล เพื่อ วัฒนธรรม สันติภาพ และ ความ ไม่ รุนแรง เพื่อ เด็ก ของ โลก ”   ( the   international   xxunk   2001 - 2010   for   a   culture   of   peace   and   xxunk   for   the   children   of   the   world )   โดย มุ่งเน้น ที่   1. การ ให้ ความเคารพ ต่อ ชีวิต ทั้งมวล   เคารพ ชีวิต และ ศักดิ์ศรี ของ แต่ละบุคคล   โดย ไม่ แบ่ง ชนชั้น หรือ ลำเอียง   2. การ ไม่ ใช้ ความรุนแรง ใน ทุก รูปแบบ   โดยเฉพาะ ต่อ เด็ก และ เยาวชน   3. การ แบ่งปัน กับ ผู้อื่น อย่าง มีน้ำใจ   เพื่อ ขจัด การ แบ่งแยก   ความ ไม่ ยุติธรรม   และ การ กดขี่ ทางการเมือง และ เศรษฐกิจ   4. การ รับฟัง เพื่อให้ เกิด ความเข้าใจ ต่อกัน   เคารพ เสรีภาพ ใน การแสดงออก   และ ยอมรับ ความแตกต่าง ทางวัฒนธรรม   5. การสงวนรักษา ผืน โลก   ฝึก ดำเนินชีวิต อย่าง รับผิดชอบ และ เคารพ ต่อ ทุก ชีวิต ใน โลก   เพื่อ รักษา สมดุล ของ ธรรมชาติ บน ผืน โลก   และ   6. การ สร้าง ความสมานฉันท์   เคารพ ต่อ หลักการ ประชาธิปไตย   และ ให้โอกาส ทุกฝ่าย มีส่วนร่วม อย่าง เท่าเทียมกัน   โดยเฉพาะ สตรี   \n","       \n","       \n","   หยุด ทำสงคราม ต่อ แรงงาน สตรี   :   หยุด นโยบาย การค้า เสรี   \n","       \n","   งาน สัมมนา เรื่อง   “ สงคราม อัน ซ่อนเร้น ต่อ สตรี :   สิทธิ แรงงาน หญิง ท่ามกลาง ภาวะสงคราม และ การ ปราบปราม ทางการเมือง ”   จัด ขึ้น โดย มูลนิธิ เพื่อ แรงงาน หญิง   เมื่อ วันที่   20   กันยายน   พ.ศ.   2552   ที่ กรุงเทพมหานคร   เป็น เวที แลกเปลี่ยน ประสบการณ์ ชีวิต ของ แรงงาน หญิง ท่ามกลาง สถานการณ์ ความขัดแย้ง และ การ ปราบปราม ทางการเมือง ใน บริบท ของ สงคราม ก่อการร้าย ระดับโลก และ วิกฤติ เศรษฐกิจ ใน ขณะนี้   จาก เวที สัมมา นี้   เรา พบ ว่า   \n","       \n","   -   ภาระ ที่ เป็นผล จาก วิกฤติ เศรษฐกิจ ระดับโลก ที่เกิด จาก กลุ่ม แรงงาน สตรี ที่ เปราะบาง นั้น มี สัดส่วน เพียง น้อย นิด เมื่อ เทียบ กับ ปัจจัย ที่ ก่อ โดย ภาค ส่วน อื่นๆ   \n","   -   เรา ขอ ประณาม การ ละเมิด สิทธิ และ เสรีภาพ ของ แรงงาน หญิง ทุก รูปแบบ ที่ เป็นผล มาจาก ข้อตกลง การค้า เสรี ที่ ไม่เป็นธรรม   \n","   -   เรา ขอ เตือน ลัทธิ คลั่ง ศาสนา ว่า คุณ กำลัง ทำร้าย เหยื่อ ซึ่ง เป็น สตรี ผู้บริสุทธิ์ จำนวนมาก   \n","   -   เรา ขอ ประณาม การ ใช้ สงคราม ก่อการร้าย เป็น เครื่องมือ เพื่อ รักษาผลประโยชน์ ด้าน ความยั่งยืน หรือ ขยาย ฐาน ทางเศรษฐกิจ   การเมือง และ กองกำลัง ทหาร   และ ประณาม การ ปราบปราม การ ชุมนุม ประท้วง ของ กลุ่ม สตรี ซึ่ง ดำเนินการ อยู่ ภายใต้ กรอบ ของ กฎหมาย   \n","   -   เรา ประณาม นโยบาย การ ใช้ ความรุนแรง ทางเพศ และ ละเมิด สิทธิ แรงงาน โดย รัฐ   เพราะ นั่น ถือว่า เป็นการ ปราบปราม การ เรียกร้อง เพื่อ เสรีภาพ   ประชาธิปไตย และ อัตลักษณ์ ของ คน พม่า และ ชนกลุ่มน้อย ต่างๆ   \n","   -   เรา ขอ ปกป้อง สิทธิ ของ แรงงาน ข้ามชาติ ผู้ ซึ่ง มี ส่วนสำคัญ ใน การพัฒนา เศรษฐกิจ ของ ประเทศ ปลายทาง การอพยพ   และ   \n","   -   เรา ขอ เน้นย้ำ ว่า ปัจจัย ที่ ผลักดัน การอพยพ ของ แรงงาน   ได้แก่   การ ถดถอย ของ เศรษฐกิจ และ การ ปราบปราม ทางการเมือง   \n","       \n","   เนื่องใน วัน สันติภาพ โลก   วันที่   21   กันยายน   พ.ศ.   2552   เรา จึง ขอ เรียกร้อง ให้ :   \n","       \n","   -   ยุติ ความรุนแรง และ การ ปราบปราม ทางการเมือง   เพียง เพื่อ ตอบสนอง กระแส โลกา ภิ วัติ และ การค้า เสรี   \n","   -   เรียกร้อง ให้ เกิด การ ส่งเสริม งาน ที่ มีคุณค่า   ความมั่นคง ด้าน อาชีพ   ค่าแรง ที่ เป็นธรรม   และ สิทธิ เสรีภาพ ใน การ จัดตั้ง เพื่อ ศักดิ์ศรี ของ แรงงาน สตรี   \n","   -   คุ้มครอง สิทธิ การ เคลื่อนย้าย และ การทำงาน ของ แรงงาน ข้ามชาติ และ ครอบครัว ของ พวกเขา   \n","       \n","   เรา จะ มุ่ง สร้าง สิ่งแวดล้อม ใน การทำงาน ที่ ดี เพื่อ สตรี   ให้ ปราศจาก ความหวาดกลัว และ ความหิวโหย เรา จะ ต่อสู้ กับ ความ ท้าทาย ทุก รูปแบบ เพื่อ ปกป้อง สิทธิ และ ทรัพยากร ของ เรา เอง เพื่อ ต่อต้าน สงคราม และ การทำร้าย ซึ่งกันและกัน ของ มนุษยชาติ   เรา จะ ต่อสู้ เพื่อ สร้าง ความสามัคคี และ สันติภาพ   \n","       \n","       \n","   มูลนิธิ เพื่อ แรงงาน หญิง   ( caw )   เป็น เครือข่าย ของ องค์กร แรงงาน หญิง กว่า   46   องค์กร   จาก   14   ประเทศ ใน ภูมิภาค เอเชียตะวันออกเฉียงใต้ มูลนิธิ เพื่อ แรงงาน หญิง มุ่ง สร้าง ความเข้มแข็ง ให้ กับ แรงงาน ใน การปกป้อง   และ ผลักดัน สิทธิ ของ ตนเอง   caw   เป็น ส่วนหนึ่ง ของ การเคลื่อนไหว ระดับโลก เพื่อ ผลักดัน ประเด็น เรื่อง สิทธิ ของ แรงงาน มา นาน กว่า   30   ปี   \n","       \n","       \n","       \n","   แถลงการณ์ วัน สันติภาพ สากล   :   หาก พวกเรา มีชีวิต เช่นนี้   สันติภาพ สากล ย่อม ไม่ เกิด   \n","       \n","   จาก การ ที่ สหประชาชาติ ได้ กำหนดให้ วันที่   21   กันยายน ของ ทุกปี   เป็น   “ วัน สันติภาพ โลก ”   ( international   day   of   peace   ซึ่ง เป็น ที่ ประจักษ์ ว่า   ณ   เวลานี้ พวกเรา ชนชั้น แรงงาน ยัง เผชิญ กับ ความรุนแรง อยู่ ทั่ว ทุ กลม xxunk   \n","       \n","   ความ รุ่น แรง ที่ พวกเรา ประสบ นั้น   ไม่ได้ มี เพียง ความรุนแรง ที่ มองเห็น ใน เชิง ปรากฏการณ์   ซึ่ง เป็นเรื่อง ปกติ ที่ เรา ประสบ อยู่ เป็นประจำ   อย่าง การ ถูก ทำลาย ร่างกาย   การ ถูก ข่มขู่ คุกคาม   เพียง เท่านั้น   แต่ เรา ยัง ได้รับ ความรุนแรง ที่ มองไม่เห็น อย่างชัดเจน   เป็น ความรุนแรง ที่ ถูก ซ่อน อยู่ ใต้ ยอด ภูเขาน้ำแข็ง และ ถูก แช่ ด้วย น้ำทะเล อัน เยือกเย็น หนาว   ที่ โดย ตัว มัน เอง เป็น ความรุนแรง แล้ว ยัง ผลักดัน ให้ เกิด ความรุนแรง ใน เชิง ป ระ กฏ การณ์ อย่าง ที่ เรา มองเห็น อีกที   นั่น คือ ความรุนแรง เชิง โครงสร้าง หรือ ความสัมพันธ์ ที่ ไม่เป็นธรรม ของ หน่วย ต่างๆ   ใน สังคม   ซึ่ง สะท้อน ให้ เห็น ผ่าน ความเหลื่อมล้ำ ทางสังคม ที่ นับวัน ยิ่ง หาก ไกล กัน เหลือเกิน   เช่น   กลุ่มคน ร้อยละ   20   ที่ มี รายได้ สูงสุด   มี ส่วนแบ่ง ของ รายได้ ใน สังคม สูง ถึง   ร้อยละ   50   ในขณะที่ กลุ่มคน ร้อยละ   20   ที่ มี รายได้ ต่ำสุด   กลับ มี ส่วนแบ่ง เพียง ร้อยละ   5.7   และ ปรากฏ คน เพียง ไม่ กี่ ตระกูล ในประเทศ นี้ ครอบครอง ทรัพยากร เกือบ หมด ประเทศ   \n","       \n","   การ ก่อตัว ของ พวกเรา ชนชั้น แรงงาน ใน ไทย มา พร้อมกับ การพัฒนา อุตสาหกรรม ไม่ ต่ำกว่า   50   ปี   ที่ เน้น การ ดึงดูด นักลงทุน ข้ามชาติ และ ใน ชาติ   ด้วย แรงงาน ราคา ถูก   สวัสดิการ ต่ำ   พร้อมกับ กฏ ระเบียบ ที่ เอื้อ ต่อ เขา   ทำให้ พวกเรา ได้รับ ค่าตอบแทน ที่ต่ำ เพียง เพื่อ ประทังชีวิต ไป วัน ๆ   เท่านั้น   นั่น ยอม ส่งผล การ ทบ ต่อ กำลัง บริโภค ภายในประเทศ ที่ต่ำ ด้วย   แต่ xxunk ภายใต้ ระบบ เสรีนิยม ใหม่ ก็ได้ สร้าง นวัตกรรม เพื่อให้ เกิด การ กระตุ้น การ บริโภค นั้น   โดย การนำ เงิน อนาคต มา ใช้ อย่าง บัตร เคร ดิส   พร้อมกับ การ ลด สวัสดิการ ที่ รัฐ เป็น ผู้รับผิดชอบ ผ่าน การ แปรรูป กิจการ สาธารณะ ต่างๆ   ให้ เป็น ของ เอกชน   อย่าง การนำ มหาวิทยาลัย ออก นอก ระบบ   การ แปรรูป รัฐวิสาหกิจ   การ เปิด เสรี ทางการค้า   เป็นต้น   นั่น ยิ่ง ทำให้ ชีวิต เรา ถูก พันธนาการ ไว้ กับ ความ เสียง มากมาย   \n","       \n","   การ กดขี่ ขูดรีด ที่ ดูเหมือนว่า เรา จะ มี เสรีภาพ ใน ตลาดแรงงาน   ซึ่ง แท้จริง แล้ว เป็น ความรุนแรง เชิง โครงสร้าง นี้   ทำให้ เรา ไม่มี ความมั่นคง ทางเศรษฐกิจ และ สังคม   และ ด้วย การ ที่ รัฐ ดึงดูด นักลงทุน ด้วย แรงงาน ราคา ถูก   ยิ่ง ทำให้ รัฐ ไม่มี มาตรา การ ที่จะ รักษาผลประโยชน์ ของ แรงงาน อย่าง จริงๆ   จัง ๆ   อย่าง กรณี เรา คือ   สหภาพ แรงงาน ไทร อัม พ์ อินเตอร์เนชั่นแนล แห่ง ประเทศไทย   ซึ่ง ถูก นายจ้าง เลิกจ้าง เพียง เพื่อ การปรับโครงสร้าง ของ บริษัท   และ ทาง บริษัท ยัง ย้ำ ว่า บริษัทแม่ ในประเทศ เยอรมนี มอง ปัญหา วิกฤต เศรษฐกิจ ที่ กำลัง เกิดขึ้น ขณะนี้ เป็น โอกาส ใน การ ขยาย ธุรกิจ   เพราะ บริษัทแม่ มี เงินลงทุน สูง   แต่กลับ ใช้ วิกฤติ เศรษฐกิจ ใน ครั้งนี้ เป็น โอกาส ใน การ เลิกจ้าง พวกเรา ถึง   1,959   คน   ซึ่ง เกือบ ทั้งหมด เป็น สตรี   และ เป็น สตรี ที่ กำลัง ตั้งครรภ์   สตรี ที่ xxunk   พิการ และ ป่วย   นั่น ย่อม เป็น การผลัก เรา นี้ เข้า สู้ xxunk ที่ เรียก ว่า วิกฤติ เศษ xxunk กิจ   แต่ เป็น โอกาส ของ นายทุน   เพียง เพื่อ หวัง แสวงหา แรงงาน ราคา ถูก และ ทำลาย สหภาพ แรงงาน   ซึ่ง เป็น องค์กร ที่ พิทักษ์ สิทธิ ของ แรงงาน   \n","       \n","   เกือบ   3   เดือน แล้ว ที่ เรา ยืนหยัด ต่อสู้ เพื่อ กลับ เข้าทำงาน   เรา ถูก ข่มขู่ คุกคาม   ถูก อา xxunk ทรัพย์ จาก การ ที่ หมด เคร ดิส   เรา ชุมนุม หน้า โรงงาน และ เคลื่อนไหว ใน สังคม ตลอด ระยะเวลา   แต่กลับ ไม่ ได้รับ การ เหลียวแล จาก รัฐ   ครั้น เมื่อ เรา ไป เรียกร้อง ใน ครั้งแรก   ต่อ รัฐบาล   6   ส.ค. 2552   กลับ ถูก เมิน   พอ มาถึง   27   ส.ค.   เรา พร้อมด้วย เพื่อน ผู้ ร่วม ช ตา กรรม อย่าง คนงาน บริษัท เอ นิ ออน และ xxunk เวล   การ ์ เม้นท์   ซึ่ง เกือบ ทั้งหมด คือ คนงาน หญิง เช่น เรา   ได้ รวมตัวกัน   กว่า   1,000   คน   ไป ทวงถาม ความคืบหน้า ใน การแก้ปัญหา ของ รัฐบาล ที่ ทำเนียบ และ รัฐสภา   ก็ ยัง ถูก เมิน ซ้ำ   พร้อมด้วย การ ละเมิด สิทธิ พลเมือง และ ทางการเมือง   รวมถึง สิทธิ ทางเศรษฐกิจ   สังคม   วัฒนธรรม   ด้วย การ พยายาม สลาย การ ชุมนุม และ ปิด กัน สิทธิ ใน การ พูด   ด้วย การ เปิด เครื่อง ทำลาย ประสาท อย่าง   lrad   ส่งผล ให้ ผู้ ร่วม ชุมนุม หลาย คน ได้รับบาดเจ็บ คือ   หู ชั้น กลาง อักเสบ   ตา xxunk มัว   หัว xxunk ผิดปกติ   เป็นต้น   พร้อมกับ การ ออกหมายจับ ผู้ ร่วม ชุมนุม ใน วันนั้น   คือ   นาย สุนทร   บุญ ยอด   นางสาว บุญ รอด   สาย วงศ์   และ นางสาว จิตรา   คช เดช   ด้วย ข้อหา มั่วสุม กัน ตั้งแต่ สิบ คน ขึ้นไป   กระทำการ อย่างหนึ่ง อย่างใด หนึ่ง ให้ เกิด ความวุ่นวาย ใน บ้านเมือง   ตาม มาตรา   215   และ มาตรา   216   ที่ มี โทษหนัก ถึง จำคุก เป็น ระยะเวลา   5   ปี   \n","       \n","   ซ้ำ ความรุนแรง เหล่านี้ กับ ถูก ตอกย้ำ ด้วย ความรุนแรง เชิง วัฒนธรรม ใน สังคม นี้   ที่ยอมรับ ว่าการ เลิกจ้าง เป็นเรื่อง ปกติ   การ สลาย การ ชุมนุม เป็นเรื่อง ที่ยอมรับ ได้   รวมถึง การ มอง ชีวิต เหล่านั้น เป็นเรื่อง ไม่ โรแมนติก ไม่ น่าสนใจ   ปัญหา แรงงาน จึง ไม่ ค่อย มี พื้นที่ หรือ ได้รับ ความสนใจ ใน สังคม นี้   \n","       \n","   เนื่องใน วันนี้ เป็น วัน สันติภาพ โลก ที่ สหประชาชาติ กำหนด ขึ้น มา   เรา จึง ขอให้ ทาง องค์การสหประชาชาติ ยุติ ความรุนแรง ที่ เรา ประสบ โดย ใน เบื้องต้น เฉพาะหน้า นี้   เรา   \n","       \n","   1. ขอให้   ดำเนินการ ช่วยเหลือ ให้ มี การ ยกเลิก หมายจับ   เพราะ ถือ เป็นการ ชุมนุม โดย สันติ ปราศจาก อาวุธ ซึ่ง ได้รับ การ รองรับ ตาม รัฐธรรมนูญ แห่ง ราช อาญา จักร ไทย xxwrep 1 และ กติกา ระหว่างประเทศ   ว่าด้วย สิทธิ พลเมือง และ สิทธิ ทางการเมือง   ที่ ประเทศไทย เป็นสมาชิก ภาคี   และ การ ออกหมายจับ ใน กรณี ชุมนุม โดย สันติ ปราศจาก อาวุธ ใน กรณี อื่นๆ   \n","       \n","   2. ขอให้   ดำเนินการ ตรวจสอบ การ ใช้ ความรุนแรง นำ โดย   พล.ต. ต   วิชัย   สังข์ ประไพ   ผู้บังคับการ ตำรวจนครบาล   1   ที่ พยายาม สลาย การ ชุมนุม โดย การ เปิด เครื่อง ทำลาย ประสาท อย่าง   lrad   ที่ มี เสียงดัง มาก   ได้รับ ผลกระทบ ต่อ คนงาน ผู้หญิง   คนงาน พิการ   และ อายุ มาก ที่ ได้ นั่ง ฟัง ปราศรัย หน้า รัฐสภา   โดย ไม่ได้ มี การ แจ้ง ให้ แก่ ผู้ ชุมนุม ทราบ ล่วงหน้า ถึง การ ดำเนินการ และ ผลกระทบ ที่จะ เกิดขึ้น ต่อ ผู้ ชุมนุม   รวมถึง การ ใช้ ความรุนแรง โดย รัฐ ต่อ ผู้ ชุมนุม ใน กรณี อื่นๆ   \n","       \n","   3. ขอให้ เข้ามา ตรวจสอบ และ หา มาตร การกดดัน ผู้ ที่ มีอำนาจ ใน การแก้ปัญหา กรณี เลิกจ้าง เพื่อ เป้าหมาย การ ทำลาย สหภาพ แรงงาน   อย่าง กรณี นี้   ถือ เป็นการ เลิกจ้าง ละเมิด สิทธิมนุษยชน   หา แหล่ง ค่าจ้าง ราคา ถูก และ ขยาย โรงงาน ใหม่ โดย ที่ ได้รับ การ ส่งเสริม การลงทุน จาก รัฐ   ไม่ ปฎิบัติ ตามกฎหมาย พรบ. แรงงานสัมพันธ์   ปี   18 ( ข้อตกลง สภาพ การ จ้าง ระหว่าง บริษัท บอดี้ แฟชั่น ( ประเทศไทย ) จำกัด กับ สหภาพ แรงงาน ไทร อัม พ์ อินเตอร์ เน ชั้น แนล แห่ง ประเทศไทย   รวมถึง ตรวจสอบ การ เลิกจ้าง และ การ ทำลาย สหภาพ แรงงาน ใน กรณี อื่นๆ   \n","       \n","   4. ขอให้   un   เรียกร้อง และ บังคับ ให้ บริษัท   ปฏิบัติตาม   code   of   conduct   ของ กลุ่ม บริษัท ไทร อัม พ์ฯ   ซึ่ง มุ่งเน้น ถึง ความสำคัญ ของ การปกป้อง สิทธิมนุษยชน   ดังที่ กล่าว ไว้ ใน ปฎิญญา ว่าด้วย สิทธิมนุษยชน   ( the   “ general   declaration   of   human   rights )   และ พร้อมใจ ปฎิบัติ ตาม ข้อตกลง ที่ เกี่ยวข้อง ของ องค์กร แรง ระหว่างประเทศ   ( ilo )   และ ข้อตกลง ว่าด้วย ความร่วมมือ   ของ องค์การสหประชาชาติ   ใน ด้าน กฎระเบียบ และ การ พัฒนาการ ปฎิบัติ งาน และ สภาวะ ทางเศรษฐกิจ   รวมถึง มาตรฐานสากล ของ องค์กร สหประชาชาติ   ( un   global   compact )   \n","       \n","   5.   ขอให้   un   เรียกร้อง และ บังคับ ให้ บริษัท เปิดเผย ข้อมูล ตาม หลัก มาตรฐานแรงงาน สากล และ แนวปฏิบัติ ของ บรรษัทข้ามชาติ   ( oecd   guidelines )   ตาม หลัก   oecd   guidelines   for   mnes   และ เปิดโอกาส ให้ องค์กร ของ ลูกจ้าง หรือ สหภาพ แรงงาน เข้าร่วม การปรึกษาหารือ กับ ฝ่ายบริหาร ใน การ ดำเนิน นโยบาย ต่างๆ   ที่ อาจ ส่ง ผลกระทบ ต่อ   ชีวิต ความเป็นอยู่ ที่ ดี ของ คนงาน   จะ เป็นการ ลด ความตึงเครียด   ความขัดแย้ง ระหว่าง ฝ่าย แรงงาน กับ ฝ่ายบริหาร   เพราะ การปรึกษาหารือ เป็นการ เคารพ ใน สิทธิมนุษยชน ของ คนงาน   เป็น หลักปฏิบัติ ที่ ใช้ กัน ใน สังคม ประชาธิปไตย   และ หลัก   oecd   guidelines   for   mnes   ใน เรื่อง ของ การ จ้างงาน และ แรงงานสัมพันธ์   อีก เช่นกัน   เพื่อ นำไปสู่ การ หา ทางออก ที่ สร้างสรรค์ และ ลด ผลกระทบ ของ การ เลิกจ้าง ใน กรณี ของ การ เลิกจ้าง พร้อมกัน หลาย คน   ซึ่ง สอดคล้อง กับ มาตรการ เร่งด่วน ระยะ   1   ปี   ของ รัฐบาล ไทย ใน เรื่อง ของ การรักษา และ เพิ่ม รายได้ ของ ประชาชน   ที่ เสนอ ให้ ร่วมมือ กับ ภาคเอกชน ใน การ ดำเนิน มาตร การชะลอ การ เลิกจ้าง และ ป้องกัน การ ขยายตัว ของ การ เลิกจ้าง   แน่นอน ทาง สหภาพแรงงานฯ มั่นใจ ว่า มี อีก หลาย ทางเลือก ที่ สร้างสรรค์ และ ก่อให้เกิด สันติภาพ   \n","       \n","   ใน ระยะ ต่อมา เรา   \n","       \n","   1. ขอให้ พิจารณา ความรุนแรง เชิง โครงสร้าง จาก แนว นโยบาย เสรีนิยม ใหม่ ของ รัฐบาล   อย่าง   การ แปรรูป กิจการ ที่ เป็นการ ให้ สวัสดิการ แก่ ประชาชน ของ รัฐ ต่างๆ   ให้ เป็น กิจการ ของ เอกชน   เช่น   การนำ มหาวิทยาลัย ออก นอก ระบบ   การ แปรรูป รัฐวิสาหกิจ   รวมถึง   การ เปิด ให้ มี การ จ้าง งานเหมา ค่าแรง ที่ นายจ้าง ไม่ต้อง รับผิดชอบ ต่อ สวัสดิการ ต่อ ลูกจ้าง   การ เปิด เขต เสรี ทางการค้า   ที่ มี การ ห้าม การ ตั้ง สหภาพ แรงงาน   เป็นต้น   \n","       \n","   2. ขอให้ สนับสนุน ให้ ผู้ใช้แรงงาน และ ผู้ เสียเปรียบ ใน สังคม อื่นๆ   มี ความมั่นคง ทั้ง ทางเศรษฐกิจ และ สังคม   ผ่าน การ ส่งเสริม ให้การ กระจาย รายได้ อย่างเป็นธรรม   ลด ช่อง ทางสังคม   เช่น   การ ส่งเสริม ให้ มี หลักประกัน สุขภาพ   ที่อยู่อาศัย   การศึกษา   รายได้ และ การมีงานทำ โดย รัฐ ที่ ทั่วหน้า และ มีประสิทธิภาพ   การ ยกเลิก ภาษีมูลค่าเพิ่ม ที่ ผลัก ภา ระมา ให้ แรงงาน หรือ ผู้มีรายได้ น้อย   เป็นต้น   \n","       \n","   เพื่อให้ เกิด การ ให้ ความเคารพ ต่อ ชีวิต ทั้งมวล   เคารพ ชีวิต และ ศักดิ์ศรี ของ แต่ละบุคคล   โดย ไม่ แบ่ง ชนชั้น หรือ ลำเอียง   การ ไม่ ใช้ ความรุนแรง ใน ทุก รูปแบบ   การ แบ่งปัน กับ ผู้อื่น อย่าง มีน้ำใจ   เพื่อ ขจัด การ แบ่งแยก   ความ ไม่ ยุติธรรม   และ การ กดขี่ ทางการเมือง และ เศรษฐกิจ   การ รับฟัง เพื่อให้ เกิด ความเข้าใจ ต่อกัน   เคารพ เสรีภาพ ใน การแสดงออก   และ ยอมรับ ความแตกต่าง ทางวัฒนธรรม   การสงวนรักษา ผืน โลก   ฝึก ดำเนินชีวิต อย่าง รับผิดชอบ และ เคารพ ต่อ ทุก ชีวิต ใน โลก   เพื่อ รักษา สมดุล ของ ธรรมชาติ บน ผืน โลก   การ สร้าง ความสมานฉันท์   เคารพ ต่อ หลักการ ประชาธิปไตย   และ ให้โอกาส ทุกฝ่าย มีส่วนร่วม อย่าง เท่าเทียมกัน   อย่างแท้จริง   ตามที่ สหประชาชาติ ได้ กำหนดให้ ปี   ค.ศ. 2001 - 2010   เป็น   “ ทศวรรษ สากล เพื่อ วัฒนธรรม สันติภาพ และ ความ ไม่ รุนแรง เพื่อ เด็ก ของ โลก ”   \n","       \n","   เรา จึง ขอ ประกาศ ไว้   ณ   หน้า สำนักงาน สหประชาชาติ ประจำ กรุงเทพมหานคร   ใน วัน สันติภาพ โลก   ประจำปี   2552   \n","       \n","   สหภาพ แรงงาน ไทร อัม พ์ อินเตอร์เนชั่นแนล แห่ง ประเทศไทย   \n","       \n","       \n","  ,xxbos   \n","   ประชา ไท -   แม้ว่า นายกรัฐมนตรี มาเลเซีย จะ เคย บอก ให้ คณะรัฐมนตรี และ หน่วยงาน ต่างๆ   ของ รัฐ ว่า ให้ เคารพ ใน เสียง วิพากษ์วิจารณ์ ของ สื่อ และ ให้ ยอมรับ ว่า   นี่ เป็น เสียงสะท้อน ที่จะ ต้อง นำ ปรับปรุง การทำงาน   ทว่า   การ โจมตี ผู้สื่อข่าว ทาง อินเตอร์ และ ผู้ เป็นเจ้าของ เว็บ ไซ ด์ ก็ ไม่มี ท่าที ว่า จะ ลดลง   \n","   \n","       \n","   \n","   ล่าสุด เมื่อ ไม่ กี่ วัน มา นี้ เจ้าของ เว็บ ไซ ด์   \" xxurl   ซึ่ง เป็น เว็บ ที่ นำเสนอ เกี่ยวกับ เรื่องราว ข่าวสาร เกี่ยวกับ สถานการณ์ มาเลเซีย ปัจจุบัน   ก็ ถูก บังคับ ให้ ถอด ลิงค์ จาก ไซ ด์ ของ เขา ออก ไป จาก เว็บ ไซ ด์   xxurl   [ 1 ]   หลังจากที่ ได้รับ อีเมล์ เตือน ใน วันที่   15   ก.ค. ที่ผ่านมา จาก เจ้าหน้าที่ตำรวจ แผนก สืบสวน ด้าน มัลติมีเดีย และ อาชญากรรม ทาง อินเตอร์เน็ต   \n","   \n","       \n","   \n","   แมค   xxunk ลิ   บอก กับ ศูนย์ เพื่อ สื่อมวลชน อิสระ   ( center   for   independent   xxunk ว่า เขา ถูก เตือน ว่า เขา อาจจะ ถูก ตรวจค้น และ ตั้ง ข้อกล่าวหา ตามกฎหมาย ว่าด้วย ความลับ ของ ทางราชการ ได้   เนื่องจาก การ เขา xxunk ทางราชการ   ทั้งนี้   คน ที่ ถูก ตั้ง ข้อกล่าวหา ใน กรณี ดังกล่าว อาจจะ ถูก จำคุก ตั้งแต่   1   ปี   ถึง   7   หาก พบ ว่า กระทำความผิด จริง ตามกฎหมาย ดังกล่าว หมวด ที่   8   \n","   \n","       \n","   \n","   แม้ว่า ขณะนี้   เว็บ ไซ ด์   xxurl .   จะ ไม่ สามารถ ออนไลน์ ได้ แล้ว ใน มาเลเซีย   แต่ เว็บ ดังกล่าว ยัง บรรจุ ข้อมูล รายงาน ของ ทาง ตำรวจ และ การสื่อสาร กัน ภายใน ของ ตำรวจ เอง   ซึ่ง ไม่มี ข้อมูล ใด ที่ มี เครื่องหมาย บอ กว่า ข้อมูล ที่ เป็นความลับ เลย   \n","   \n","       \n","   \n","   ใน ปี   2005   เว็บ ดังกล่าว ได้ นำเสนอ รายงาน ที่ กล่าว หาว่า   อาชญากร สร้าง หลักฐาน ปลอม ว่า ตนเอง ตาย ด้วย ความช่วยเหลือ เจ้าหน้าที่ตำรวจ ระดับสูง จำนวน หนึ่ง   และ ได้ เสนอ เป็นข่าว หน้าหนึ่ง ใน หนังสือพิมพ์ มาเลเซีย หลาย ฉบับ   \n","   \n","       \n","   \n","   ก่อนหน้านี้ เมื่อ วันที่   14   กรกฎาคม   เว็บ ไซ ด์ ที่ ลิงค์ กับ ฝ่ายค้าน   \" malaysia   today \"   ซึ่ง บรรณาธิการ   คือ   ราช   เพ ตรา   กามา รุด ดิน   ถูก ยึด คอมพิวเตอร์ ไป   2 เครื่อง จาก บ้าน ของ เขา ใน สุ ไห ง   บาล อห ์ ใกล้ ๆ   กับ กัวลาลัมเปอร์   เมืองหลวง ของ มาเลเซีย   จนกระทั่งบัดนี้ เขา ก็ ยัง ไม่ได้ คอมพิวเตอร์ คืน   \n","   \n","       \n","   \n","   ใน วันที่   8   กรกฎาคม   เขา เคย ถูก สืบสวน เป็นเวลา   2   ชั่วโมง ภายใต้ กฎหมาย การ ปลุกระดม มวลชน   ปี   1960   เนื่องจาก นำเสนอ ข่าว ที่ เกี่ยวข้อง กับ เรื่อง ราชวงศ์ เน กรี   เซ บิ ลัน   โดย กล่าว หาว่า   คอรัปชั่น และ แทรกแซง ทางการเมือง   \n","   \n","       \n","   \n","   ข้อหา การ ปลุกระดม มวลชน เพื่อ ต่อต้าน รัฐบาล ใน มาเลเซีย นั้น เป็น ประเด็น ที่ ให้ คำ จัด ความ ไว้ กว้าง มาก   แต่ ได้ มี บท ที่ ตั้งใจ ป้องกัน การ ตั้งคำถาม ต่อ ตำแหน่ง ผู้ปกครอง มาเลเซีย   จำเลย อาจ สามารถ ถูก ตัดสิน จำคุก   3 - 5   ปี   \n","   \n","       \n","   \n","   มี ความคืบหน้า เกี่ยวกับ ประเด็น เดียวกัน คือ   เมื่อ วันที่   12   กรกฎาคม   2005   ตำรวจ ได้ xxunk ใน การสืบสวน เว็บ ไซ ด์ ข่าว อิสระ   \"   xxurl       ทั้งนี้   การสืบสวน ภายใต้ กฎหมาย การ ปลุกระดม มวล xxunk   1960 ถูก xxunk ประกาย ขึ้น มา โดย การกล่าวหา ว่า มี การ ตีพิมพ์ จดหมาย ที่ เป็นการ ปลุกระดม ใน เว็บ ไซ ด์ เกี่ยวกับ นโยบาย การเล่นพรรคเล่นพวก ของ รัฐบาล มา เลย ์   และ ผล ก็ คือ   การจับกุม   คอมพิวเตอร์   15   เครื่อง   และ   เซิร์ฟเวอร์   4   เครื่อง จาก สำนักงาน   malaysiakini   ใน กัวลาลัมเปอร์   เมื่อ วันที่   20   มกราคม   2003   \n","   \n","       \n","   \n","   อย่างไรก็ตาม   ทาง เว็บ ไซ ด์ ดังกล่าว รายงาน ว่า   ตำรวจ ได้คืน เซิร์ฟเวอร์   2   ตัว สุดท้าย คืน มา ให้ ด้วย ซึ่ง เดิม นั้น ทาง มาเลเซีย กินี ได้ บริจาค เครื่องคอมพิวเตอร์ ทั้ง   17   เครื่อง ที่ ได้คืน มา ให้ กับ ตำรวจ ไป แล้ว ตั้งแต่ ปี     2003   \n","   \n","       \n","   \n","   พันธมิตร สื่อมวลชน เอเชียตะวันออกเฉียงใต้   ( southeast   asia   press   xxunk   seapa )   ซึ่ง เป็น องค์กร ที่ สนับสนุน การทำงาน และ พิทักษ์ สิทธิ ของ สื่อ อิสระ ใน กลุ่ม ประเทศ อาเซียน ตั้ง ข้อ สังเกตว่า   การตัดสินใจ ของ เจ้าหน้าที่ ปิด การ ไต่สวน คดี ของ มาเลเซีย กินี ได้ เลยกำหนด มา อย่าง ยาวนาน   และ เรื่อง ที่ ได้ ยึด เอา คอมพิวเตอร์ ของ มาเลเซีย กินี ไป ไว้ ถึง   2   ปี   แสดงให้เห็น ว่า   มาตรการ เหล่านั้น ได้ ถูก นำมาใช้ เพื่อ ข่มขู่   สำนักพิมพ์ ทาง เว็บ   \n","   \n","       \n","   \n","   ทั้งนี้ ทาง   seapa   ได้ ออก แถลงข่าว ที่ แสดงถึง ความกังวลใจ อย่างยิ่ง ว่า   รัฐบาล มาเลเซีย จะ ยังคง ใช้ พระราชบัญญัติ ว่าด้วย ความลับ ของ ทางราชการ และ   พ.ร.บ. ปลุกระดม มวลชน เป็น เครื่องมือ ใน การ ต่อต้าน เว็บ ไซ ด์ ข่าว อื่นๆ อีก   ทั้งๆ ที่ รัฐบาล รับ xxunk ว่า จะ คุ้มครอง เสรีภาพ ใน การแสดงความคิดเห็น ทาง สื่อ อินเตอร์เน็ต ก็ตาม   \n","   \n","       \n","   \n","   ที่มา :   southeast   asian   press   alliance   ( seapa )   \n","  \n","y: LMLabelList\n",",,,,\n","Path: /content/drive/MyDrive/ject_final_petch/prachathai_data;\n","\n","Valid: LabelList (15 items)\n","x: LMTextList\n","xxbos   เรื่อง ที่เกิด ขึ้นกับ ประเทศไทย ใน ห้วง หลาย ปี มา นี้   นับว่า เป็น ปรากฏการณ์ อัน แปลกประหลาด ระดับโลก ได้ เลย ทีเดียว   ค่าที่ เป็น กรณี อัน ไม่มีใคร เหมือน แล ไม่ เหมือน ใคร   สำนวน หนัง xxunk เรียก ว่า   “ only   in   thailand ”   อัน อาจ xxunk ได้ ดังนี้   \n","   1 )   เป็น ประเทศ เดียว ใน โลก จริงๆ   ที่ มี การขับไล่ รัฐบาล อัน มาจาก การเลือกตั้ง   ด้วย การ xxunk เรียก ทหาร ให้ มา ทำ การปฏิวัติ   ครั้น ก่อการ สำเร็จ   ก็ มี คน นำ เอา ดอกไม้ ไป คล้อง ปากกระบอกปืน รถถัง   จน เป็น ภาพ ข่าว ฮือฮา ไป ทั่วโลก   \n","   2 )   เป็น ประเทศ ที่ ไม่มีปัญหา เรื่อง ชนกลุ่มน้อย ให้ ปวด สมอง เมื่อ เทียบ กับ เพื่อนบ้าน อย่าง พม่า   อัน มี ทั้ง ไทย ใหญ่   มอญ   กะเหรี่ยง   คะ ฉิ่น   คะ ยา   ฯลฯ   ซึ่ง เป็น   xxunk   xxunk   ที่ รัฐบาล ของ เขา เอง ก็ กังวล อยู่ ไม่น้อย   ครั้น ปัญหา ไม่มี   จึง แบ่งกลุ่ม กันเอง โดย ใช้ สี เสื้อ เป็น xxunk   เริ่ม จาก   “ เหลือง ”   ก่อน   แล้วก็ มา   “ แดง ”   บางครั้ง ก็ มี   “ ขาว ”   โผล่ มา xxunk   หลัง สุด มาแรง คือ   “ น้ำเงิน ”   เหมือน ประเทศ มี มหกรรม กีฬา สี   เพียงแต่ เป็น กีฬา สี ที่ กะ เอา กัน ถึงตาย   แต่ละ กลุ่ม จึง ออก แนว อาฆาตมาดร้าย กัน สุด ๆ   ทั้งที่ ต่าง ก็ ถือ บัตรประชาชน ออก โดย กระทรวงมหาดไทย ใบ เดียวกัน แท้ๆ   \n","   เป็น กีฬา สี xxunk ที่ ไม่รู้ จะ จบ ลง เมื่อไร   และ อย่างไร   นับว่า เป็น ปรากฏการณ์   only   in   thailand   จริงๆ   \n","   3 )   เป็น ประเทศ ที่ มี รัฐบาล   แต่ บริหาร ราชการ แผ่นดิน ไม่ได้   รัฐบาล ชุด หนึ่ง   นายกฯ   ถูก ให้ ออก เพราะ ไป สอน xxunk ทาง โทรทัศน์   รัฐบาล ชุด ต่อมา ต้อง ด่วน xxunk   เพราะ พรรค ถูก ศาลรัฐธรรมนูญ สั่ง ยุบ   ทั้งที่ ประชาชน ลงคะแนนเสียง เลือก เข้ามา มาก สุด   ( เป็นเรื่อง ประหลาด ซ้อน ประหลาด   –   ผู้เขียน )   รัฐบาล หลัง สุด ก็ ยิ่ง บริหาร ลำบาก   เพราะ เป็น พรรค ร่วม เสียง xxunk น้ำ   ต่าง เกรงใจ กัน ไป เกรงใจ กัน มา   นายกฯ   เอง ลง พื้นที่ แต่ละ ที ต้อง มี กำลัง อารักขา เป็น เรือน พัน   ครั้น จะ จัดงาน ประชุม นานาชาติ ก็ ต้อง ใช้ กองกำลัง จาก   3   เหล่าทัพ มา ดูแล โรงแรม ที่พัก ของ xxunk   โรงแรม ละ   1   เหล่าทัพ   \n","   อย่างนี้   ไม่ เรียก   “ only   in   thailand ”   จะ ให้ เรียก ว่า อะไร หนอ ?!   \n","   4 )   เป็น ประเทศ ที่ รัฐบาล มี แนวคิด ใน การ จัดการ เรื่อง ใดๆ   ไม่ เหมือน ใคร   และ ไม่มีใคร เหมือน   ยกตัวอย่าง กรณี การแก้ปัญหา โรคระบาด ไข้หวัดใหญ่ สายพันธุ์ ใหม่   2009   แทนที่จะ มุ่ง   “ ออก มาตรการ ”   ต่างๆ   มา ควบคุม การ แพร่ระบาด ของ เชื้อโรค   กลับ ไป   “ แก้ไข ตาราง การ รายงาน สถานการณ์ ของ โรค ”   โดย เปลี่ยน จาก รายงาน ทุกวัน เป็น อาทิตย์ ละ หน แทน   \n","   เจ้า   “ หวัด ใหญ่ ”   ก็ เลย xxunk   xxunk ว่า จะ ลง หลัก ปัก ฐาน เป็น   xxunk   xxunk   อยู่ ใน สยาม ประเทศ ให้ xxunk กัน ไป เลย   ( สถานการณ์ ไข้หวัดใหญ่ ตัว นี้ ที่ ประเทศ เม็กซิโก และ สหรัฐอเมริกา สงบ ลง จน แทบจะ ไม่มีใคร พูด ถึงกัน แล้ว ใน ตอนนี้   ขณะที่ ประเทศไทย ยังคง มี ข่าว การ แพร่ระบาด และ ผู้คน เสียชีวิต ขึ้น หน้าหนึ่ง หนังสือพิมพ์ แทบ ทุกวัน   –   ผู้เขียน )   \n","   5 )   เป็น ประเทศ ที่ อุดม ไป ด้วย   “ โฆษก ”   เฉพาะ ฝั่ง รัฐบาล เอง ก็ ปา เข้าไป อย่างน้อย   3   หน่อ   คือ   โฆษก รัฐบาล   หรือ โฆษก ประจำ สำนักนายกรัฐมนตรี   ( อาจารย์   ปณิธาน )   โฆษก พรรคประชาธิปัตย์   ( เป็น หมอ ชื่อ   บุ xxunk   และ โฆษก ประจำตัว หัวหน้าพรรค ปชป.   ( เป็น   สส.   ชื่อ   เทพ ไท )   แต่ละ ท่าน ก็ มี หน้าที่ ออกมา   “ แย่งชิง พื้นที่ ข่าว ”   อันเป็น วิถี ทางการเมือง ที่ เชื่อ กัน ว่า จะ ทำให้ ฝ่าย ของ ตน ได้เปรียบ คู่แข่งขัน   ซีก ฝ่ายค้าน   ( พรรค เพื่อ ไทย )   ที่ มีบทบาท เด่น หน่อย ก็ เห็น จะ เป็น โฆษก พรรค   ( อดีต พระเอก หนัง ชื่อ   พร้อม พงศ์   นพ ฤทธิ์ )   แถม ให้ อีก   1   ก็ คือ   ส.ส.   เชียงใหม่ อย่าง คุณ สุ รพ งษ์   โต xxunk ชัย กุล   ซึ่ง ก็ ไม่ได้ เป็น โฆษก อะไร   แต่ มีชื่อ xxunk อยู่ บน กระดานข่าว ค่อน ข้างมาก   \n","   เหตุ ที่   “ โฆษก ”   มีความสำคัญ ก็ เพราะ   บ้านเมือง นี้ เขา อยู่ กัน ได้ ด้วย กระแส   วัน ๆ   ผู้มีอำนาจ จึง ต้อง คิด ให้ ได้ ว่า จะ ใช้ ข่าว อะไร ไป กลบ ข่าว ที่ กำลัง เป็น ภัย กับ ตนเอง   อาทิเช่น   อาศัย ข่าว คดีลอบสังหาร แกนนำ พันธมิตร   กลบ ข่าว เสื้อ แดง ถวาย ฏีกา   แล้ว ให้ข่าว ปลด ผู้บัญชาการ ตำรา จ แห่งชาติ มา กลบ ข่าว คดีลอบสังหาร ฯ   อีก ทอด หนึ่ง   ที่ ฮือฮา สุด ก็ คือ ข่าว   “ หมี แพนด้า xxunk ”   บดบัง เรื่อง รัฐบาล บริหาร เศรษฐกิจ xxunk เสีย xxunk นาน เป็น xxunk เลย ทีเดียว   \n","   สงคราม ยึด พื้นที่ ข่าว จึง เป็นเรื่อง   “ only   in   thailand ”   ที่ ชัดเจน อีก เรื่อง หนึ่ง   \n","   6 )   เป็น ประเทศ เดียว ใน โลก ที่ ประชาชน มี ความอดทน กับ นักการเมือง สูง มาก ๆ   ดัง ตัวอย่าง   \n","   ( 1 )      ส.ส.   ใน สภา ยกมือ อนุมัติ ให้ รัฐบาล กู้เงิน ตั้ง   800,000   ล้าน บาท ไป ทำ อะไร ก็ ไม่รู้   ...   ประชาชน ก็ ไม่ว่า อะไร   \n","   ( 2 )      ส.ว.   ( ซึ่ง ควร ต้อง เป็นกลาง )   กิน เงินเดือน เป็น เรือน แสน   วัน ๆ   ทำ แต่ เรื่อง ไม่ สมาน ฉันท์   ...   ประชาชน ก็ ไม่ว่า อะไร   \n","   ( 3 )      คน เป็น รัฐมนตรี สั่ง ให้ ข้าราชการ ตั้งโต๊ะ ล่า รายชื่อ ชาวบ้าน มา สู้ กับ การเคลื่อนไหว ทางการเมือง ของ ภาค ประชาชน   ...   ประชาชน ก็ ไม่ว่า อะไร   \n","   ฯลฯ   \n","   เรื่อง ชวน xxunk ที่เกิด ขึ้นกับ สยาม ประเทศ ใน ห้วง หลาย ปี มา นี้ ยัง มี อีก มาก   แต่ละ เรื่อง ล้วน บั่นทอน การพัฒนา ชาติ ประเทศ ไม่ ให้ ก้าว รุด ไป ข้างหน้า เพื่อ จะ ได้ ทัดเทียม ประเทศ ที่ เจริญ แล้ว ทั้งหลาย   บาง เรื่อง ก็ เป็นปัญหา ใน เชิง โครงสร้าง   บาง เรื่อง เป็นปัญหา ด้านวัฒนธรรม   แต่ หลาย เรื่อง กลับ เป็นความ อ่อน ด้อย ทั้ง ใน ทาง คุณภาพ และ ทาง ทัศนคติ ของ ผู้คน   \n","   เป็น   “ ปัญหา ใน องค์ รวม ”   ที่ ต้อง รีบ หา จุดเริ่มต้น เพื่อ การ แก้ไข โดยเร็ว   ซึ่ง ก็ ไม่รู้ ว่า ใคร ควร เป็นเจ้าภาพ   แต่ ความจริง ก็ คือ เวลา ที่ มี อยู่ เหลือ น้อยลง ไป xxunk แล้ว   \n","   \n","       \n","  ,xxbos   วันนี้   ( 22   ก.ค. 52 )   ที่ โรงแรม เชอ รา ตัน   แกรนด์   ลา กู น่า   ภูเก็ต   จ. ภูเก็ต   เวลา   18.00 น.   นาง ฮิลลารี   xxunk แฮม   คลินตัน   รมว. ต่าง ประเทศสหรัฐอเมริกา   แถลง   ก่อน เข้าร่วม ประชุม กับ รัฐมนตรี ต่างประเทศ อาเซียน ว่า   สหรัฐอเมริกา กลับมา แล้ว   ( the   us   is   back )   เพื่อ แสดงออก ถึง ภาร ความผูกพัน ของ สหรัฐอเมริกา ที่จะ เสริม ขยาย ความสัมพันธ์ ใน ภูมิภาค นี้   จึง ได้มา ร่วม ประชุม รัฐมนตรี ต่างประเทศ   และ ก็ได้ เริ่ม กระบวนการ โดย การ เซ็นต์ สัญญา ไมตรี และ ความร่วมมือ เอเชียตะวันออกเฉียงใต้ กับ อาเซียน   เรา จะ สร้างความสัมพันธ์ ที่ ลึกซึ้ง และ มี พล วัตร ใน ฐานะ หุ้นส่วน กับ อาเซียน   \n","       \n","   สหรัฐอเมริกา กลับมา ใน เอเชียตะวันออกเฉียงใต้ แล้ว   ประธานาธิบดี โอ บา มา และ ใน ส่วนตัว เชื่อ ว่า ภูมิภาค นี้ มีความสำคัญ ต่อ สันติภาพ และ ความมั่งคั่ง ของ โลก   และ เรา ตั้งใจ ที่จะ มี ความเกี่ยวพัน xxunk กับ หุ้นส่วน ใน อาเซียน   ในขณะที่ เรา ก็ พบ กับ ความ ท้าทาย หลายอย่าง อยู่ ต่อหน้า   จาก เรื่อง ความมั่นคง ระดับโลก และ ระดับ ภูมิภาค ไป จนถึง ปัญหา เศรษฐกิจ โลก   ปัญหา สิทธิมนุษยชน และ การเปลี่ยนแปลง สภาพภูมิอากาศ   ( โลก ร้อน )   หลังจากนี้ ตน ใน ฐานะ ผู้แทน สหรัฐ จะ เข้าไป เซ็นสัญญา ไมตรี กับ อาเซียน   โดย สัญญา นี้ ได้ ผนึก แน่น ความสัมพันธ์ ของ เรา ว่า จะ ร่วม ทำงาน ใน ฐานะ หุ้นส่วน ใกล้ชิด   \n","   รัฐมนตรี ต่างประเทศ สหรัฐ   กล่าว ด้วยว่า   สหรัฐอเมริกา จะ ส่ง ผู้แทน ทาง การทูต ไป ประจำ ที่ สำนัก เลขาธิการ อาเซียน   ที่ กรุง จาการ์ตา   ประเทศ อินโดนีเซีย   ใน ระยะเวลา อัน ใกล้ นี้   โดย ได้ เน้นย้ำ กับ คณะทำงาน ทุกคน ให้ ใส่ใจ กับ สำนัก เลขาธิการ อาเซียน ให้ มาก และ รัฐบาล อินโดนีเซีย ด้วย   ซึ่ง รัฐบาล สหรัฐ เห็น ความสำคัญ ที่จะ ทำ เรื่อง นี้   \n","       \n","   นาง ฮิลลารี กล่าว อี กว่า   พรุ่งนี้   ( 23   ก. ค)   จะ มี การประชุม ครั้งแรก ระดับ รัฐมนตรี ระหว่าง สหรัฐ กับ ประเทศ ใน ลุ่ม แม่น้ำ โขง ตอน ล่าง   ได้แก่   กัมพูชา   ลาว   เวียดนาม   และ ประเทศไทย   เพื่อ ปรึกษาหารือ ใน เรื่อง น้ำ   สิ่งแวดล้อม   สาธารณสุข   โดย ขอ ย้ำ ว่า รัฐบาล ประธานาธิบดี โอ บา มา ต้องการ มี ความสัมพันธ์ อย่างลึกซึ้ง กับ เอเชีย   และ ใส่ใจ กับ วิกฤติ ความเปลี่ยนแปลง ของ สภาพภูมิอากาศ   โดย ได้ ขอให้ สภา คองเกรส เพิ่ม เงินช่วยเหลือ อีก   7   เท่า   สำหรับ กองทุน ใน เรื่อง ดังกล่าว เพื่อ ใช้ ใน ภูมิภาค นี้   และ จะ เปิดตัว วิทยาการ ใหม่ ซึ่ง จะ รวมถึง การวิจัย   การลงทุน และ นวัตกรรม ใหม่ ๆ   เกี่ยวกับ สภาพแวดล้อม เพื่อที่จะ สร้าง แนว ทางการ แก้ไขปัญหา ที่จะ เกิดขึ้น ใน เอเชีย   โดยเฉพาะอย่างยิ่ง ใน เอเชียตะวันออกเฉียงใต้ ที่ น่าจะ จัด ให้ มี การประชุม ระดับสูง เรื่อง สภาพแวดล้อม ภายใน อาเซียน   \n","   เมื่อ ทำงาน ร่วมกัน เรา ก็ หวัง ว่า จะ ช่วย โลก   เผชิญหน้า กับ ปัญหา สภาพอากาศ ที่ ร้อน ขึ้น และ เปลี่ยน ผ่าน ไป สู่ การ ใช้ พลังงาน ที่ สะอาด   อนาคต ที่ ถือว่า เป็น ก้าวย่าง ที่ สำคัญ สำหรับ เอเชียตะวันออกเฉียงใต้ และ อาเซียน และ ก็ เป็น อนาคต ที่ สำคัญ ของ เรา ด้วย   \n","   นาง ฮิลลารี กล่าว ต่อมา ว่า   สมาชิก   10   ประเทศ ของ อาเซียน มี ประชากร เกือบ   600   ล้าน คน   รวมทั้ง พันธมิตร หลัก ของ สหรัฐ   2   ชาติ ใน นี้   ( ไทย - ฟิลิปปินส์ )   และ ประเทศ ประชาธิปไตย ที่ ใหญ่ เป็น อันดับ   3   ของ โลก ซึ่ง เพิ่ง ผ่าน การเลือกตั้ง ที่ xxunk   ( อินโดนีเซีย )   ภูมิภาค นี้ เป็น คู่ ค้า ใหญ่ อันดับ   6   ของ สหรัฐ   โดย มี การลงทุน ของ สหรัฐ อยู่ ใน ภูมิภาค นี้ มากกว่า อยู่ ใน จีน เสีย   และ ที่ สำคัญ ยัง เป็น เส้นทาง การขนส่ง ทางเรือ ที่ สำคัญ   เพราะฉะนั้น อาเซียน จึง เป็น ภูมิภาค แห่ง ความหลากหลาย ที่ สำคัญ ยิ่ง   ซึ่ง คน ประกอบ ไป ด้วย ประชากร ที่ แตก ต่างกัน ทั้ง เรื่อง ศาสนา   วัฒนธรรม   อะไรก็ตาม ที่ หลากหลาย ใน ประสบการณ์ ของ มนุษยชาติ สามารถ สร้าง เป็น ชุมชน ได้ ที่นี่   เพิ่งจะ สัปดาห์ นี้ เอง ที่ กลุ่ม อาเซียน ได้ ตกลง กัน สร้าง แนวทาง ของ เรื่อง สิทธิมนุษยชน   ซึ่ง จะ ช่วย สนับสนุน ส่งเสริม คุณ ค่าที่ เรา มี อยู่ ร่วมกัน   และ หวัง อย่างยิ่ง ว่า นี่ จะ เป็น ส่วน ช่วย ใน การพัฒนา พม่า ด้วย   \n","   นอกจากนี้   รัฐมนตรี ต่างประเทศ สหรัฐ   ยัง กล่าว แสดง ความเสียใจ อย่าง xxunk   ต่อ เหตุการณ์ ระเบิด การก่อการร้าย ใน อินโดนีเซีย   เมื่อ อาทิตย์ ที่แล้ว   โดย ระบุ ว่า   “ เรา ยืน อยู่ ข้าง อินโดนีเซีย ใน เวลา ที่ ยุ่งยาก นี้   และ ขอ เฉลิมฉลอง ต่อ ความสำเร็จ ใน การเลือกตั้ง และ ต่อ ความเป็นประชาธิปไตย ”   \n","   นาง ฮิลลารี   กล่าว ด้วยว่า   รัฐบาล โอ บา มา มี ความภูมิใจ ที่จะ ส่งเสริม ความสัมพันธ์ ใน ทาง ยุทธศาสตร์ กับ อินโดนีเซีย   และ จะ ขยาย ความสัมพันธ์ ใน ทุกๆ   ด้าน   ทั้ง ใน ทาง การทูต   การเมือง   และ ใน ทาง ยุทธศาสตร์   โดย ให้ ความมั่นใจ ว่า จะ ได้ผล ใน ทาง ดี   นี่ ถือว่า เป็น ก้าว สำคัญ ที่ สหรัฐ ต้องการ ที่จะ ก้าว ไป พร้อมกับ อาเซียน และ สนับสนุน ให้ อาเซียน ก้าว ไป ข้างหน้า ด้วย   \n","   ย้ำ คุย อนาคต ของ เกาหลีเหนือ   ภายใต้ เงื่อนไข ต้อง เลิก โครงการ นิวเคลียร์   \n","   ใน ส่วน การประชุม ร่วมกับ รัฐมนตรี ใน   6   ชาติ   ( จีน   ญี่ปุ่น   เกาหลีเหนือ   เกาหลีใต้   สหรัฐ   รัสเซีย )   นาง ฮิลลารี   กล่าวว่า จะ มี การหารือ เรื่อง การบังคับใช้ ข้อ มติ   xxunk   ของ คณะมนตรีความมั่นคง แห่ง สหประชาชาติ   รวมถึง หารือ ถึง มาตรการ ของ แต่ละ ชาติ และ มาตรการ โดยรวม ที่ ใช้ ร่วมกัน ทั้ง มาตรการ ด้าน การเงิน   การ ห้าม การค้า อาวุธ   และ การ เข้าไป ตรวจ อาวุธ และ มาตรการ อื่นๆ   ตาม ข้อ มติ   ทั้งนี้   รัฐมนตรี คนอื่นๆ   อีก   4   คน   ไม่ รวม สหรัฐ   ( จีน   ญี่ปุ่น   เกาหลีใต้   รัสเซีย )   ก็ เห็นด้วย กับ มาตรการ ที่ว่า นี้   เพื่อที่จะ แสดงออก ถึง ความปรารถนา อย่างชัดเจน ว่าการ ขจัด นิวเคลียร์   เป็น สิ่ง เดียว ที่ ควรจะ ทำ สำหรับ เกาหลีเหนือ   \n","   “ เรา ไม่ได้ ต้องการ ให้รางวัล เกาหลีเหนือ เพียง เพราะว่า เขา จะ กลับ มาสู่ โต๊ะ การ เจรจา   หรือ จะ ให้รางวัล กับ สิ่ง ที่ เขา สัญญา ว่า จะ ทำ แล้ว ยัง ไม่ได้ ทำ   สิ่ง นี้ ได้ เกิดขึ้น ตั้ง นาน แล้ว   และ ก็ ขึ้นอยู่กับ เขา เอง ว่า จะ ทำตาม หรือไม่   และ ถ้าหาก ไม่ ทำ ก็ จะ เผชิญหน้า กับ การ โดดเดี่ยว จาก นานาชาติ และ การกดดัน จาก การ คว่ำบาตร ของ โลก ”   \n","   เมื่อ ถามถึง แผนการ ลด อาวุธ นิวเคลียร์ ใน เกาหลีเหนือ อย่าง เป็น รูปธรรม   นาง ฮิลลารี   ตอบ ว่า   สหรัฐฯ   จีน   รัสเซีย   ญี่ปุ่น และ เกาหลีใต้   ทั้งหมด เป็น เอกภาพ กัน ใน เรื่อง นี้   โดย มี เป้าหมาย ร่วมกัน ใน การ หยุด โครงการ นิวเคลียร์ ใน เกาหลีเหนือ   ดังนั้น เรา มี กระบวนการ ใน การยุติ โครงการ นิวเคลียร์ ใน คาบสมุทร เกาหลี   เรา ได้ พูด กับ เกาหลีเหนือ ชัดเจน แล้ว   ถ้าหาก เกาหลีเหนือ ตัดสินใจ ที่จะ ยกเลิก โครงการ   สหรัฐฯ และ ประเทศ หุ้นส่วน อื่น   ๆ   จะ ปรับ ความสัมพันธ์ เข้าสู่ ระดับ ปกติ   และ จะ ให้ ความช่วยเหลือ ที่จะ ทำให้ ชาว เกาหลีเหนือ มีชีวิต ที่ ดีกว่า   \n","       \n","   “ มัน เป็น xxunk ที่ เห็น ที่ ประชาชน ใน เกาหลีเหนือ   ดู สิ่ง ที่เกิด ขึ้นกับ ประชาชน ใน เกาหลีเหนือ   คุณ ก็ รู้ดี อยู่ ว่า เขา ไม่มี อะไร พอ กิน เลย   ไม่ มีโอกาส ใน ชีวิต ซึ่ง เขา ควรจะ ได้รับ   เรา มี จุดยืน ที่ ชัดเจน มาก   ว่า เรา ต้องการ ที่จะ พูดถึง อนาคต ของ เกาหลีเหนือ   แต่ ภายใต้ เงื่อนไข ว่า เกาหลีเหนือ ต้อง เลิก โครงการ นิวเคลียร์ ”   \n","   นาง ฮิลลารี   กล่าว ด้วยว่า   นโยบาย ของ สหรัฐ และ อีก   5   พันธมิตร นั้น   ต้องการ ที่จะ สร้าง ความมั่นคง ทาง ภูมิภาค และ คิด ว่า ได้ ดำเนิน นโยบาย นี้ อย่าง เป็น เอกภาพ   ซึ่ง หวัง ว่า เกาหลีเหนือ คงจะ ตอบสนอง ด้วยดี   \n","   ผู้สื่อข่าว ถาม ต่อ มาถึง รูปธรรม ที่ มี การกล่าวอ้าง ว่า เป็น อย่างไร และ ได้ ตกลง เรื่อง นี้ กับ อีก   4   ชาติ   คือ เกาหลีใต้   จีน   รัสเซีย   ญี่ปุ่น แล้ว หรือยัง   บอก ได้ ไหม ว่า แผนการ นี้ รวมถึง การ ที่ เกาหลีเหนือ จะ ส่งมอบ xxunk โต เนียม ด้วย หรือไม่   นาง ฮิลลารี   ตอบ ว่า   เป้าหมาย คือ การ ให้ เลิก โครงการ นิวเคลียร์ อย่าง เบ็ดเสร็จ เด็ดขาด ไม่ ถอยหลัง กลับ ไปหา มัน อีก   ซึ่ง เกาหลีเหนือ ผูกพัน เรื่อง นี้ ตั้งแต่ ปี   2006   แล้ว   แต่ ก็ ไม่ได้ ทำตาม   ผลสุดท้าย เกาหลีเหนือ ต้อง ผูกพัน สิ่ง ที่จะ ต้อง ทำ และ ยกเลิก ความสามารถ ทั้งหมด หรือ อะไรก็ตาม ที่ เกี่ยวกับ นิวเคลียร์   เป็นเรื่อง ที่ ยาก และ ท้าทาย อยู่ ใน ขณะนี้   แต่   5   ประเทศ เรา ไม่ เพียง ผูกพัน กับ เป้าหมาย แต่ จะ ต้อง ทำ เพื่อให้ บรรลุเป้าหมาย นั้น   และ ข้อ มติ แห่ง คณะมนตรีความมั่นคง แห่ง สหประชาชาติ   ก็ ได้รับ การ สนับสนุน อย่าง เป็นเอกฉันท์ จาก พวกเรา และ ได้รับ การบังคับใช้ จาก พวกเรา   \n","   “ ดิฉัน คิด ว่า นี่ เป็น จุดยืน ที่ เข้มแข็ง ที่สุด ต่อ เกาหลีเหนือ และ ตอนนี้ รอคอย ว่า เกาหลีเหนือ จะ ตอบสนอง อย่างไร   เรา คาด ว่า เกาหลีเหนือ จะ ตอบสนอง อย่าง เป็น บวก ”   \n","   เรียกร้อง พม่า ปล่อย   “ อองซาน ซูจี ”   \n","   ต่อ ข้อซักถาม กรณี ของ ประเทศ พม่า   คิด ว่า ควรจะ ออกจาก อาเซียน หรือไม่   รมว. ต่างประเทศ สหรัฐฯ   กล่าวว่า   อาเซียน และ ประเทศ ใน อาเซียน กำลัง เดิน ไป ใน แนวทาง ที่ เป็น บวก มาก ๆ   การ เพิ่ม การ เน้นย้ำ เรื่อง สิทธิมนุษยชน ใน กลุ่ม อาเซียน ถือว่า เป็น สิ่ง ที่ น่ายินดี อย่างยิ่ง และ ได้รับ การตอบรับ ที่ ดี อย่างยิ่ง   แต่ว่า พม่า ไป ในทางตรงกันข้าม กับ ประเทศ อาเซียน อื่นๆ   ทั้งนี้ ว่า สหรัฐอเมริกา ต้องการ ที่จะ เห็น พฤติกรรม ที่ เปลี่ยนแปลง ของ รัฐบาล พม่า และ คิด ว่า ประเทศ อื่น ใน ภูมิภาค นี้ ก็ ต้องการ เห็น เช่นกัน   เมื่อวาน นี้   ( 21   ก.ค. 52 )   ตน ได้ พูดถึง ความร่วมมือ ทาง ระหว่าง พม่า กับ เกาหลีเหนือ เรื่อง อาวุธ ร้ายแรง   และ อาจ รวมถึง อาวุธ นิวเคลียร์   ซึ่ง นำ ความกังวล มาสู่ ภูมิภาค นี้   \n","       \n","   รัฐมนตรี ต่างประเทศ สหรัฐ   กล่าว ต่อมา ว่า   เป็น เรื่องสำคัญ มาก ที่จะ สนับสนุน ให้ รัฐบาล พม่า เริ่มต้น เปิดกว้าง เพื่อ เดินตาม ทิศทาง รูปแบบ การปกครอง ที่ ประเทศ อาเซียน อื่นๆ   เขา เป็น กัน   และ เรียกร้อง ให้ มี การปล่อยตัว   นาง อองซาน ซูจี   ผู้นำ ฝ่ายค้าน ของ พม่า   ซึ่ง หาก เธอ ได้รับ การปล่อยตัว ก็ จะ เป็นการ เปิดโอกาส ให้ สหรัฐฯ   ขยาย ความสัมพันธ์ กับ พม่า   รวมทั้ง การลงทุน ใน พม่า   แต่ ก็ ขึ้นอยู่กับ ผู้นำ พม่า เอง   \n","   “ เรา ก็ได้ เพียง หวัง ว่า ใน การ เจรจา พูดคุย ระหว่างประเทศ อื่นๆ   และ ผู้แทน ของ พม่า ใน ที่ประชุม นี้ อาจจะ มี ความก้าวหน้า บ้าง ที่จะ ทำให้ ผู้นำ พม่า เปลี่ยนแปลง ทิศทาง   นี่ มัน เป็นเรื่อง ความสนใจ และ ความ ห่วง กังวล ที่ สำคัญ มาก ของ สหรัฐ   แต่ เรา อยาก จะ เห็น ว่า มี ประจักษ์พยาน อะไร บาง อย่างว่า พม่า ได้ แสดง ความเปลี่ยนแปลง   ทั้งนี้   ด้วย ความเคารพ ต่อ อาเซียน   ก็ ขึ้นอยู่กับ ว่า อาเซียน จะ ทำ อย่างไร ต่อ พม่า   เรา ควร ใส่ใจ ให้ความสำคัญ กับ การ ลอง พยายาม ที่จะ ชักชวน พม่า   เพื่อที่จะ ไปให้พ้น จาก การ ถูก โดดเดี่ยว   และ ปฏิบัติ ต่อ ประชาชน ของ ตัวเอง ให้ ดีขึ้น   ให้ ประชาชน มีโอกาส เลือกตั้ง อย่างแท้จริง   ซึ่ง จะ เป็นประโยชน์ ต่อ ประชาชน พม่า ในอนาคต ”   \n","   \n","   \n","   ฮิลลารี เปิดโอกาส นัก สิทธิมนุษยชน สตรี ของ ไทย เข้าพบ   \n","   ก่อนหน้านี้   ช่วง เช้า วันเดียวกัน   ( 22   ก. ค)   นาง ฮิลลารี ได้ เปิดโอกาส ให้ นัก สิทธิมนุษยชน สตรี   ที่ทำงาน ใน ไทย   4   คน   เข้าพบ และ พูดคุย สั้น ๆ   อย่าง ไม่เป็นทางการ   ที่ พระราชวัง พญา ไท   คือ   นาง เตือนใจ   ดี เทศน์   อดีต   ส.ว.   เชียงราย   นาง อังคณา   นี ละ ไพจิตร   ภรรยา ของ นาย สม ชาย   นี ละ ไพจิตร   อดีต ประธาน ชมรม นักกฎหมาย มุสลิม   น.ส. สุ ภิญญา   กลาง ณรงค์   รองประธาน คณะกรรมการ รณรงค์ เพื่อ การปฏิรูป สื่อ   และ ผู้ประสานงาน เครือข่าย พลเมือง เน็ต   และ เจ้าหน้าที่ หญิง สหประชาชาติ   ประจำ ประเทศไทย   \n","   อย่างไรก็ตาม   รัฐมนตรี ต่างประเทศ สหรัฐ ได้ เชิญ   พญ. ซิ xxunk   หม่อง   แพทย์ ผู้ลี้ภัย ชาว พม่า   และ เจ้า ของรางวัล แมก xxwrep 1 ไซ   โดย   พญ. ซิ xxunk ได้ เดินทาง มาถึง ภายใน งาน แล้ว   แต่ เมื่อ ทราบ ว่า   ไม่ได้ เป็นการ พบปะ   แบบ ส่วนตัว   แต่ มี การ เผยแพร่ ผ่าน สื่อมวลชน   จึง ปฏิเสธ ที่จะ เข้าพบ   \n","       \n","  ,xxbos   เสวนา วิพากษ์ หนังสือ   \" แผนที่ ประวัติศาสตร์   ( สยาม )   ประเทศไทย \"   อัน มี   \" สุ จิตต์   วงษ์ เทศ \"   เป็น   บรรณาธิการ   เมื่อ ประวัติศาสตร์ ประเทศไทย   ถูก อธิบาย ใหม่ ผ่าน   \" แผนที่ \"   แบบ รื้อ   \" มุมมอง ความ เป็น ไทย เก่า \"   ที่ เคย การ ถ่ายทอด ความทรงจำ กัน มา ตั้งแต่ มี   \" แผนที่ \"   ของ   \" ทองใบ   แตง น้อย \"   ซึ่ง แทบ ไม่เคย ถูก เปลี่ยนแปลง แก้ไข มา ก่อน ตั้งแต่ สมัย สมเด็จกรมพระยาดำรงราชานุภาพ ปู ทาง สร้าง   \" ประวัติศาสตร์ ไทย \"   เป็นต้นมา   \n","   \n","       \n","   วัน ศุกร์ ที่   14   ก.ย.   งาน เสวนา   \" คนไทย เป็น ใคร   มาจาก ไหน   ใน แผนที่ ประวัติศาสตร์   ( สยาม )   ประเทศไทย \"   ถูก จัด ขึ้น โดย สำนัก คณะกรรมการ วัฒนธรรม แห่งชาติ   xxunk   ร่วมกับ   คณะ โบราณคดี   และ สถาบัน วิจัยและพัฒนา   มหาวิทยาลัยศิลปากร   เพื่อ สร้าง การ แลกเปลี่ยน เรียนรู้ ความเข้าใจ ใน เรื่อง ของ ประวัติศาสตร์   สังคม และ วัฒนธรรม ของ คนไทย ตั้งแต่ สมัย xxunk   และ คล้าย กับ เป็นการ เปิด ตัวหนังสือ   ผู้ก่อตั้ง นิตยสาร ศิลปวัฒนธรรม   เป็น บรรณาธิการ   ที่จะ นำ ไป ให้ สำนักงาน   หน่วยงาน หรือ องค์กร ที่ เกี่ยวข้อง นำไปใช้ ใน การ เผยแพร่ เป็น ความรู้ สาธารณะ ต่อไป   \n","       \n","   หนังสือ เล่ม นี้ เป็นการ มอง ประวัติศาสตร์ ประเทศไทย โดย อธิบาย ผ่าน   \" แผนที่ \"   แบบ รื้อ   \" มุมมอง เก่า \"   ที่ เคย ถ่ายทอด ความทรงจำ   \" ความ เป็น ไทย \"   ชุด ดั้งเดิม ผ่าน   \" แผนที่ \"   ของ นาย ทองใบ   แตง น้อย   ที่ แทบ ไม่เคย มี การเปลี่ยนแปลง แก้ไข มา ก่อน   หลังจากที่ สมเด็จกรมพระยาดำรงราชานุภาพ เริ่มต้น ปู ทาง สร้าง   \" ประวัติศาสตร์ ไทย \"   เป็นต้นมา   \n","       \n","   \n","       \n","   แม้ว่า   สุ จิตต์   วงษ์ เทศ   จะ เคย กล่าว เอาไว้ ว่า   \" อย่า เพิ่ง เชื่อ \"   ทุกอย่าง ที่ ปรากฏ ใน หนังสือ เล่ม นี้   แต่ คง ปฏิเสธ ไม่ได้ ว่า หนังสือ เล่ม นี้ ได้ สร้าง เป็น กรอบ   โครงสร้าง หรือ เป็น อีก หลัก ไมล์ หนึ่ง ทาง การศึกษา ประวัติศาสตร์   ( สยาม )   ประเทศไทย   ที่ ท้าทาย ให้ วงการ วิชาการ ต้อง หัน กลับมา ศึกษา เรื่อง นี้ กัน ใหม่ อย่างละเอียด มาก ๆ   อีกครั้ง   \n","       \n","   ขอ เกริ่น เบื้องต้น ก่อน เข้าสู่ เนื้อหา ใน การ อภิปราย ว่า   ภาพรวม ของ   หนังสือ   \" แผนที่ ประวัติศาสตร์   ( สยาม )   ประเทศไทย \"   แบ่ง เนื้อหา ออก เป็น สาม ส่วนใหญ่ ๆ   คือ   \n","       \n","   ส่วน แรก   มอง ภาพรวม ของ สุวรรณภูมิ โดยเฉพาะ ประเทศไทย   แยก ออก เป็น   11   บท   ใน แต่ละ บท จะ ประกอบ ไป ด้วย เรื่องราว ตั้งแต่ สมัย   5,000   ปีก่อน   เรื่อย มา จนถึง   พ.ศ. xxunk   ซึ่ง เป็น ยุคปัจจุบัน   เนื้อ ความใน แต่ บท จะ เล่า เรื่องราว และ พัฒนา ของ คนไทย ตั้งแต่ เริ่ม มี ชุมชน   จน ไป ถึง การ เป็น รัฐ ประชาชาติ ประชาธิปไตย   จนได้ ชื่อว่า เป็น ประเทศไทย   \n","       \n","   ส่วน ที่สอง   บอกเล่า เรื่องราว ของ   4   ภาค ของ ไทย   คือ   ภาคเหนือ   ( xxunk - ล้าน นา )   ภาค กลาง   ( xxunk แม่น้ำ เจ้าพระยา )   ภาคใต้   ( คาบสมุทร xxunk )   และ ภาค ตะวันออกเฉียงเหนือ   ( ลุ่ม แม่น้ำ โขง - ชี - มูล )   จะ เห็น พัฒนาการ ของ แต่ละ ภาค ใน เรื่อง ของ สังคม   และ วัฒนธรรม   และ ให้ความสำคัญ กับ บริเวณ ที่ เป็น ศูนย์กลาง ของ แต่ละ ภาค   ทำให้ เห็น พัฒนาการ ที่ ไม่ เท่าเทียมกัน ของ แต่ ภาค อย่างชัดเจน   \n","       \n","   ส่วน ที่สาม   เป็นการ เปรียบเทียบ วัฒนธรรม ต่างๆ   ใน โลก รวมถึง เหตุการณ์ ที่ เกิดขึ้น ใน ยุคสมัย ต่างๆ   ดังนั้น   หาก มอง ใน ภาพรวม ของ หนังสือ เล่ม นี้ แล้ว   นอกจาก จะ เดิน ไป บน เส้นทาง ของ การ ทำความรู้จัก ตัวเอง มากยิ่งขึ้น แล้ว ยัง รับรู้ ไป ถึง เรื่องราว และ วัฒนธรรม ของ โลก ที่ เกิดขึ้น ใน ช่วงเวลา นั้นๆ   อีกด้วย   \n","       \n","   งาน เสวนา ใน ครั้งนี้ ยัง มีส่วน ของ การวิพากษ์ ทั้ง ตัวหนังสือ เอง และ อภิปราย ต่อไป ถึง คำถาม สุด คลาสสิก เรื่อง   \" คนไทย มาจาก ไหน \"   คนไทย มาจาก เทือกเขา อัน ไต หรือว่า อยู่ ที่นี่ มา นาน แล้ว   ซึ่ง มี วิทยากร อย่าง   ดร. ชาญ วิ ทย์   เกษตร ศิ ริ   อดีต อธิการบดี มหาวิทยาลัยธรรมศาสตร์   รศ. สุ รพ ล   นา ถะ xxunk   คณบดี คณะ โบราณคดี   มหาวิทยาลัยศิลปากร   อ. สถาพร   ศรี สัจ จัง   ( พนม   นันท พฤกษ์ )   ศิลปิน แห่งชาติ   และ ผู้อำนวยการ สถาบัน ทักษิณ คดี ศึกษา   มหาวิทยาลัย ทักษิณ   สงขลา   รศ. สาย ชล   สัต ยา นุ รักษ์   ภาค วิชาประวัติศาสตร์   คณะ มนุษยศาสตร์   มหาวิทยาลัยเชียงใหม่   และ   รศ.ดร. ดารา รัตน์   เมตตา ริ กา นนท์   สาขา ประวัติศาสตร์ และ โบราณคดี   คณะ มนุษยศาสตร์ และ สังคมศาสตร์   มหาวิทยาลัยขอนแก่น   มา ร่วม แลกเปลี่ยน อภิปราย   \n","       \n","   \n","   บรรยากาศ ใน งาน   \n","       \n","       \n","   ดร. ชาญ วิ ทย์   เกษตร ศิ ริ   เริ่มต้น กล่าว เกี่ยวกับ เรื่อง คนไทย เป็น ใคร   มาจาก ไหน   ว่า   \n","   ถ้า มี ใคร สัก คน ถาม คำถาม นี้   ซึ่ง คงจะ มีสติ ไม่ ค่อย ดี ที่ ถาม   คน ส่วนใหญ่ ก็ จะ ตอบ ว่า   \" ไม่รู้ สิ   เป็น ใคร มาจาก ไหน \"   แต่ ถ้า ถาม นักเรียน   นิสิต   นักศึกษา   ซึ่ง มี คะแนน กำกับ อยู่   จะ ตอบ ว่า   \n","       \n","   \" คนไทย   คือ   คน ที่ พูด ภาษาไทย   นับถือ ศาสนาพุทธ   มี เชื้อชาติ ไทย   และ รัก ชาติ   ศาสนา   พระมหากษัตริย์   และ อา จะ แถม ด้วย การ บอ กว่า เป็น คนดี มี คุณธรรม   พร้อม ที่จะ ตอบ xxunk แผ่นดิน \"   \n","       \n","   แล้ว ถ้า มี การ xxunk อีก ก็ จะ ตอบ ว่า   \n","       \n","   \" มาจาก ภูเขา อัน ไต   และ อาณาจักร น่าน เจ้า   แล้ว สุโขทัย   อยุธยา   รัตนโกสินทร์ \"   \n","       \n","   ดร. ชาญ วิ ทย์   จึง ตั้ง ข้อ สังเกตว่า   ตกลง คำตอบ มี ตายตัว อยู่ แล้ว   บางคน อาจ คิด ว่า คำถาม และ คำตอบ แบบนี้ เชย ไป แล้ว ก็ได้ จึง น่าจะ เลิก ถาม และ เลิก ตอบ   แต่ว่า ทั้ง คำถาม และ คำตอบ นี้ ได้ ปรากฏ อยู่ ใน หนังสือ ของ   \" ทองใบ   แตง น้อย \"   ที่ ได้ กล่าว ไว้ ว่า   คนไทย มี พัฒนา และ เริ่ม ขึ้น จาก ภูเขา อัน ไต   ใน มองโกเลีย   เหนือ แม่น้ำ xxunk   และ เหนือ กำแพงเมืองจีน   \n","       \n","   ตั้งแต่ เมื่อ   5,000   ปี มา แล้ว   คนไทย ได้ อพยพ ลงมา ตั้งถิ่นฐาน อยู่ ระหว่าง แม่น้ำ xxunk และ แม่น้ำ แยง ซี   จน ถูก คนจีน รุกราน จึง อพยพ มา ตั้ง อาณาจักร น่าน เจ้า   อยู่ ได้ ไม่นาน ก็ ถูก จีน และ มองโกเลีย รุกราน   ก็ เลย ต้อง มา ตั้ง อาณาจักร   สุโขทัย   อยุธยา   รัตนโกสินทร์   \n","       \n","   \" อันนี้ คือ สิ่ง ซึ่ง สร้าง ขึ้น มา โดย   ทองใบ   แตง น้อย   ซึ่ง สิ่ง เหล่านี้ เปรียบเสมือน โปรแกรม สำเร็จรูป ที่ ฝัง ลง ไป ใน หัว ของ เรา \"   ดร. ชาญ วิ ทย์   กล่าว และ อธิบาย ต่อไป ว่า   \n","       \n","   ต่อมา คนไทย ก็ สร้าง อาณาจักร สุโขทัย จน เป็น อาณาจักร ที่ กว้างใหญ่ กิน พื้นที่ ไป จนถึง มะละกา และ สิงคโปร์   เรื่อง นี้ เป็น โปรแกรม ที่ ใส่ เข้าไป ใน หัว ของ เยาวชน ด้วย เพราะ   \" แผนที่ \"   นี้ ได้ บรรจุ ลง ใน แบบเรียน   \n","       \n","   จาก อาณาจักร สุโขทัย ก็ เรื่อย ลง มาจาก ถึง อาณาจักร อยุธยา   ใน สมัย ของ สมเด็จ พระ นเรศวร มหาราช ประเทศไทย ใหญ่ ยิ่ง   มา จนถึง สมัย ธนบุรี และ รัตนโกสินทร์ ที่ ยิ่งใหญ่   แต่ พอ มา ใน สมัย รัชกาล ที่   5   ก็ เสียดินแดน   ใน หนังสือ ของ ทองใบ   แตง น้อย   บรรจุ ไว้ เลย ว่า ไทย เสีย   12   จุ ไทย   \n","       \n","   ถ้า ถาม ว่า   ทองใบ   แตง น้อย   ไป เอา ข้อมูล เหล่านี้ มาจาก ไหน   คาด ว่า คง เอา มาจาก ขุน วิจิตร มาตรา   และ หลวง วิจิตร วาท การ   ทั้งนี้   ขุน วิจิตร มาตรา   เป็น ผู้ ที่ ชนะเลิศ ใน การเขียน เรื่อง หลัก ไทย   พ.ศ. xxunk   ซึ่ง เขียน บอก ไว้ ว่า   \" คนไทย มาจาก เทือกเขา อัน ไต \"   \n","       \n","   ต่อมา   เมื่อ มี การ คิด ต่อไป ว่า ขุน วิจิตร มาตรา นำ เรื่องราว เหล่านี้ มี จาก ไหน   คำตอบ ที่ ได้   คือ   \" ได้ มาจาก หลวง วิจิตร วาท การ \"   ซึ่ง เป็น คนเขียน ประวัติศาสตร์ สากล   5   เล่ม   เป็น ทางเดิน ของ ความรู้ และ เรื่องราว ของ คนไทย มาจาก ไหน   ถ้า ไล่ ข้อมูล ต่อไป อีก จะ พบ ว่า หลวง วิจิตร วาท การนำ ข้อมูล เหล่านี้ มาก จาก สมเด็จกรมพระยาดำรงราชานุภาพ ที่ เขียน ไว้ ใน พระ นิพนธ์ คำนำ ของ พระราชพงศาวดาร ฉบับ xxunk   ซึ่ง คน ส่วนใหญ่ จะ ไม่ ค่อย ได้ อ่าน เพราะ ข้าม ไป อ่าน เรื่อง ใน สมัย ของ สมเด็จ พระ นเรศวร   \n","       \n","   อย่างไรก็ตาม   ดร. ชาญ วิ ทย์   ระบุ ว่า   ส่วน ของ คำนำ ของ พระราชพงศาวดาร ฯ   เป็น ส่วน ที่ สำคัญ มาก   เขียน ไว้ ใน ปี   พ.ศ. 2457   สมเด็จกรมพระยาดำรงราชานุภาพ กล่าวว่า   \" คนไทย ใน เมือง จีน นั้น คือ เมือง ไทย เดิม   กล่าว โดยสรุป เมื่อ สมัย อยุธยา   หรือ สมัย ต้น รัตนโกสินทร์   เรา ไม่ รู้จัก เทือกเขา อัน ไต   ไม่ รู้จัก อาณาจักร น่าน เจ้า   จนกระทั่ง เมื่อ xxunk ได้ แผ่อิทธิพล เข้ามา   xxunk อังกฤษ และ ฝรั่งเศส ที่ เข้ามา พร้อม ความรู้ ทางวิชาการ แบบ ใหม่ ๆ   ที่ ว่าด้วย ภาษาศาสตร์   มานุษยวิทยา   โบราณคดี   ฯลฯ   นักวิชาการ เหล่านี้ ก็ได้ จัดแบ่ง กลุ่มคน แล้ว ได้ มี เขียน เรื่อง น่าน เจ้า   เทือกเขา อัน ไต \"   \n","       \n","   คนไทย ใน ส่วน ต่างๆ   รวมทั้ง สมเด็จกรมพระยาดำรงราชานุภาพ   เป็น คนไทย รุ่น แรก ที่ มีโอกาส ได้ เรียนหนังสือ กับ พวก ฝรั่ง และ ได้รับ ข้อมูล เหล่านี้   จน ในที่สุด ได้มา ปรากฏ อยู่ ใน ประวัติศาสตร์ พงศาวดาร สยาม นับ ตั้งแต่นั้นมา   ความ รู้เรื่อง คนไทย มาจาก ภูเขา อัน ไต   น่าน เจ้า   อยุธยา   สุโขทัย   และ รัตนโกสินทร์ ใช้เวลา เดินทาง มา ร่วม ร้อย ปี   ความรู้ แบบนี้ ถูก สร้าง ขึ้น และ ฝัง ลึก อยู่ ใน สมอง ของ เรา   ยาก แก่ การ เอา ออก และ ลบเลือน   \n","       \n","   นอกจากนี้   ดร. ชาญ วิ ทย์   ยัง ได้ กล่าวถึง ข้อแตกต่าง ระหว่าง ประวัติศาสตร์ ของ ทองใบ   แตง น้อย   และ   สุ จิตต์   วงษ์ เทศ   ไว้ อย่างชัดเจน อี กว่า   \n","       \n","   ใน หนังสือ เล่ม นี้   มี   11   บท   ใน บท แรก สุ จิตต์   ขึ้น   5,000   ปี แรก   มี ชุมชน คน ดึกดำบรรพ์ สุวรรณภูมิ   บรรพบุรุษ คนไทย และ คน อุษาคเนย์   ซึ่ง สุ จิตต์ ใช้ วิธี เหมา รวม   แล้วก็ นำ เอา หลักฐานทางโบราณคดี มา   เช่น หลักฐาน ว่าด้วย บ้านเก่า บ้าง   บ้าน เชียง บ้าง   บอ กว่า นี่ มี คน อาศัย อยู่   แต่ เรา ไม่รู้ ว่า เขา เป็น คนไทย หรือเปล่า   เพราะ ยัง ไม่มี ภาษา ที่ สามารถ จารึก ได้   \n","       \n","   พอ บท ที่สอง   4,000   ปี   ถอย มา เรื่อยๆ   มนุษย์ ใน แถบ นี้ รู้จัก ถลุง โลหะ   ทำ เครื่องมือ   เครื่องใช้ แทน เครื่องมือ หิน ที่ เคย ใช้ แต่เดิม   มี ศาสนา   คือ   นับถือ   ผี   นับถือ ดิน ฟ้า   \n","       \n","   4,000   ปี   เริ่ม มี ชุมชน ถลุง เหล็ก   เติบโต เป็น บ้านเมือง   บน เส้นทาง การคมนาคม ทางบก และ ทางทะเล   มี แผนที่ ใน การ ประกอบ การอธิบาย เหมือนกับ   ทองใบ   แตง น้อย   แล้วก็ ขยับ มา ประมาณ หลัง   พ.ศ. 1   เป็น สุวรรณภูมิ มี การ เชื่อมโยง ระหว่าง ตะวันออก กับ ตะวันตก   หมาย ความ ว่า จาก ด้าน ทะเล จีน   กับ มหาสมุทร อินเดีย   แล้วก็ มี แผนที่ และ ชุมชน ต่าง   ๆ   \n","       \n","   เมื่อ   พ.ศ. 500   มี รัฐ เล็ก ๆ   รับ พระพุทธศาสนา   มี พราหมณ์   มี ผี   เป็น บรรพบุรุษ ของ คน ที่นี่   แล้วก็ มี แผนที่ อีก   \n","       \n","   หลัง   พ.ศ. 1000   รัฐ ใหญ่ ใน สุวรรณภูมิ   เริ่ม เข้า เรื่องราว ของ ความ เป็น ไทย   เป็น ต้นทาง ของ ประวัติศาสตร์ สยาม   ประเทศไทย   \n","       \n","   ดู ไป จนถึง บท ที่   7   หลัง   พ.ศ. 1500   รัฐ พื้นเมือง   เริ่ม ควบคุม การค้า ด้วยตัวเอง   มี ภาษา ซึ่ง เป็น ภาษา การค้า   คือ   ชวา   มลายู และ   ไทย   ลาว   ซึ่ง เป็น ภาษา ที่ ใหญ่ มาก   จน ทำให้ คนอื่น หันมา ใช้ ภาษา นี้ กัน เยอะ มาก   ทำให้ มี รัฐ ใหญ่ เกิดขึ้น   \n","       \n","   หลัง   พ.ศ. xxunk   มี ความเปลี่ยนแปลง โดย มี การ เกิดขึ้น ของ ศาสนา มวลชน   ซึ่ง ศาสนา มวลชน นี้ แปล ว่า อิสลาม   พุทธศาสนา แบบ เถรวาท   ในขณะที่ ศาสนาพราหมณ์ - ฮินดู   นั้น เป็น ศาสนา ของ ชนชั้น ปกครอง   ซึ่ง จุด นี้ เป็น จุดเริ่มต้น ของ การ แพร่ ของ ศาสนา มวลชน   แล้ว ในขณะเดียวกัน ก็ จะ มี อักษร ไทย และ คนไทย   \n","       \n","   หลัง   พ.ศ. 2000   มี กรุงศรีอยุธยา   ศูนย์กลาง การค้า นานาชาติ   เรียก ประเทศ ว่า เมือง ไทย   เรียก ตัวเอง ว่า   คนไทย   แล้ วสุ จิตต์ ก็ ไล่ มา หลัง   พ.ศ. 2300   เป็น xxunk   กรุงรัตนโกสินทร์   ราชอาณาจักร สยาม   จนกระทั่ง หลัง พ.ศ. xxunk   เกิด รัฐ ประชาชาติ ประชาธิปไตย   ยกเลิก ชื่อ สยาม   ใช้ ชื่อ ประเทศไทย   นี่ คือ เส้นทาง ของ   \" สุ จิตต์   วงษ์ เทศ \"   เมื่อ เทียบ กับ ของ   \" ทองใบ   แตง น้อย \"   \n","       \n","   จากนั้น   ดร. ชาญ วิ ทย์   วิเคราะห์ เพื่อ สรุป ว่า   สุ จิตต์ ได้   \" ทำ อะไร   และ ไม่ได้ ทำ อะไร \"   \n","       \n","   ถ้า นำมา เทียบ กับ ประวัติศาสตร์ ของ ทองใบ   แตง น้อย   แนว การเขียน ของ ทองใบ จะ เป็น การเขียน ประวัติศาสตร์ ด้วย การ ใช้ แผนที่ เป็น กรอบ   คือ   แผนที่ ประเทศไทย ปัจจุบัน เป็น กรอบ   เพราะฉะนั้น อันนี้ เป็น ประวัติศาสตร์ ไทย ใน กรอบ แผนที่ ประเทศไทย   และ ทัศนคติ   อุดมการณ์ ใน การเมือง นั้น เป็น ประวัติศาสตร์ แบบ ราชา ชาตินิยม   และ เสนา อำ มา ต ยา ธิป ไต ย   เป็น ชาตินิยม ใน รูปแบบ ของ พระราชา กับ บรรดา เหล่า เสนา อำมาตย์ ทั้งหลาย   นี่ คือ   ประวัติศาสตร์ ใน แนว ของ ทองใบ   แตง น้อย   \n","       \n","   ส่วน ของ คุณ สุ จิตต์   สิ่ง ที่ พยายาม ทำ ก็ คือ   การเขียน ประวัติศาสตร์ โดย ใช้ แผนที่   ซึ่ง เป็น ประชา ชาตินิยม   เพราะฉะนั้น ในแง่ นี้ สุ จิตต์ ใช้ มากกว่า ทองใบ   แตง น้อย ใช้   คือ   การนำ โบราณคดี อัน เป็นการ ใช้ หลักฐาน บางอย่าง ที่อยู่ ใต้ดิน ซึ่ง มี การ ขุดค้น อยู่ บ่อยๆ   นับตั้งแต่ ทศวรรษ   1960   หรือ   ประมาณ   40 - 50   ปี ที่ผ่านมา   มา ใช้ ใน แผนที่ ประวัติศาสตร์   โดย บวก กับ หลักฐาน ด้าน มานุษยวิทยา   ซึ่ง จุด นี้   สุ จิตต์ มี ฐานข้อมูล ที่ ทันสมัย ทำให้ งาน มี ความหลากหลาย ที่ ทำให้ เห็น ความหลากหลาย ทาง ชาติ ด้าน ชาติ พันธ์ เด่นชัด มาก   และ ยัง นำ เอา ประวัติศาสตร์ มา เป็น กรอบ ใน การวางแผน ที่   \n","       \n","   อย่างไรก็ตาม แม้ จะ เห็นได้ชัด เจน ว่า งาน ของ สุ จิตต์   ใช้ ทั้ง มานุษยวิทยา และ ประวัติศาสตร์ ใส่ เข้าไป ใน แผนที่ ของ เขา ที่ ทำให้ เห็น มากกว่า แผนที่ ของ ทองใบ   แตง น้อย   แต่ ก็ ยัง เป็น เวอร์ชั่น ที่ ไม่ เป็น แบบ ประชา ชาตินิยม เสีย ทีเดียว   เพราะ ยัง มี xxunk ของ ราชา ชาตินิยม   และ อำ มา ต ยา ราชาธิปไตย อยู่   \n","       \n","   ดังนั้น หนังสือ   \" แผนที่ ประวัติศาสตร์   ( สยาม )   ประเทศไทย \"   เล่ม นี้ คง เป็น เพียง กรอบ หรือ โครง ที่ ยัง ไม่มี เนื้อหา ที่ ชัดเจน   ยัง ต้องการ การศึกษา ใหม่ อย่างละเอียด   ปัญหา ที่เกิด จาก การ อ่านหนังสือ เล่ม นี้   คือ   \n","       \n","   1. ต้อง ทำ การศึกษา ถึง ที่มา ของ คนไทย ใหม่   เพราะ แทบจะ ไม่มีใคร เชื่อ แล้ว ว่า คนไทย เดิน จาก เทือกเขา อัน ไต ซึ่ง อยู่ ใน กำแพงเมืองจีน   มา จนถึง ประเทศไทย ใน ปัจจุบัน ได้   \n","       \n","   2. จะ ต้อง ศึกษา แผนที่ ซึ่ง เป็น วิวัฒนาการ สมัยใหม่   ซึ่ง เกิดขึ้น มา กับ xxunk   มาจาก การ ค้นพบ วิทยาการ ใหม่ ๆ   ของ ชาวต่างชาติ ที่ ได้ เข้ามา ใน เมือง ไทย   ให้ เข้าใจ อย่างถ่องแท้   \n","       \n","   3. ใน การศึกษา แผนที่ ประวัติศาสตร์ จะ ต้อง ใช้ ทฤษฎี เป็นตัว ส่อง นำทาง   งาน สอง ชิ้น ที่จะ ช่วย เป็น ทฤษฎี ใน การ ส่อง ทาง นั้น   ก็ คือ   งาน ที่ เกี่ยวกับ รัฐ ชาติ และ ลัทธิชาตินิยม   ใน หนังสือ ที่ ชื่อว่า   xxunk   community   โดย   xxunk   anderson   และ หนังสือ   \n","   siam   map   ของ   อ. ธงชัย   วินิจ กู ล   เพื่อที่จะ เข้าใจ งาน ของ ทองใบ   แตง น้อย   และ งาน ของ สุ จิตต์   วงศ์ เทศ   ใน จุด ที่ว่า ทำไม งาน ของ ทองใบ   จึง มี การ กล่าวว่า ไทย มี การ เสียดินแดน   5 - 6   ครั้ง   แต่ งาน ของ สุ จิตต์ กลับ ไม่มี ตรงนี้   จึง ทำให้ ควร ต้อง มี การศึกษา กัน ใหม่ อย่างละเอียด อีกครั้ง   \n","       \n","   สำหรับ นักวิชาการ อีก ท่าน หนึ่ง ที่ ได้มา ให้ มุมมอง ใน เรื่อง นี้ จาก หลักฐานทางโบราณคดี ที่ ปรากฏ ใน ประเทศไทย   คือ   รศ. สุ รพ ล   นา ถะ xxunk   คณบดี คณะ โบราณคดี   มหาวิทยาลัยศิลปากร   ได้ กล่าวว่า   \"คน   ( ไทย )   อาจจะ อยู่ ที่นี่   เริ่มต้น ที่นี่ ไม่ได้ มาจาก เขา อัล ไต \"   เพราะ   พบ ส่วน กระ โห ลก ของ ออ ส ตรา โล xxunk   ที่ เกาะ คา   จ. ลำปาง   อายุ นับ แสน ปี   หรือ   พบ โฮ โม ซา เปีย น ที่ ถ้ำ ลอด   จ. แม่ฮ่องสอน อายุ   12,000   ปี   และ พบ อารยธรรม อื่นๆ   กระจาย ไป ทั่ว ใน หลาย พื้นที่   \n","       \n","   ช่วงเวลา ที่เกิด หมู่บ้าน เกษตรกรรม นั้น   โครงสร้าง ทางสังคม มี การ แยกแยะ ตาม แบบ ลักษณะ ของ ที่มา อย่างชัดเจน   นอกจากนี้ ยัง มี การปรากฏ ขึ้น ของ วัฒนธรรม ที่ เกิดขึ้น พร้อม หมู่บ้าน เกษตรกรรม ก็ คือ   การ ติดต่อ แลกเปลี่ยน ผลผลิต ระหว่าง กัน   ทำให้ พบ เครื่องประดับ ที่ ทำ จาก xxunk ทะเล ที่ เหมือน ๆ   กัน ใน หลาย พื้นที่   ทั้ง ใน ภาคเหนือ   ภาคอีสาน   หรือ ภาค กลาง   \n","       \n","   \" เห็นได้ชัด ว่า มี การ ติดต่อ   ปฏิสัมพันธ์ กัน ทั้ง โดยตรง   และ โดยอ้อม   ทำให้ เห็น ถึง การ แบ่งแยก สถานะ ทางสังคม อย่าง เด่นชัด   โดย ของ ที่ ได้ มาจาก ที่อื่น จะ เป็น ของ ที่ หา ยาก เป็น ตัวแปร กำหนดให้ เกิด ความแตกต่าง   เมื่อ ผู้ใด มี จะ ทำให้ ดู มี ฐานะ มากกว่า ผู้อื่น   ทำให้เกิด สังคม ที่ เป็นไป ใน ลักษณะ ซับซ้อน มากขึ้น เรื่อยๆ   สิ่ง หนึ่ง ที่ เป็น หลักฐานทางโบราณคดี ที่ สำคัญ ทำให้ เห็น ถึง การ ติดต่อ แลกเปลี่ยน ค้าขาย กัน   อันเป็น ปัจจัย ทำให้เกิด พัฒนาการ ทางสังคม   ก็ คือ   ได้ พบ ว่า ใน ช่วง ประมาณ   3,000 - 3,500   ปี มา แล้ว   ใน พื้นที่ บางแห่ง ของ ประเทศไทย   เกิด ชุมชน ที่ มี ความสามารถ ใน การทำงาน หัตถกรรม บางอย่าง   และ ผลิต สิ่งของ บางอย่าง ที่ บาง ชุมชน ต้องการ   เช่น   มี เหมือง ทองแดง xxunk ที่ ใหญ่ ที่สุด ใน เอเชียตะวันออกเฉียงใต้ ที่ พลู xxunk   และ ใน บริเวณ ใกล้ ๆ   เขา วง พระจันทร์   จ. ลพบุรี ได้ พบ แหล่ง ถลุง ทองแดง xxunk มากกว่า แสน ตัน   \n","       \n","   \" พื้นที่ ที่ เป็น ตลาด สำคัญ ของ ทองแดง จาก บริเวณ เขา วง พระจันทร์ นั้น   สันนิษฐาน ว่า จะ เป็น บริเวณ ภาคอีสาน ของ ไทย   และ บริเวณ ประเทศ เพื่อนบ้าน   เพราะ พบ ของใช้ ที่ ทำ จาก ทองแดง มากมาย   จะ เห็น ว่า การปฏิสัมพันธ์ กัน นั้น กิน พื้นที่ ไกล มาก   นำมาซึ่ง พัฒนาการ ของ สังคม ที่ มี สถานะ แตก ต่างกัน เริ่ม ชัดเจน มากขึ้น \"   \n","       \n","   รศ. สุ รพ ล   กล่าว ต่อไป ว่า   เมื่อ ศึกษา จาก ของใช้ ของ คน แต่ละ ภาค ของ ไทย ทำให้ เห็น ถึง วัฒนธรรม บางประการ ที่ แตก ต่างกัน   ใน ช่วง ระยะ เวลานี้ มี กลุ่มชาติพันธุ์ อยู่ หลาย กลุ่ม ด้วยกัน   อาจจะ พูด ภาษา ที่ แตก ต่างกัน   แต่ เมื่อ พื้นที่ ต่างๆ   มี ปฏิสัมพันธ์ กัน ก็ ทำให้เกิด พัฒนาการ ไป พร้อมๆ กัน   \n","       \n","   จน ใน ช่วง   2,500   ปี   มา แล้ว   พัฒนาการ ทางเทคโนโลยี ที่ สำคัญ   คือ มี การผลิต เหล็ก และ กระจก มากมาย   อีก ทั้ง เกิด วัฒนธรรม ที่ มี ที่ มาจาก แดน ไกล ใน ประเทศไทย   วัฒนธรรม ดังกล่าว   เช่น   อินเดีย   จีน ตอน ใต้   เวียดนาม ตอนเหนือ และ ตอนกลาง   ผล ที่ ตามมา ก็ คือ การ เกิด ชุมชน ขนาดใหญ่   มี การ แบ่ง พื้นที่ ภายใน ชุมชน อย่างชัดเจน   โดย ผู้มีอำนาจ สูงสุด ใน ชุมชน เป็น ผู้ แบ่ง การ ใช้ประโยชน์ พื้นที่ ใน ระยะเวลา นั้น   กระบวนการ ใน การ แลกเปลี่ยน ค้าขาย มี การพัฒนา ไป อีก ใน ระดับ หนึ่ง   \n","       \n","   ใน ช่วง   1,500 - 1,600   ปี   มา แล้ว ประชากร ใน ท้องถิ่น ของ ไทย บาง กลุ่ม ตัดสินใจ ที่จะ รับ ศาสนาพุทธ ที่ มาจาก อินเดีย มา เป็น ศาสนา ประจำ ชุมชน   ทำให้ เปลี่ยน กายภาพ ของ ชุมชน มา เป็น ชุมชน ขนาดใหญ่ มาก   มี การ ขุด คู ล้อมรอบ ชุมชน จน กลายเป็น เมือง   ประชากร เหล่านั้น ได้ กลาย มา เป็น ประชากร ยุค ประวัติศาสตร์ ของ ไทย   \n","       \n","   เมื่อ มอง จาก xxunk ที่ เห็น จาก ยุค ประวัติศาสตร์ ช่วงแรก ๆ   ใน มุมมอง ของ xxunk เห็น ว่า ใน บริเวณ ประเทศไทย ปัจจุบัน มี คน อาศัย อยู่ นาน แล้ว ตั้งแต่   ราว   5,000   ปี   คือ มีวัฒนธรรม ที่ เป็นต้นแบบ ของ วัฒนธรรม ปัจจุบัน เกิดขึ้น มา ก่อน แล้ว   และ กระบวน การเปลี่ยนแปลง ที่ เกิดขึ้น ราว   5,000   ปี มา แล้ว นั้น   มี การ เคลื่อนย้าย ของ กลุ่ม คนใน พื้นที่ ออก ไป   แล้ว มี การ ย้าย ข้าวของ กลุ่มคน จาก ต่าง พื้นที่ เข้ามา   กระบวนการ เคลื่อนย้าย แบบนี้ เกิด ขึ้นอยู่ ตลอดเวลา   ทำให้ มี การ ผสมผสาน ของ วัฒนธรรม เกิดขึ้น   \n","       \n","       \n","  ,xxbos   \n","   \n","   \n","       \n","   \n","   ศาล ใน กรุง เฮ ก   xxunk   มี คำพิพากษา แก่ นาย แวน   อัน รา ท   นักธุรกิจ xxunk ที่ ขาย วัตถุดิบ ใน การผลิต xxunk ให้ แก่   ซัดดัม   ฮุส เซ็น   อดีต ประธานาธิบดี อิรัก   ใน ระหว่าง ปี   ค.ศ. 1980 - 1988   ว่า มีความผิด ฐาน มีส่วนร่วม ใน การ ฆ่า ล้าง เผ่าพันธุ์ ชาว อิรัก เชื้อสาย เคิร์ด   เมื่อ ปี   1988   ที่ เมือง ฮา ลาบ จา   ซึ่ง อยู่ ทาง ตอนเหนือ ของ ประเทศ อิรัก   ผล จาก การ ใช้ xxunk ใน ครั้งนั้น มี ชาว เคิร์ด เสียชีวิต ราว   5,000   คน   \n","   \n","       \n","   \n","   อย่างไรก็ตาม   ก่อนที่ คณะ ผู้พิพากษา จะ ตัดสิน ลงโทษ นาย อัน รา ท นั้น   ต้อง มี คำตัดสิน ก่อน ว่า   การ โจมตี ชาว อิรัก เชื้อสาย เคิร์ด ที่ เมือง ฮา ลาบ จา เป็นการ ฆ่า ล้าง เผ่าพันธุ์ หรือไม่   เพราะ การ พิจารณาคดี ใน ครั้งนี้ ถือ เป็นคดี แรก เกี่ยวกับ อาชญากรรม สงคราม ที่ มี ต่อ ชาว เคิร์ด ใน อิรัก และ อิหร่าน   \n","   \n","       \n","   \n","   ทนาย จำเลย ได้ โต้แย้ง ว่า   กรณี ดังกล่าว นี้ ถือ เป็นเรื่อง ที่ ไม่ สามารถ ยอม รับได้   เนื่องจาก ผู้ต้องหา ตัว xxunk ใน การ ก่อ อาชญากรรม อย่าง นาย ซัดดัม ฮุสเซน   ยัง อยู่ ใน ระหว่าง การ พิจารณาคดี ที่ กรุง แบกแดด   \n","   \n","       \n","   \n","   นาย แวน   อัน รา ท   วัย   63   ปี ถูกจับ ฐาน จำหน่าย วัตถุดิบ ใน การผลิต xxunk หลาย xxunk เพื่อ ใช้ ใน สงคราม ต่อต้าน อิหร่าน และ ชาว อิรัก เชื้อสาย เคิร์ด ระหว่าง ปี   1980 - 1988   โดย อัยการ ได้ กล่าว ด้วยว่า   เขา ยังคง จัดหา วัตถุดิบ ดังกล่าว ให้ แก่ อิรัก   หลังจากที่ มี คำ สั่งห้าม ใน ปี   1984   ออกมา แล้ว   \n","   \n","       \n","   \n","   ทั้งนี้ นาย อัน รา ท ยอมรับ ว่า   เขา ส่ง สารเคมี ที่ ใช้ เป็น มัสตาร์ด ก๊าซ ใน การ โจมตี ที่ ฮา ลา xxunk   ใน xxunk ดิ ช   ทาง ตอนเหนือ ของ อิรัก   อย่างไรก็ตาม เขา ปฏิเสธ ที่จะ ยอมรับ ว่า   มีส่วน สมคบคิด ใน การ ก่อเหตุ ดังกล่าว   เขา ไม่รู้ ว่า วัตถุดิบ ดังกล่าว จะ ถูก นำไปใช้ เป็น xxunk เพื่อ โจมตี   เขา ยัง บอก อีก ด้วยว่า   เมื่อ เขา เห็นภาพ เหตุการณ์ การ โจมตี ด้วย xxunk นั้น เขา ยัง ตกใจ มาก   \n","   \n","       \n","   \n","   นาย แวน   อัน รา ท ถูก จับกุม ใน ปี   1989   ที่ เมือง มิ ลาน   ประเทศ อิตาลี   ตาม คำขอ ของ รัฐบาล สหรัฐอเมริกา   ภายหลัง   ได้รับ การปล่อยตัว และ หลบ ซ่อนตัว อยู่ ใน อิรัก   จนถึง ปี   2003   เมื่อ รัฐบาล สหรัฐฯ   บุก อิรัก ใน เดือน มีนาคม   2003   เขา ได้ เดินทาง กลับ xxunk   และ ถูกจับ อีกครั้ง ใน เดือน ธันวาคม   2004   ที่ บ้าน ของ เขา ใน อัม ส เต xxunk   \n","   \n","       \n","   \n","   . xxrep 58   \n","   \n","   ที่มา   :   xxurl   \n","  ,xxbos        \n","     ประชา ไท   - 6   ส.ค.   48           \" เรา กำลัง พัฒนา และ แก้ไข ไม่ใช่ ประจาน กันเอง     ผม ไม่ใช่ ว่า คณะ   กรรมการ สิทธิมนุษยชน   แต่ ผม ว่า สื่อ ด้วย ที่ ได้ ประจาน ประเทศ ไป ทั่วโลก     ผม ว่า มา ช่วยกัน คิด ช่วยกัน ทำ ดีกว่า     เพราะ นี่ คือ ประเทศ ของ เรา     ต้อง นึกถึง ประเทศ ของ เรา     เรา ต้อง เดิน ไป ด้วยกัน     ไม่ใช่ มา ล่ม เรือ เสีย ก่อน     ผม จะ ทำ ทุกอย่าง เพื่อให้ สิทธิมนุษยชน ยกระดับ มาก ที่สุด และ เร็ว ที่สุด     โดย มี กลไก ภาครัฐ และ ภาค ส่วน ต่างๆ   ร่วมมือ กัน   ผม ขอร้อง ว่า อย่า มา ประจาน ประเทศ ตัวเอง เลย ครับ \"    พ.ต.ท. ทักษิณ     ชิน วัตร     นายก   รัฐมนตรี     กล่าว   \n","        \n","     วันนี้ ที่     โรงแรม โซฟิเทล     กรุงเทพฯ   มี การประชุม เชิงปฏิบัติการ     \" สิทธิมนุษยชน สังคม ไทย ... มาตร   ฐาน สู่ การปฏิบัติ \"     โดย การ ริเริ่ม ของ กระทรวงยุติธรรม   ร่วมกับ กระทรวง การพัฒนา สังคม และ ความมั่นคง ของ มนุษย์     คณะกรรมการ สิทธิมนุษยชน แห่งชาติ และ องค์กร พัฒนา เอกชน ด้าน สิทธิ ฯ    โดย มี   พ.ต.ท. ทักษิณ   ชิน วัตร   นายกรัฐมนตรี     เป็น ประธาน การประชุม         \n","        \n","     พ.ต.ท. ทักษิณ กล่าวว่า     \"   ผม คิด ว่า ทุก สังคม ต้อง มี การ ตรวจสอบ และ ถ่วงดุล     ไม่ใช่ xxunk กัน   สังคม ต้องการ ดุลยภาพ     จะ มา สนอง ความรู้สึก ของ ตนเอง เพียง อย่าง เดียว คง ไม่ได้     ถ้า คณะ กรรม   การ สิทธิ ฯ   คิด ว่า รัฐ ต้อง ปรับปรุง ต้อง มา บอก กัน \"   \n","        \n","    \" สื่อมวลชน บาง ฉบับ ไม่ไหว จริงๆ   การ อยู่ ร่วมกัน ต้อง มี จริยธรรม ร่วมกัน   ขอ เถอะ นี่ คือ   ประเทศ ของ เรา   ผม ก็ อยู่ ได้ เท่าที่ ประชาชน ให้ อยู่     เรา ต่าง ก็ มา ทำหน้าที่     ผม เคารพ สื่อ     สื่อ ก็ ต้อง เคารพ ผม ใน การทำงาน ให้ ประเทศ \"    นายกรัฐมนตรี     กล่าว   \n","        \n","     พ.ต.ท.   ทักษิณ     เห็น ว่า   สังคม เปลี่ยนแปลง ไป     ทุกฝ่าย รวมถึง วัฒนธรรม การทำงาน ก็ จำเป็นต้อง เปลี่ยน ตาม     ซึ่ง ขณะนี้ เป็นช่วง เปลี่ยน ผ่าน ของ ยุคสมัย จาก ระบบ กล่าว xxunk ปรปักษ์     ด้วย ความเคยชิน ของ ผู้ บังคับใช้ กฎหมาย     \n","        \n","    \" ตอนนี้ รถไฟ แต่ละ ขบวน ยัง ไป ไม่ พร้อมกัน     มี หลาย ส่วน ที่ ต้อง พา กัน เดิน     ไม่ใช่ ต่าง ไป xxunk เสรีภาพ ให้ ตนเอง ขณะที่ ไป ละเมิด สิทธิ ของ คนอื่น     ซึ่ง ถ้า กติกา ไม่สมบูรณ์ ก็ ต้อง มี การ ปรับ \"   \n","        \n","     อย่างไรก็ตาม     นายกรัฐมนตรี     มองว่า     การปรับปรุง สิทธิมนุษยชน   ต้อง รวมถึง ประชาชน ทั้งหลาย แม้ ไม่มี สัญชาติ ไทย ก็ตาม     ก็ ต้อง เคารพ ศักดิ์ศรี ของ ความเป็นมนุษย์     ต้อง ดู สิทธิ ขั้นพื้นฐาน ใน การดำรง ความ เป็น คน     เช่น คนจน ไม่ มีโอกาส   รัฐ จึง ต้อง เข้าไป ให้โอกาส     ดังนั้น   กระบวน การแก้ปัญหา ความยากจน รวมถึง ปัจจัยสี่     จึง เป็น นโยบาย ที่ ต้อง ทำ ซึ่ง รัฐบาล ต้อง ทำให้ ได้ ใน   4   ปี นี้   \n","        \n","     พร้อมกัน นั้น     นายกรัฐมนตรี รับปาก ว่า     จะ มี การ ร่าง แผนแม่บท สิทธิมนุษยชน   โดย จะ ปรากฏ บน เว็บ ไซ ด์    เพื่อให้ ประชาชน แก้ไข แนะนำ ได้ สะดวก และ มี การ ตรวจสอบ ได เต ลอด เวลา   ซึ่ง ช่วย ให้ ตื่นตัว ตลอดเวลา และ พร้อม ที่จะ เปลี่ยนแปลง ตาม วัฒนธรรม ของ โลก     \n","        \n","   ยิ่งไปกว่านั้น     พ.ต.ท.   ทักษิณ     ได้ ย้ำ ว่า รัฐบาล จะ ตั้ง ศูนย์ พิสูจน์ บุคคล เพื่อ ตรวจสอบ การร้องเรียน ใน กรณี เกี่ยวกับ ตำรวจ อุ้ม ผู้ต้องหา     และ พร้อม ยินดี จัด งบประมาณ ให้ องค์กร พัฒนา เอกชน ( เอ็นจีโอ )    เนื่องจาก เห็น ว่า เป็น องค์กร ที่ทำงาน เพื่อ สังคม อีกด้วย     \n","   \n","       \n","   เรื่อง ที่ เกี่ยวข้อง   \n","   \" ฮิวแมน   ไรท์ ส   วอ ทช์\" เตือน รัฐ เลิก พ.ร.ก. ฉุกเฉิน   [ 1 ]   \n","   คำแปล ภาษาไทย   จดหมาย องค์การ ฮิวแมน ไรท์ วอ ทช์   ถึง   นายกรัฐมนตรี   พ.ต.ท.   ทักษิณ   ชิน วัตร   [ 2 ]   \n","  \n","y: LMLabelList\n",",,,,\n","Path: /content/drive/MyDrive/ject_final_petch/prachathai_data;\n","\n","Test: None, model=SequentialRNN(\n","  (0): AWD_LSTM(\n","    (encoder): Embedding(18944, 400, padding_idx=1)\n","    (encoder_dp): EmbeddingDropout(\n","      (emb): Embedding(18944, 400, padding_idx=1)\n","    )\n","    (rnns): ModuleList(\n","      (0): WeightDropout(\n","        (module): LSTM(400, 1550, batch_first=True)\n","      )\n","      (1): WeightDropout(\n","        (module): LSTM(1550, 1550, batch_first=True)\n","      )\n","      (2): WeightDropout(\n","        (module): LSTM(1550, 1550, batch_first=True)\n","      )\n","      (3): WeightDropout(\n","        (module): LSTM(1550, 400, batch_first=True)\n","      )\n","    )\n","    (input_dp): RNNDropout()\n","    (hidden_dps): ModuleList(\n","      (0): RNNDropout()\n","      (1): RNNDropout()\n","      (2): RNNDropout()\n","      (3): RNNDropout()\n","    )\n","  )\n","  (1): LinearDecoder(\n","    (decoder): Linear(in_features=400, out_features=18944, bias=True)\n","    (output_dp): RNNDropout()\n","  )\n","), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f4be19c9170>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/content/drive/MyDrive/ject_final_petch/prachathai_data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.train.GradientClipping'>, clip=0.12)], callbacks=[RNNTrainer\n","learn: ...\n","alpha: 2\n","beta: 1], layer_groups=[Sequential(\n","  (0): WeightDropout(\n","    (module): LSTM(400, 1550, batch_first=True)\n","  )\n","  (1): RNNDropout()\n","), Sequential(\n","  (0): WeightDropout(\n","    (module): LSTM(1550, 1550, batch_first=True)\n","  )\n","  (1): RNNDropout()\n","), Sequential(\n","  (0): WeightDropout(\n","    (module): LSTM(1550, 1550, batch_first=True)\n","  )\n","  (1): RNNDropout()\n","), Sequential(\n","  (0): WeightDropout(\n","    (module): LSTM(1550, 400, batch_first=True)\n","  )\n","  (1): RNNDropout()\n","), Sequential(\n","  (0): Embedding(18944, 400, padding_idx=1)\n","  (1): EmbeddingDropout(\n","    (emb): Embedding(18944, 400, padding_idx=1)\n","  )\n","  (2): LinearDecoder(\n","    (decoder): Linear(in_features=400, out_features=18944, bias=True)\n","    (output_dp): RNNDropout()\n","  )\n",")], add_time=True, silent=False)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"blDSQ0ehjPn0","executionInfo":{"status":"ok","timestamp":1618040864662,"user_tz":-420,"elapsed":869755,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"62f8f8b1-50ae-43bb-8d3a-c1e019156bde"},"source":["#train frozen\n","#print('training frozen')\n","#learn.freeze_to(-1)\n","#learn.fit_one_cycle(1, 1e-3, moms=(0.8, 0.7))\n","learn.lr_find()\n","learn.recorder.plot(suggestion=True)\n","min_grad_lr = learn.recorder.min_grad_lr\n","learn.fit_one_cycle(2, min_grad_lr)\n","#train unfrozen\n","print('training unfrozen')\n","learn.unfreeze()\n","#learn.fit_one_cycle(10, 1e-4, moms=(0.8, 0.7),\n"," #                  callbacks=[SaveModelCallback(learn, every='improvement', monitor='accuracy', name='best_lm')])\n","learn.fit_one_cycle(2, 1e-3)\n","\n","learn.save('/content/drive/MyDrive/ject_final_petch/prachathai_lm')\n","learn.save_encoder('/content/drive/MyDrive/ject_final_petch/prachathai_enc')"],"id":"blDSQ0ehjPn0","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      0.00% [0/1 00:00<00:00]\n","    </div>\n","    \n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='99' class='' max='538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      18.40% [99/538 00:31<02:21 11.6795]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n"],"name":"stderr"},{"output_type":"stream","text":["LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n","Min numerical gradient: 2.29E-06\n","Min loss divided by 10: 9.12E-04\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.418201</td>\n","      <td>3.822891</td>\n","      <td>0.367098</td>\n","      <td>02:52</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>4.364960</td>\n","      <td>3.816464</td>\n","      <td>0.369509</td>\n","      <td>02:53</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n"],"name":"stderr"},{"output_type":"stream","text":["training unfrozen\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>3.968503</td>\n","      <td>3.598673</td>\n","      <td>0.385625</td>\n","      <td>02:52</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>3.763435</td>\n","      <td>3.513504</td>\n","      <td>0.400759</td>\n","      <td>02:52</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  parameters = [p for p in parameters if p.grad is not None]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"innocent-tulsa","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"error","timestamp":1618746327808,"user_tz":-420,"elapsed":814,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"dc115996-f95a-4445-ad07-a3770ae8da47"},"source":["learn.lr_find()\n","learn.recorder.plot(suggestion=True)\n","min_grad_lr = learn.recorder.min_grad_lr"],"id":"innocent-tulsa","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dcbdfb7c4e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmin_grad_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_grad_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3nfCJJzH7x36","executionInfo":{"status":"ok","timestamp":1618041767217,"user_tz":-420,"elapsed":1772290,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"26ecf57b-184a-49eb-ac89-63c53a6c1186"},"source":["import pandas as pd \n","import numpy as np\n","from pythainlp import word_tokenize\n","from ast import literal_eval\n","from tqdm import tqdm_notebook\n","from collections import Counter\n","import re\n","import emoji\n","import string\n","from fastai.text import *\n","from fastai.callbacks import CSVLogger, SaveModelCallback\n","from pythainlp.ulmfit import *\n","\n","# #when training with augmented set\n","train_df = pd.read_csv('/content/drive/MyDrive/ject_final_petch/train_df.csv')\n","valid_df = pd.read_csv('/content/drive/MyDrive/ject_final_petch/valid_df.csv')\n","\n","#test set\n","test_df = pd.read_csv('/content/drive/MyDrive/ject_final_petch/test_df.csv')\n","\n","model_path = '/content/drive/MyDrive/ject_final_petch/prachathai_data/'\n","\n","#lm data\n","data_lm = load_data(model_path,'/content/drive/MyDrive/ject_final_petch/prachathai_lm.pkl')\n","data_lm.sanity_check()\n","\n","#classification data\n","tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th)\n","processor = [TokenizeProcessor(tokenizer=tt, chunksize=10000, mark_fields=False),\n","            NumericalizeProcessor(vocab=data_lm.vocab, max_vocab=60000, min_freq=3)]\n","data_cls = (ItemLists(model_path,train=TextList.from_df(train_df, model_path, cols=['body_text'], processor=processor),\n","                     valid=TextList.from_df(valid_df, model_path, cols=['body_text'], processor=processor))\n","    .label_from_df(list(train_df.columns[1:]))\n","    .add_test(TextList.from_df(test_df, model_path, cols=['body_text'], processor=processor))\n","    .databunch(bs=50)\n","    )\n","data_cls.sanity_check()\n","data_cls.save('/content/drive/MyDrive/ject_final_petch/prachathai_cls.pkl')\n","\n","#just load\n","data_cls = load_data(model_path,'/content/drive/MyDrive/ject_final_petch/prachathai_cls.pkl')\n","data_cls.sanity_check()\n","print(len(data_cls.vocab.itos))\n","\n","#model\n","config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False,\n","             output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n","trn_args = dict(bptt=70, drop_mult=0.5, alpha=2, beta=1, max_len=500)\n","\n","learn = text_classifier_learner(data_cls, AWD_LSTM, config=config, pretrained=False, **trn_args)\n","#load pretrained finetuned model\n","learn.load_encoder('/content/drive/MyDrive/ject_final_petch/prachathai_enc')\n","\n","#train unfrozen\n","\n","learn.freeze_to(-1)\n","learn.fit_one_cycle(4, slice(5e-3, 2e-3), moms=(0.8, 0.7))\n","learn.unfreeze()\n","learn.fit_one_cycle(4, slice(2e-3 / 100, 2e-3), moms=(0.8, 0.7))\n","\n","#save test results\n","probs, y_true = learn.get_preds(ds_type = DatasetType.Test, ordered=True)\n","probs = probs.numpy()\n","pickle.dump(probs, open(f'{model_path}probs.pkl','wb'))"],"id":"3nfCJJzH7x36","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(a, dtype=dtype, **kwargs)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"},{"output_type":"stream","text":["18944\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.755581</td>\n","      <td>0.696960</td>\n","      <td>01:12</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.758076</td>\n","      <td>0.704868</td>\n","      <td>01:08</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.600100</td>\n","      <td>0.455115</td>\n","      <td>01:11</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.506039</td>\n","      <td>0.384019</td>\n","      <td>01:17</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.448237</td>\n","      <td>0.344489</td>\n","      <td>01:12</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.409211</td>\n","      <td>0.342509</td>\n","      <td>01:09</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.346663</td>\n","      <td>0.312569</td>\n","      <td>01:28</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.335172</td>\n","      <td>0.271515</td>\n","      <td>01:38</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.303799</td>\n","      <td>0.263982</td>\n","      <td>01:30</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.275354</td>\n","      <td>0.259785</td>\n","      <td>01:26</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":842},"id":"qUYOMDNr_MyQ","executionInfo":{"status":"ok","timestamp":1618042194133,"user_tz":-420,"elapsed":2199197,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"1689b403-08f2-4f35-ccef-8a18a6374fb5"},"source":["learn.lr_find()\n","learn.recorder.plot()"],"id":"qUYOMDNr_MyQ","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='5' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      71.43% [5/7 06:07<02:26]\n","    </div>\n","    \n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.226275</td>\n","      <td>#na#</td>\n","      <td>01:17</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.227437</td>\n","      <td>#na#</td>\n","      <td>01:11</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.227229</td>\n","      <td>#na#</td>\n","      <td>01:11</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.244424</td>\n","      <td>#na#</td>\n","      <td>01:13</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.409133</td>\n","      <td>#na#</td>\n","      <td>01:14</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='9' class='' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      56.25% [9/16 00:53<00:41 0.7089]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"},{"output_type":"stream","text":["LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZgAAAEGCAYAAABYV4NmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxc5Xn//c+l3ZIlW5blVbLlFW8sNmIPwUlMYgJhCSWBpg1ZGp40IXubkK1JoE+blbT5PSSBNpClbEkD/TkFggPEgCHGlhe8G8vyIglblrxpsfa5nj9mTAZHlmV7js7M6Pt+veblOWfOGV23z2i+us9yH3N3REREEi0j7AJERCQ9KWBERCQQChgREQmEAkZERAKhgBERkUBkhV1AoowePdorKirCLkNEJKWsXr26yd1Lg3jvtAmYiooKqqqqwi5DRCSlmNnuoN5bu8hERCQQChgREQmEAkZERAIRaMCY2WIz22Zm1WZ2Rz/L3WhmbmaVsekKM2s3s3Wxx0+DrFNERBIvsIP8ZpYJ3ANcCdQBq8xsibtvPm65QuAzwCvHvcUOdz8vqPpERCRYQfZgLgSq3b3G3buAR4Dr+ljuLuA7QEeAtYiIyCALMmAmArVx03WxeW8wswVAubs/0cf6U8xsrZk9b2aXB1iniIgEILTrYMwsA7gb+FAfL+8FJrn7ATM7H/gfM5vr7s3HvcdtwG0AkyZNCrhiEZHk83/X1QNw7bkTMLOQq3mzIHsw9UB53HRZbN4xhcA8YJmZ7QIuBpaYWaW7d7r7AQB3Xw3sAGYe/wPc/T53r3T3ytLSQC5EFRFJWgfbuvjGkk089MqesEvpU5ABswqYYWZTzCwHuBlYcuxFdz/i7qPdvcLdK4AVwLXuXmVmpbGTBDCzqcAMoCbAWkVEUs73nt5KS0cPd10/L+l6LxDgLjJ37zGz24GngUzgfnffZGZ3AlXuvqSf1d8K3Glm3UAE+Li7HwyqVhGRVLN2zyEeWVXL371lCjPHFoZdTp8sXW6ZXFlZ6RqLTESGgt6Ic909y2ls6eTZLyxkeO7p9xXMbLW7VyawvDfoSn4RkRTz0Mo9bKxv5mtXzzmjcAmaAkZEJIUcaO3ke7/fyqXTSrjmnPFhl9MvBYyISAr5zu+30t7dy53XzU3KA/vxFDAiIiliXe1hfl1Vx0cum8L0Mcl5YD+eAkZEJAVEIs43lmyitDCXT71jRtjlDIgCRkQkBfx2TR2v1h7mjsWzkvrAfjwFjIhIkmvp6OY7v9/G/EkjuWH+xJOvkCRSIwZFRIaw//NcNU2tnfzs1koyMpL7wH489WBERJLYjsZWHnhpJ++rLOPc8pFhl3NKFDAiIkmqpzfCN5dsIi8rk39816ywyzllChgRkSTU0xvhc79+lRe3N/HFq2ZRWpgbdkmnTMdgRESSTHdvhE8/vJanNu7jS4tn8bcXTw67pNOigBERSSJdPRFuf2gNSzc38LWrZ/N3l08Nu6TTpoAREUkSkYjziQdX88yW/Xzr2rncemlF2CWdER2DERFJEo+treeZLfv5p2vmpHy4gAJGRCQpHO3q4XtPb+W88pF8+LKKsMtJCAWMiEgSuPf5GhqaO/n6NbOTfpTkgVLAiIiEbN+RDu59YQdXnzOe8yePCruchFHAiIiE7HtPbyPicMfi1LuYsj8KGBGREG2oO8Jv10Tv8VI+Kj/schJKASMiEhJ3564nNlNSkMMn3jYt7HISTgEjIhKSF7Y3sXLnQT575UyK8rLDLifhFDAiIiH5ybJqxhXl8f7K8rBLCYQCRkQkBOtqD7Oi5iAffcsUcrLS86s4PVslIpLkfrpsB0V5Wdxy0aSwSwmMAkZEZJDtaGzl6c37+OAlFQzPTd8hIRUwIiKD7D9eqCEnM4MPpcmQMCeigBERGUQNzR08tqaemyrLGD089W4idioUMCIig+j+5TvpiUS47fL0u+7leAoYEZFBcqS9mwdf2cO7zx7PpJL0umq/LwoYEZFB4O585fENHO3q4e8Xpn/vBRQwIiKD4oGXdvHE+r18cfEs5k4YEXY5g0IBIyISsKpdB/mXJ7fwzjlj+X/eOjXscgaNAkZEJECNLZ188qE1lBUP4/vvOzdtbiY2EOl7hY+ISMh6eiN86uE1HGnv5ucfvjAtB7TsT6A9GDNbbGbbzKzazO7oZ7kbzczNrDJu3pdj620zs3cFWaeISBDu/sNrrKg5yL/ccDazxxeFXc6gC6wHY2aZwD3AlUAdsMrMlrj75uOWKwQ+A7wSN28OcDMwF5gAPGNmM929N6h6RUQS6Y/b9vPjZTu45cJy3rugLOxyQhFkD+ZCoNrda9y9C3gEuK6P5e4CvgN0xM27DnjE3TvdfSdQHXs/EZGk9/rhdj7/6DpmjSvkG++ZG3Y5oQkyYCYCtXHTdbF5bzCzBUC5uz9xquvG1r/NzKrMrKqxsTExVYuInIHu3gifengtXT0RfvyBBeRlZ4ZdUmhCO4vMzDKAu4EvnO57uPt97l7p7pWlpaWJK05E5DR9/+ltrN59iH+98Rymlg4Pu5xQBXkWWT0Qf5u2sti8YwqBecCy2Gl744AlZnbtANYVEUk6y7bt594XavjARZO49twJYZcTuiB7MKuAGWY2xcxyiB60X3LsRXc/4u6j3b3C3SuAFcC17l4VW+5mM8s1synADGBlgLWKiJyRSMT59lNbmTK6gK9fMyfscpJCYD0Yd+8xs9uBp4FM4H5332RmdwJV7r6kn3U3mdmvgc1AD/BJnUEmIsls6eYGtu5r4YfvP3dIH3eJZ+4edg0JUVlZ6VVVVWGXISJDkLtz9Y+Wc7Srh2c+fwVZmakzSIqZrXb3ypMveepS539BRCRJPbtlP5v3NvPJt01PqXAJmv4nRETOgLvzo+e2Uz5qGNfP/4urKYY0BYyIyBlY9loj6+uO8MmF08lW7+VN9L8hInKa3J1/f2Y7E0cOG7LDwfRHASMicpqWVzexrvYwn3jbNHKy9HV6PP2PiIichqNdPXxzySYmjMjjr85X76Uvuh+MiMhp+NaSzdQ0tfHgRy8iN0vXvfRFPRgRkVP0u1df59GqWj6xcBqXTh8ddjlJSwEjInIKag8e5SuPbWDBpJF8dtHMsMtJagoYEZEBOjYUPwb/fvN8nZZ8EjoGIyIyQP/+zHbW1R7mnr9eQPmo/LDLSXqKXxGRAag9eJT7XqjhvfMncvU548MuJyUoYEREBuC7T28jM8P44uJZYZeSMhQwIiInsa72ML979XU+dvkUxo3IC7uclKGAERHph7vzL09sYfTwHG67YlrY5aQUBYyISD+Wbm5g5a6DfO7KmQzP1XlRp0IBIyJyAt29Eb7z1FamjxnO+yvLwy4n5ShgRERO4OGVe6hpauPLV83SjcROg/7HRET6cKC1kx/+4TUunjqKt88aE3Y5KUkBIyLSh39+YgutnT3cdd08zCzsclKSAkZE5DjPv9bI42vr+fuF05kxtjDsclKWAkZEJM7Rrh6++vgGppYW8ImFOi35TOicOxGROP/2zHbqDrXz6G0Xk5et+7ycCfVgRERiNtYf4T9frOGWC8u5aGpJ2OWkPAWMiAjQG3G+/NgGSobncsdVs8MuJy0oYEREiF7zsqH+CF+/Zg4jhmWHXU5aUMCIyJB3qK2L7y/dxsVTR/EeDcWfMAoYERnyvr90Gy0dPXzrWl3zkkgKGBEZ0jbWH+GhlXv44CWTOWucrnlJJAWMiAxZ7s43lmxiVH4On100M+xy0o4CRkSGrMfX1rN69yG+dNUsHdgPgAJGRIakw0e7+NentnJe+Uj+akFZ2OWkJV3JLyJDjrtzx283cPhoFw986AIyMnRgPwjqwYjIkPPoqlp+v2kf//DOs5g3cUTY5aQtBYyIDCk7Glv51u82c9n0Ej52+dSwy0lrgQaMmS02s21mVm1md/Tx+sfNbIOZrTOz5WY2Jza/wszaY/PXmdlPg6xTRIaGrp4In3lkLXnZGfzgpvO0ayxggR2DMbNM4B7gSqAOWGVmS9x9c9xiD7n7T2PLXwvcDSyOvbbD3c8Lqj4RGXp+sHQbG+ubue9vz2fciLywy0l7QfZgLgSq3b3G3buAR4Dr4hdw9+a4yQLAA6xHRIawlTsPcu8LNXzgokm8c+64sMsZEoIMmIlAbdx0XWzem5jZJ81sB/Bd4NNxL00xs7Vm9ryZXd7XDzCz28ysysyqGhsbE1m7iKSRzp5e7nhsPeWjhvHVqzVS8mAJ/SC/u9/j7tOALwFfi83eC0xy9/nA54GHzKyoj3Xvc/dKd68sLS0dvKJFJKXc88cd1DS28c/Xn01+jq7OGCxBBkw9UB43XRabdyKPANcDuHunux+IPV8N7AA0joOInLLtDS38ZFk11583gStm6g/RwRRkwKwCZpjZFDPLAW4GlsQvYGYz4iavBrbH5pfGThLAzKYCM4CaAGsVkTQUiTh3PLaBgtwsvnbNnLDLGXIC6yu6e4+Z3Q48DWQC97v7JjO7E6hy9yXA7Wa2COgGDgG3xlZ/K3CnmXUDEeDj7n4wqFpFJD09tHIPq3cf4nt/dQ6jh+eGXc6QY+4nP3HLzAqAdnePmNlMYBbwlLt3B13gQFVWVnpVVVXYZYhIktjf0sE7vv88Z5eN4MG/u0j3eTkBM1vt7pVBvPdAd5G9AOSZ2URgKfC3wM+DKEhEJBEefqWW1q4e7rpeNxELy0ADxtz9KPBe4MfufhMwN7iyREROXyTi/GZ1LZdOK2Fa6fCwyxmyBhwwZnYJ8AHgidi8zGBKEhE5Myt2HqDuUDvvqyw/+cISmIEGzGeBLwOPxw7UTwX+GFxZIiKn7zdVdRTmZfEuXbEfqgGdRebuzwPPA5hZBtDk7p/ufy0RkcHX3NHNUxv3cuOCMvKytaMlTAPqwZjZQ2ZWFDubbCOw2cz+MdjSRERO3RPr99LRHeEm7R4L3UB3kc2JDUx5PfAUMIXomWQiIknl11W1zBgznHPLdCOxsA00YLLNLJtowCyJXf+ikY9FJKlU729h7Z7DvK+yXKcmJ4GBBsy9wC6iQ+q/YGaTgeZ+1xARGWS/WV1HZoZx/fy/GLhdQjDQg/w/An4UN2u3mb0tmJJERE5dT2+Ex9bU8/ZZYygt1LAwyWCgB/lHmNndx+69YmY/INqbERFJCsu2NdLY0slN55eFXYrEDHQX2f1AC/C+2KMZeCCookRETtV/vbKbMYW5vG3WmLBLkZiBjqY8zd1vjJv+lpmtC6IgEZFTtbOpjWXbGvncoplkZ4Z+H0WJGeiWaDeztxybMLPLgPZgShIROTW/+tNusjONWy7StS/JZKA9mI8DvzSzYyeWx9+7RUQkNG2dPfxmdS1XzRvPmMK8sMuROAM9i+xV4FwzK4pNN5vZZ4H1QRYnInIyj6+tp6Wjh1svrQi7FDnOKe2sdPfm2BX9AJ8PoB4RkQFzd375p13Mm1jEgkkjwy5HjnMmR8N0mayIhGpFzUFea2jlg5dU6Mr9JHQmAaOhYkQkVL94eRfF+dlce+6EsEuRPvR7DMbMWug7SAwYFkhFIiIDUH+4naWb93HbW6dpWP4k1W/AuHvhYBUiInIqHl1VC8AHLpoUciVyIroiSURSjrvzv+tf5+KpJZSPyg+7HDkBBYyIpJzXGlqpaWzjqrPHh12K9EMBIyIp58kNezGDxXPHhV2K9EMBIyIp58kNe7mwYpSG5U9yChgRSSnbG1rYvr+Vd2v3WNJTwIhISnlyw77o7rF52j2W7BQwIpJSntq4l8rJxYwt0sCWyU4BIyIpY0djK1v3tWj3WIpQwIhIynhqw15Au8dShQJGRFLGkxv2sWDSSMaP0EhVqUABIyIpYVdTG5v3Nmv3WApRwIhISnhyY3T3mK7eTx0KGBFJeu7Of6+u4/zJxUwcqd1jqUIBIyJJb0XNQWoa27jlQo2cnEoCDRgzW2xm28ys2szu6OP1j5vZBjNbZ2bLzWxO3Gtfjq23zczeFWSdIpLcHnxlN0V5WVxzjnaPpZLAAsbMMoF7gKuAOcAt8QES85C7n+3u5wHfBe6OrTsHuBmYCywGfhx7PxEZYhpbOnl60z7+6vxy3VgsxQTZg7kQqHb3GnfvAh4BrotfwN2b4yYL+PPdM68DHnH3TnffCVTH3k9EhpjfrK6lu9f5a91YLOX0e0fLMzQRqI2brgMuOn4hM/sk8HkgB3h73Lorjlt3Yh/r3gbcBjBpkj58IukmEnEeXrmHi6eOYvqY4WGXI6co9IP87n6Pu08DvgR87RTXvc/dK929srS0NJgCRSQ0L1Y3UXuwnb++aHLYpchpCDJg6oHyuOmy2LwTeQS4/jTXFZE09OCK3ZQU5PCuuWPDLkVOQ5ABswqYYWZTzCyH6EH7JfELmNmMuMmrge2x50uAm80s18ymADOAlQHWKiJJZu+Rdp7dup+bKsvJzdLB/VQU2DEYd+8xs9uBp4FM4H5332RmdwJV7r4EuN3MFgHdwCHg1ti6m8zs18BmoAf4pLv3BlWriCSfR1fV0htx/lrXvqQsc/eTL5UCKisrvaqqKuwyRCRBFt39PGMKc3noYxeHXUpaM7PV7l4ZxHuHfpBfROR4Dc0dVO9vZeFZOnknlSlgRCTpvFTdBMBl00eHXImcCQWMiCSd5dVNjCrIYfa4orBLkTOggBGRpOLuvFx9gEumlZCRYWGXI2dAASMiSWVHYxv7mjt4i3aPpTwFjIgklZd3xI6/TFPApDoFjIgkleXbmygfNYxJJflhlyJnSAEjIkmjpzfCn2oOqPeSJhQwIpI0Nr7eTEtHj05PThMKGBFJGseuf7l0WknIlUgiKGBEJGm8VN3E7PFFlAzPDbsUSQAFjIgkhfauXqp2HeIt09V7SRcKGBFJClW7D9LVG9HxlzSigBGRpPBS9QGyM40Lp4wKuxRJEAWMiCSF5dWNzJ9UTH5OYLepkkGmgBGR0DU0d7CxvpkrZmp4/nSigBGR0D27ZT8AV84ZG3IlkkgKGBEJ3bNbGigfNYwZY4aHXYokkAJGREJ1tKuH5dVNLJo9FjMNz59OFDAiEqrl25vo7Ilw5WztHks3ChgRCdWzW/ZTmJfFBTo9Oe0oYEQkNJGI8+zWBhaeNYbsTH0dpRttUREJzbq6wzS1drFo9piwS5EAKGBEJDTPbG4gK8NYOFMBk44UMCISmme37OeCilGMyM8OuxQJgAJGREKx58BRtjW0sEgXV6YtBYyIhOKZLQ0AOv6SxhQwIhKKZ7c2MHPscCaXFIRdigREASMig665o5tXag7yDl1cmdYUMCIy6F58rYmeiPOOWdo9ls4UMCIy6J7d2sDI/GzmTyoOuxQJkAJGRAZVb8R5flsjC2eWkpmhwS3TmQJGRAbVq3WHOdDWxdt1/CXtKWBEZFD9cet+MjOMK2bo7pXpLtCAMbPFZrbNzKrN7I4+Xv+8mW02s/Vm9qyZTY57rdfM1sUeS4KsU0QGz7Nb9nP+5GJdvT8EBBYwZpYJ3ANcBcwBbjGzOcctthaodPdzgP8Gvhv3Wru7nxd7XBtUnSIyePYeaWfz3mberrPHhoQgezAXAtXuXuPuXcAjwHXxC7j7H939aGxyBVAWYD0iErI/bm0E0OnJQ0SQATMRqI2brovNO5GPAk/FTeeZWZWZrTCz6/tawcxuiy1T1djYeOYVi0igntvaQFnxMKaPGR52KTIIssIuAMDM/gaoBK6Imz3Z3evNbCrwnJltcPcd8eu5+33AfQCVlZU+aAWLyCnr6O5leXUT768sx0ynJw8FQfZg6oHyuOmy2Lw3MbNFwFeBa92989h8d6+P/VsDLAPmB1iriATsTzUH6OiO8DbtHhsyggyYVcAMM5tiZjnAzcCbzgYzs/nAvUTDZX/c/GIzy409Hw1cBmwOsFYRCdhzW/YzLDuTi6eWhF2KDJLAdpG5e4+Z3Q48DWQC97v7JjO7E6hy9yXA94DhwG9iXeY9sTPGZgP3mlmEaAh+290VMCIpKhJxntu6n8umjyYvOzPscmSQBHoMxt2fBJ48bt4/xT1fdIL1XgbODrI2ERkcbZ09fOHXr1J/uJ0vvHNm2OXIIEqKg/wikp72HDjKbb+q4rWGFr5+zRxumN/fiaSSbhQwIhKIl6ub+MRDa3CHX3zkQi7X0DBDjgJGRBLuD5sb+Ph/rWbq6AL+44OVVIzWXSuHIgWMiCTUmj2H+NTDa5g3oYj/+ruLKMzTmGNDlUZTFpGEqWls5aM/X8XYojx+9qELFC5DnAJGRBKisaWTWx9YiZnxiw9fyOjhuWGXJCFTwIjIGWvt7OEjP19FY0snP7tVx1wkSsdgROSMHD7axYceWMXmvc3c97fnM39ScdglSZIY8j2YSMT5yuMbWL37YNiliKScxpZObr5vBZtfb+bHH1jAO3QbZIkz5Hswew4eZemmBh56ZQ9XzCzlC++cyTllI8MuK61FIs6m15v547b97Dl4lLMnjmD+pJHMHl9EduaQ/5snZdQfbudv/vMV9h3p4GcfqtR1LvIXzD09RrmvrKz0qqqq01r3aFcPv/zTbu59fgeHjnZz5ZyxfG7RTOZMKEpwlUPb+rrD/OLl3Tz/WiNNrdGBs4vzszl0tBuA3KwMzikbwUVTSrhkWgkLJhUzLEfjViWjjfVHuO2XVbR09PDAhy+gsmJU2CXJaTKz1e5eGch7K2D+rKWjm5+/tIv7XqyhpaOHd84Zy2cWzWDuhBFn9L6tnT2srzvMJVNLhuR9MHojzk+WVfPDZ7ZTkJPJFWeN4W1nlfLWmaWUFOSw90gHa/YcYu2ew1TtPsTG+iP0RpyczAxmTyjC3Wnt6KGls4eOrl5mjS/k4qklXDxVITTYjrR3c/fSbfxqxW5GFeTy8w9fwLyJZ/b7IeFSwAxAIgLmmCPt3Tzw0k5+tnznGQfNn3Yc4B9+Ex3o7y3TR/Ov7z2b8lH5CakzFbx+uJ3PPrqOlTsP8p5zJ/DP189jxLD+r41o6eimatch/lRzgA11R8jNzmB4bhaFeVlkZWSwvv4IG+oOE3HIzjTKR+VTVpxPWfEwyoqHMWJYNrlZmeRmZZCTlcGhti52NrWxo7GNnU2tdHRHKC3MZUxhLqWFuYwqyCEvO7p8XnYmhXlZTCsdztTSAvJzErcXua2zh6c37WPJq69ztKuXOeOLmDOhiDnji5g5tpCcrOTdPRiJOI+trefbT23hYFsXf3PxZL5w5VmMyNd1LqlOATMAiQyYY44PmqvPHs/nrpw5oNu9dnT38t3fb+P+l3ZSUZLPexeUcd8LNfRGnC8uPotbL6kgIyN9ezO9EefxtfXc+btN9EacO6+bx3sXTExYD66lo5uq3YdYufMguw+0UXeonbpD7Rxs6+pz+ZysDCpK8pk6ejj5OZk0tnayv7mT/S0dHG7v5kS/BhNHDmNqaUEsxIZRXpxPRUkBcycUDWj79fRGeLG6if9ZW8/STQ20d/dSVjyMMYW5bNnbQnt3LwAFOZm8ZcZo3jFrLAtnlTKmMO+0/28Sqam1k9+uruPRVbXUNLWxYNJI7rxunnotaUQBMwBBBMwxR9q7+c8Xa/jZ8p10dPdyw/wyrj1vAvHfL109EZo7umlu76G5vZv/WVfPjsY2PnjJZO64ahb5OVm8fridrzy+gWXbGpk/aSQ3Lijj8hmjmVwSzjUDR452s7Y2umtqzZ5D1DS2Mbkkn7kTipg7YQQzxg6nub2HukNHqTvUzr4jHZQVD+P8ycWcWz6Sgty//Ove3Vm6uYEfLN3Gaw2tzJ80kn97/3mD1sajXT20dvTQ2ROhs6eXju4II4ZlM2HkMDJPEAjuTldvhI7uCJ3dvRxu72bH/lZ2NLZSvb+Vmqa2vwiv8SPyuPa8CVx/3kRmjy/6i/dbW3uY/7u2nv9dv5cDbV2MGJbNNeeM54b5Ezl/cjFmRm/E2X2gjU2vN7Oi5gDPbd3P3iMdAFSU5JOfk0VedrRXVZyfw4VTRnHZ9BKmlQ4PbFfr0a4eqve38lpDK89uaeAPmxvoiTgXVBTzNxdP5j3nTEjrP4yGIgXMAAQZMMccaO3kJ8t28MsVu+nqifS7bPmoYfzLDWf/xZk17tG/7H+w9DXqD7e/sexl00ZzTtlI5kwo4qyxhWd8XKGts4dtDS20dvTQ1hk9fnGwrYtdTW3sbGpj14E2GpqjB9ozDGaOLWTm2EJ2HWhj676Wv2ifGZQU5NDUGv2SzcwwZo0rpGJ0AaPycyguyKEoL4vfrd/Lq7WHmVpawBeuPIur5o1Lmy+k1s5o2G7d28L/rn+dZdsa6Yk408cMZ8SwbNq7euno7uVIezcH2rrIycpg0ewxXHfeRBaeVUpuVv/b1N3Zuq+F57buZ/PeZjq7owHZ0d3L3iMdb3xexhTmcsm0EuZOKGLWuCJmjSuktDD3tELn9cPtvPBaIy9ub+LVusPUHWp/47Xi/GxuXFDGzReWM31M4Sm/t6QGBcwADEbAHNPY0smuA23E/zpnZWYwYlg2RXlZFOZln3R/urtT09TGS9VNvLi9iRU1B2jp6AGiX/gVowuYNS76pX/W2EJmjitk8qh8sk5yGu+B1k5+/vIufvHyLppj7xevpCCHitEFVJQUMLW0gPnlIzmnfCTD43oj3b2RN/56L87Poax4GONHDCMnK4MjR7tZU3uINbsPsWbPIfYe6eBQW9cbu5kmjhzGZxbN4L3zJ5601lR3sK2LJ9a/ztLNDUTcGZadxbCcTPKzMzm/opjF88ZRlMCxuPYcOMpLO5p4qbqJlTsPsr+l843XRhXkMHdCEfMmjuDs2KOseFifoVN/uJ0HV+zmD5sb2L6/FYBxRXlcMGUUM8cMZ8bY4UwfU0hFyck/b5L6FDADMJgBE4RIxKk71M7mvc1siT1ea2hh98GjbxwfyM40Jo3Kjx2AHs6YwlzyczLJz81iWHYmy7c38mhVLZ09Ed45Zyw3LiijuCCH4blZDM/NYkR+dkK/8OL1Rpwj7d0U5WXpS2mQHGzrYuu+Zrbta2HL3mY21kc/Mz2R6Admckk+i2aPZdHssVRWFLOu9jAPvLSTpzc1AHDptBKumBk9m2/GmOB2u0lyU8AMQKoHzIm0d/WyfX8L2/a1sKOxjZrG6DGB3Qfa6Dbw7mUAAAiJSURBVO5987bLzjRumD+R2946bUAnIkj66eju5bWGFtbVHua5rft5ufoAXb0RcrMy6OyJUJSXxS0XTeKDl1QwceSwsMuVJKCAGYB0DZgT6emN0NrZw9GuXo529dDW2cv4kXlJc/aRJIe2zh5e3N7E8upGzhpXxI0LJib01GtJfUEGjD5pKSorM4OR+TmMHDqX1MhpKMjNYvG8cSyeNy7sUmQI0s5yEREJhAJGREQCoYAREZFAKGBERCQQChgREQmEAkZERAKhgBERkUAoYEREJBBpcyW/mTUCu4+bPQI4corzTvZ8NNB0mmX29bNPZZmBtGew2nKyWk+2zKm25fjpY8/j52nbDKzWky2jbRPud0B/ywXRlgJ3f/Ow74ni7mn7AO471Xknew5UJbKeU1lmIO0ZrLacaXtOtS39tCF+nraNtk1Sb5uBtCWR2yboz9nJHum+i+x3pzFvIM8TWc+pLDOQ9gxWWwb6Pida5lTbcvz0706wzOnStul/vrbN4H0H9LdcMrXlpNJmF9lgMbMqD2hguMGWTm2B9GpPOrUF0qs9asvApXsPJgj3hV1AAqVTWyC92pNObYH0ao/aMkDqwYiISCDUgxERkUAoYEREJBBDOmDM7H4z229mG09j3fPNbIOZVZvZjyzuhuZm9ikz22pmm8zsu4mt+oT1JLwtZvZNM6s3s3Wxx7sTX/kJawpk28Re/4KZuZmNTlzF/dYTxLa5y8zWx7bLUjObkPjK+6wniLZ8L/b7st7MHjezkYmv/IQ1BdGem2K/+xEzC/xkgDNpwwne71Yz2x573Bo3v9/fqz4FeQ50sj+AtwILgI2nse5K4GLAgKeAq2Lz3wY8A+TGpsekcFu+CfxDumyb2GvlwNNEL8odnaptAYrilvk08NMUbss7gazY8+8A30nlzxkwGzgLWAZUJmsbYvVVHDdvFFAT+7c49ry4v/b29xjSPRh3fwE4GD/PzKaZ2e/NbLWZvWhms45fz8zGE/0FX+HR//lfAtfHXv574Nvu3hn7GfuDbUVUQG0JTYDt+SHwRWDQzm4Joi3u3hy3aAGD1J6A2rLU3Xtii64AyoJtxZ8F1J4t7r5tMOqP/bzTasMJvAv4g7sfdPdDwB+Axaf7PTGkA+YE7gM+5e7nA/8A/LiPZSYCdXHTdbF5ADOBy83sFTN73swuCLTa/p1pWwBuj+26uN/MioMrdUDOqD1mdh1Q7+6vBl3oAJzxtjGz/9fMaoEPAP8UYK0nk4jP2TEfIfrXcZgS2Z6wDKQNfZkI1MZNH2vXabU3a4A/dEgws+HApcBv4nYv5p7i22QR7V5eDFwA/NrMpsZSf9AkqC0/Ae4i+tfxXcAPiH4BDLozbY+Z5QNfIbo7JlQJ2ja4+1eBr5rZl4HbgW8krMgBSlRbYu/1VaAHeDAx1Z1WDQlrT1j6a4OZfRj4TGzedOBJM+sCdrr7DYmuRQHzZhnAYXc/L36mmWUCq2OTS4h+8cZ348uA+tjzOuCxWKCsNLMI0QHlGoMsvA9n3BZ3b4hb7z+A/w2y4JM40/ZMA6YAr8Z+6cqANWZ2obvvC7j24yXicxbvQeBJQggYEtQWM/sQcA3wjsH+Y+w4id42YeizDQDu/gDwAICZLQM+5O674hapBxbGTZcRPVZTz+m0N+gDUMn+ACqIOzgGvAzcFHtuwLknWO/4A17vjs3/OHBn7PlMot1NS9G2jI9b5nPAI6m8bY5bZheDdJA/oG0zI26ZTwH/ncJtWQxsBkoH8/MV9OeMQTrIf7pt4MQH+XcSPcBfHHs+aiDt7bOuMDZosjyAh4G9QDfRnsdHif6V+3vg1diH/p9OsG4lsBHYAfx//HlUhBzgv2KvrQHensJt+RWwAVhP9K+28YPRlqDac9wyuxi8s8iC2Da/jc1fT3Tgwokp3JZqon+IrYs9BuWMuADbc0PsvTqBBuDpZGwDfQRMbP5HYtukGvjwydrb30NDxYiISCB0FpmIiARCASMiIoFQwIiISCAUMCIiEggFjIiIBEIBI2nNzFoH+ee9nKD3WWhmRyw6WvJWM/v+ANa53szmJOLniySCAkbkFJhZv6NfuPulCfxxL3r0auz5wDVmdtlJlr8eUMBI0lDAyJBzopFmzew9sUFK15rZM2Y2Njb/m2b2KzN7CfhVbPp+M1tmZjVm9um4926N/bsw9vp/x3ogDx67f4aZvTs2b3Xsvhr9DsHj7u1EL0A8Nmjnx8xslZm9ama/NbN8M7sUuBb4XqzXM+0MRtQVSQgFjAxFJxppdjlwsbvPBx4hOqz/MXOARe5+S2x6FtGhzS8EvmFm2X38nPnAZ2PrTgUuM7M84F6i99I4Hyg9WbGxUaxnAC/EZj3m7he4+7nAFuCj7v4y0dEW/tHdz3P3Hf20U2RQaLBLGVJOMlpuGfBo7N4XOUTHYTpmSawnccwTHr3nT6eZ7QfG8ubhzAFWuntd7OeuIzpeVCtQ4+7H3vth4LYTlHu5mb1KNFz+zf88KOc8M/tnYCQwnOgN1E6lnSKDQgEjQ80JR5oF/g9wt7svMbOFRO/oeUzbcct2xj3vpe/fpYEs058X3f0aM5sCrDCzX7v7OuDnwPXu/mpsFOKFfazbXztFBoV2kcmQ4tE7Qe40s5sALOrc2Msj+PMQ5Lf2tX4CbAOmmllFbPr9J1sh1tv5NvCl2KxCYG9st9wH4hZtib12snaKDAoFjKS7fDOri3t8nuiX8kdju582AdfFlv0m0V1Kq4GmIIqJ7Wb7BPD72M9pAY4MYNWfAm+NBdPXgVeAl4Ctccs8Avxj7CSFaZy4nSKDQqMpiwwyMxvu7q2xs8ruAba7+w/Drksk0dSDERl8H4sd9N9EdLfcvSHXIxII9WBERCQQ6sGIiEggFDAiIhIIBYyIiARCASMiIoFQwIiISCD+f6nMXMu3/0S3AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLbE4bpyszQj","executionInfo":{"status":"ok","timestamp":1618042194640,"user_tz":-420,"elapsed":2199697,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"abb4827a-9c14-479f-bf15-847f2e8e7261"},"source":["probs = pickle.load(open(f'{model_path}probs.pkl','rb'))\n","preds = (probs > 0.5).astype(int)\n","y_true = np.array(test_df.iloc[:,1:])\n","probs.shape, y_true.shape"],"id":"dLbE4bpyszQj","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((309, 4), (309, 4))"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"pzkocD_HtN18","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042194641,"user_tz":-420,"elapsed":2199696,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"bd57bc5d-2e83-4c21-d6e2-7f85bd67c5ce"},"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","#macro metrics\n","for i in range(4):\n","    print(\n","        (preds[:,i]==y_true[:,i]).mean(),\n","        f1_score(preds[:,i],y_true[:,i]),\n","        precision_score(preds[:,i],y_true[:,i]),\n","        recall_score(preds[:,i],y_true[:,i])\n","         )"],"id":"pzkocD_HtN18","execution_count":null,"outputs":[{"output_type":"stream","text":["0.7993527508090615 0.7769784172661871 0.7448275862068966 0.8120300751879699\n","0.9061488673139159 0.8571428571428571 0.8446601941747572 0.87\n","0.86084142394822 0.7772020725388601 0.872093023255814 0.7009345794392523\n","0.9061488673139159 0.8053691275167786 0.8 0.8108108108108109\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYM1YA2xIlnU"},"source":["#benchmark_labels = ['การเมือง','ต่างประเทศ','เศรษฐกิจ','การศึกษา']"],"id":"gYM1YA2xIlnU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BX5n-XTA2R3w"},"source":["learn.save('/content/drive/MyDrive/ject_final_petch/06a-learner_text_classifier-2')"],"id":"BX5n-XTA2R3w","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EObLpEXMITqR"},"source":["learn.load('/content/drive/MyDrive/ject_final_petch/06a-learner_text_classifier-2');"],"id":"EObLpEXMITqR","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfmsnzSFITsv","executionInfo":{"status":"ok","timestamp":1618042207207,"user_tz":-420,"elapsed":2212252,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"250e3abe-dc7a-47e5-9ba4-519f972ca824"},"source":["learn.predict(\"ระบอบการเมืองการปกครองภายใต้ทักษิณ\") #1"],"id":"HfmsnzSFITsv","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 0., 0.]),\n"," tensor([0., 0., 0., 0.]),\n"," tensor([0.4889, 0.4281, 0.0719, 0.2198]))"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoVZRkVXFog9","executionInfo":{"status":"ok","timestamp":1618042207208,"user_tz":-420,"elapsed":2212248,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"4f952535-be8c-46aa-ebf0-e60f03ff53fa"},"source":["learn.predict(\"ระบอบการเมืองการปกครองภายใต้ทักษิณ\") #2"],"id":"SoVZRkVXFog9","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 0., 0.]),\n"," tensor([0., 0., 0., 0.]),\n"," tensor([0.4889, 0.4281, 0.0719, 0.2198]))"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cu4zTS-5ITvf","executionInfo":{"status":"ok","timestamp":1618042207208,"user_tz":-420,"elapsed":2212241,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"66addd78-e62f-4a2d-96f5-69920d82d84f"},"source":["learn.predict(\"ประชาไท-5 กค. 48 คณะรัฐมนตรีเรียกงบประมาณคืน\") #1"],"id":"Cu4zTS-5ITvf","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([1., 1., 0., 0.]),\n"," tensor([1., 1., 0., 0.]),\n"," tensor([0.8516, 0.5746, 0.0491, 0.0795]))"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRiscWcVFsFG","executionInfo":{"status":"ok","timestamp":1618042207209,"user_tz":-420,"elapsed":2212237,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"bb4f29af-0158-4614-9565-39242e14564d"},"source":["learn.predict(\"ประชาไท-5 กค. 48 คณะรัฐมนตรีเรียกงบประมาณคืน\") #2"],"id":"PRiscWcVFsFG","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([1., 1., 0., 0.]),\n"," tensor([1., 1., 0., 0.]),\n"," tensor([0.8516, 0.5746, 0.0491, 0.0795]))"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJTNeAOtITx2","executionInfo":{"status":"ok","timestamp":1618042207209,"user_tz":-420,"elapsed":2212229,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"1a1a8697-cd18-4034-9049-e94299395229"},"source":["learn.predict(\"เครือข่ายวิถีชนคนพอเพียง ได้มีการจัดค่ายเยาวชน\") #1"],"id":"vJTNeAOtITx2","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 1., 1.]),\n"," tensor([0., 0., 1., 1.]),\n"," tensor([0.1398, 0.0255, 0.9239, 0.7836]))"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwSgyZ-cFu4G","executionInfo":{"status":"ok","timestamp":1618042207210,"user_tz":-420,"elapsed":2212224,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"bec7b71f-04d1-43ce-df59-b79438ad58cf"},"source":["learn.predict(\"เครือข่ายวิถีชนคนพอเพียง ได้มีการจัดค่ายเยาวชน\") #2"],"id":"gwSgyZ-cFu4G","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 1., 1.]),\n"," tensor([0., 0., 1., 1.]),\n"," tensor([0.1398, 0.0255, 0.9239, 0.7836]))"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lu4Xz4r6ITz3","executionInfo":{"status":"ok","timestamp":1618042207210,"user_tz":-420,"elapsed":2212218,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"270d0696-030d-47c6-8dee-657304e30e5b"},"source":["learn.predict(\"อันดับการศึกษาภายในประเทศไทยผลเฉลี่ยแย่ที่สุดในรอบปี\") #1"],"id":"Lu4Xz4r6ITz3","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 0., 1.]),\n"," tensor([0., 0., 0., 1.]),\n"," tensor([0.1905, 0.0669, 0.1920, 0.9375]))"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9z_xXwLfFzMl","executionInfo":{"status":"ok","timestamp":1618042207211,"user_tz":-420,"elapsed":2212213,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"b2ebc6a2-1727-47c9-9495-5ff26c92252b"},"source":["learn.predict(\"อันดับการศึกษาภายในประเทศไทยผลเฉลี่ยแย่ที่สุดในรอบปี\") #2"],"id":"9z_xXwLfFzMl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 0., 1.]),\n"," tensor([0., 0., 0., 1.]),\n"," tensor([0.1905, 0.0669, 0.1920, 0.9375]))"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gf6AAjwUIT2Z","executionInfo":{"status":"ok","timestamp":1618042207211,"user_tz":-420,"elapsed":2212207,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"127924c4-b5ae-4c00-c6c3-51aad90995b5"},"source":["learn.predict(\"หุ้นของประเทศไทยลดลงอย่างมาก\") #1"],"id":"gf6AAjwUIT2Z","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([1., 0., 1., 0.]),\n"," tensor([1., 0., 1., 0.]),\n"," tensor([0.7163, 0.1123, 0.7627, 0.0675]))"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"N1lPb4C8IT4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042207211,"user_tz":-420,"elapsed":2212202,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"dddee722-a061-48af-c43f-ad1982cc0809"},"source":["learn.predict(\"หุ้นของประเทศไทยลดลงอย่างมาก\") #2"],"id":"N1lPb4C8IT4x","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([1., 0., 1., 0.]),\n"," tensor([1., 0., 1., 0.]),\n"," tensor([0.7163, 0.1123, 0.7627, 0.0675]))"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"VS6zvd_4IT7A"},"source":[""],"id":"VS6zvd_4IT7A","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKETNKPMIT9u"},"source":[""],"id":"rKETNKPMIT9u","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBNhdAXHIUBh"},"source":[""],"id":"YBNhdAXHIUBh","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"advisory-influence"},"source":["##### train_cls.py."],"id":"advisory-influence"},{"cell_type":"code","metadata":{"id":"gross-insight"},"source":["# #when training with augmented set\n","#train_df = pd.read_csv('/content/drive/MyDrive/ject_petch/train_df.csv')\n","#valid_df = pd.read_csv('/content/drive/MyDrive/ject_petch/valid_df.csv')\n","\n","#test set\n","#test_df = pd.read_csv('/content/drive/MyDrive/ject_petch/test_df.csv')"],"id":"gross-insight","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"figured-noise"},"source":["#model_path = '/content/drive/MyDrive/ject_petch/prachathai_data/'\n","\n","#lm data\n","#data_lm = load_data(model_path,'/content/sample_data/prachathai_lm_1.pkl')\n","#data_lm.sanity_check()"],"id":"figured-noise","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"seeing-practitioner"},"source":["#classification data\n","#tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th)\n","#processor = [TokenizeProcessor(tokenizer=tt, chunksize=10000, mark_fields=False),\n","#            NumericalizeProcessor(vocab=data_lm.vocab, max_vocab=60000, min_freq=3)]\n","#data_cls = (ItemLists(model_path,train=TextList.from_df(train_df, model_path, cols=['body_text'], processor=processor),\n","#                     valid=TextList.from_df(valid_df, model_path, cols=['body_text'], processor=processor))\n","#    .label_from_df(list(train_df.columns[1:]))\n","#    .add_test(TextList.from_df(test_df, model_path, cols=['body_text'], processor=processor))\n","#    .databunch(bs=50)\n","#    )"],"id":"seeing-practitioner","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"charming-relative"},"source":["#data_cls.sanity_check()\n","#data_cls.save('/content/drive/MyDrive/ject_petch/prachathai_cls.pkl')"],"id":"charming-relative","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"perceived-order"},"source":["#just load\n","#data_cls = load_data(model_path,'/content/prachathai_data/prachathai_cls_1.pkl')\n","#data_cls.sanity_check()\n","#print(len(data_cls.vocab.itos))"],"id":"perceived-order","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"analyzed-diabetes"},"source":["#model\n","#config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False,\n","#             output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n","#trn_args = dict(bptt=70, drop_mult=0.5, alpha=2, beta=1, max_len=500)\n"],"id":"analyzed-diabetes","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pvPLWvG60dx"},"source":["#learn = text_classifier_learner(data_cls, AWD_LSTM, config=config, pretrained=False, **trn_args)"],"id":"2pvPLWvG60dx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHKgC2vX87_S"},"source":["#learn.opt_func = partial(optim.Adam, betas=(0.7, 0.99))"],"id":"OHKgC2vX87_S","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZEYYnoKrJyef"},"source":["#learn.unfreeze()"],"id":"ZEYYnoKrJyef","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"www4vsq7HQNf"},"source":["#learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))\n","#learn.recorder.plot()"],"id":"www4vsq7HQNf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwGMmbdeIcKQ"},"source":["learn.save('04a-learner_text_classifier-2')"],"id":"HwGMmbdeIcKQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFU2E3tqIjZ4"},"source":["learn.load('04a-learner_text_classifier-2');"],"id":"yFU2E3tqIjZ4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPeY48XqIoUm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042219806,"user_tz":-420,"elapsed":2224766,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"40eda977-4a14-4008-8f27-d6ccdfc543cf"},"source":["learn.predict(\"ระบอบการเมืองการปกครองภายใต้ทักษิณ\")"],"id":"DPeY48XqIoUm","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 0., 0.]),\n"," tensor([0., 0., 0., 0.]),\n"," tensor([0.4889, 0.4281, 0.0719, 0.2198]))"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"70UErfhNJErp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042219807,"user_tz":-420,"elapsed":2224763,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"5579d3b7-36c5-4333-fb27-96ac0bf1ef50"},"source":["learn.predict(\"ประชาไท-5 กค. 48 คณะรัฐมนตรีเรียกงบประมาณคืน\")"],"id":"70UErfhNJErp","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([1., 1., 0., 0.]),\n"," tensor([1., 1., 0., 0.]),\n"," tensor([0.8516, 0.5746, 0.0491, 0.0795]))"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"QubAF_K6JMYp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042219807,"user_tz":-420,"elapsed":2224759,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"e5452ed5-16c3-4c05-e508-96cc9bace568"},"source":["learn.predict(\"เครือข่ายวิถีชนคนพอเพียง ได้มีการจัดค่ายเยาวชน\")"],"id":"QubAF_K6JMYp","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 1., 1.]),\n"," tensor([0., 0., 1., 1.]),\n"," tensor([0.1398, 0.0255, 0.9239, 0.7836]))"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"VmuSXhUKJcS5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042219807,"user_tz":-420,"elapsed":2224755,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"187d0709-c611-422a-b930-fdd02f68b1e6"},"source":["learn.predict(\"อันดับการศึกษาภายในประเทศไทย\")"],"id":"VmuSXhUKJcS5","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([0., 0., 0., 1.]),\n"," tensor([0., 0., 0., 1.]),\n"," tensor([0.2904, 0.0370, 0.0492, 0.9339]))"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"mz0NffsVJcdS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618042219808,"user_tz":-420,"elapsed":2224752,"user":{"displayName":"Kwanchai Klommek","photoUrl":"","userId":"13341572052410456308"}},"outputId":"b2af66b5-56e2-42be-f5b7-f9f2456bc09c"},"source":["learn.predict(\"หุ้นของประเทศไทยลดลงอย่างมาก\")"],"id":"mz0NffsVJcdS","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(MultiCategory tensor([1., 0., 1., 0.]),\n"," tensor([1., 0., 1., 0.]),\n"," tensor([0.7163, 0.1123, 0.7627, 0.0675]))"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"c-_GgYuyJckT"},"source":["benchmark_labels = ['การเมือง','ต่างประเทศ','เศรษฐกิจ','การศึกษา']"],"id":"c-_GgYuyJckT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xHq0ffHJcmp"},"source":[""],"id":"_xHq0ffHJcmp","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_lct8ShJcpV"},"source":[""],"id":"j_lct8ShJcpV","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dSPsV8p66Vg"},"source":[""],"id":"2dSPsV8p66Vg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAt4bxxfuzIX"},"source":[""],"id":"TAt4bxxfuzIX","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"optimum-artist"},"source":[""],"id":"optimum-artist","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"buried-angola"},"source":[""],"id":"buried-angola","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"specific-suffering"},"source":[""],"id":"specific-suffering","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"revised-stretch"},"source":[""],"id":"revised-stretch","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fifteen-civilian"},"source":[""],"id":"fifteen-civilian","execution_count":null,"outputs":[]}]}